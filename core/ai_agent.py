from typing import List, Dict, Any, Optional
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.agents import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain.tools import BaseTool
from tools.langchain.langchain_tools import tool_registry, MCPTool
from mcp.servers.mcp import get_all_mcp_tools
from mcp.tools.tool_manager import tool_manager, ToolCategory
from core.conversation_history import ConversationHistory
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class AIAgent:
    """AI ì—ì´ì „íŠ¸ - ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •í•˜ê³  ì‹¤í–‰"""

    def __init__(self, api_key: str, model_name: str = "gpt-3.5-turbo"):
        self.api_key = api_key
        self.model_name = model_name
        self.llm = self._create_llm()
        self.tools: List[MCPTool] = []
        self.agent_executor: Optional[AgentExecutor] = None
        self.conversation_history = ConversationHistory()

        # MCP ë„êµ¬ ë¡œë“œ ë° ë“±ë¡
        self._load_mcp_tools()
        # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¡œë“œ
        self.conversation_history.load_from_file()

    def _create_llm(self):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if self.model_name.startswith("gemini"):
            # Gemini ëª¨ë¸ì— ë©€í‹°ëª¨ë‹¬ ì§€ì› í™œì„±í™” ë° ì´ë¯¸ì§€ ì²˜ë¦¬ ìµœì í™”
            return ChatGoogleGenerativeAI(
                model=self.model_name,
                google_api_key=self.api_key,
                temperature=0.1,  # ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                convert_system_message_to_human=True,  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¸ê°„ ë©”ì‹œì§€ë¡œ ë³€í™˜
                max_tokens=4096,  # ì¶©ë¶„í•œ í† í° í• ë‹¹
            )
        else:
            return ChatOpenAI(
                model=self.model_name, 
                openai_api_key=self.api_key, 
                temperature=0.1,
                max_tokens=4096
            )

    def _load_mcp_tools(self):
        """MCP ë„êµ¬ ë¡œë“œ ë° LangChain ë„êµ¬ë¡œ ë“±ë¡"""
        try:
            all_mcp_tools = get_all_mcp_tools()
            if all_mcp_tools:
                # ëª¨ë“  ë„êµ¬ ë“±ë¡
                self.tools = tool_registry.register_mcp_tools(all_mcp_tools)
                tool_manager.register_tools(all_mcp_tools)
                logger.info(f"AI ì—ì´ì „íŠ¸ì— {len(self.tools)}ê°œ ë„êµ¬ ë¡œë“œë¨")
            else:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"MCP ë„êµ¬ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _should_use_tools(self, user_input: str, force_agent: bool = False) -> bool:
        """AIê°€ ìì—°ì–´ë¥¼ ì´í•´í•˜ì—¬ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°ì •"""
        import time
        start_time = time.time()
        logger.info(f"ğŸ¤” ë„êµ¬ ì‚¬ìš© íŒë‹¨ ì‹œì‘: {user_input[:30]}...")
        
        try:
            # ë„êµ¬ ì„¤ëª… ìˆ˜ì§‘
            tool_descriptions = []
            for tool in self.tools[:8]:  # ì£¼ìš” ë„êµ¬ë“¤ë§Œ
                desc = getattr(tool, "description", tool.name)
                tool_descriptions.append(f"- {tool.name}: {desc[:100]}")

            tools_info = (
                "\n".join(tool_descriptions)
                if tool_descriptions
                else "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì—†ìŒ"
            )

            # Agent ëª¨ë“œ ì„ íƒ ì‹œ ë” ì ê·¹ì ì¸ íŒë‹¨ ê¸°ì¤€ ì ìš©
            agent_context = ""
            if force_agent:
                agent_context = "\n\nIMPORTANT: The user has specifically selected Agent mode, indicating they want to use available tools when possible. Be more inclined to use tools for information gathering, searches, or data processing tasks."

            decision_prompt = f"""User request: "{user_input}"

Available tools:
{tools_info}

Determine if this request requires using tools to provide accurate information.

Requires tools:
- Real-time information search (web search, news, weather, etc.)
- Database queries (travel products, flights, etc.)
- External API calls (maps, translation, etc.)
- File processing or calculations
- Current time or date-related information
- Specific data lookups or searches
- Location-based queries
- If you request something You don't know, utilize appropriate tools.

Does not require tools:
- General conversation or questions
- Explanations or opinions
- Creative writing or idea suggestions
- General knowledge already known{agent_context}

Answer: YES or NO only."""

            messages = [
                SystemMessage(
                    content="You are an expert at analyzing user requests to determine if tools are needed. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=decision_prompt),
            ]

            llm_start = time.time()
            response = self.llm.invoke(messages)
            llm_elapsed = time.time() - llm_start
            decision = response.content.strip().upper()

            result = "YES" in decision
            mode_info = " (Agent ëª¨ë“œ)" if force_agent else " (Ask ëª¨ë“œ)"
            total_elapsed = time.time() - start_time
            logger.info(
                f"ğŸ¤” ë„êµ¬ ì‚¬ìš© íŒë‹¨{mode_info}: {decision} -> {result} (LLM: {llm_elapsed:.2f}ì´ˆ, ì´: {total_elapsed:.2f}ì´ˆ)"
            )
            return result

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"âŒ ë„êµ¬ ì‚¬ìš© íŒë‹¨ ì˜¤ë¥˜: {elapsed:.2f}ì´ˆ, ì˜¤ë¥˜: {e}")
            return False

    def _create_agent_executor(self) -> AgentExecutor:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„±"""
        if not self.tools:
            return None

        # OpenAI ë„êµ¬ ì—ì´ì „íŠ¸ ìƒì„± (GPT ëª¨ë¸ìš©)
        if not self.model_name.startswith("gemini"):
            from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

            system_message = """You are a helpful AI assistant that can use various tools to provide accurate information.

**Instructions:**
- Analyze user requests carefully to select the most appropriate tools
- Use tools to gather current, accurate information when needed
- Organize information in a clear, logical structure
- Respond in natural, conversational Korean
- Be friendly and helpful while maintaining accuracy
- If multiple tools are needed, use them systematically
- Focus on providing exactly what the user asked for

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment.

**Response Format:**
- Use clear headings and bullet points when appropriate
- Highlight important information
- Keep responses well-organized and easy to read"""

            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system_message),
                    ("human", "{input}"),
                    MessagesPlaceholder(variable_name="agent_scratchpad"),
                ]
            )

            agent = create_openai_tools_agent(self.llm, self.tools, prompt)
            return AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=True,
                max_iterations=2,
                handle_parsing_errors=True,
            )

        # ReAct ì—ì´ì „íŠ¸ ìƒì„± (Gemini ë“± ë‹¤ë¥¸ ëª¨ë¸ìš©)
        else:
            react_prompt = PromptTemplate.from_template(
                """
You are a helpful AI assistant that can use various tools to provide accurate information.

**Instructions:**
- Analyze user requests carefully to select the most appropriate tools
- Use tools to gather current, accurate information when needed
- Organize information in a clear, logical structure
- Respond in natural, conversational Korean
- Be friendly and helpful while maintaining accuracy
- If multiple tools are needed, use them systematically
- Focus on providing exactly what the user asked for

Available tools:
{tools}

Tool names: {tool_names}

Follow this format exactly:

Question: {input}
Thought: I need to analyze this request and determine which tool(s) would be most helpful.
Action: tool_name
Action Input: input_for_tool
Observation: tool_execution_result
Thought: Based on the result, I will provide a comprehensive answer in Korean with clear formatting.
Final Answer: [Provide a well-organized response in Korean with clear headings, bullet points, and highlighted important information]

{agent_scratchpad}
                """
            )

            agent = create_react_agent(self.llm, self.tools, react_prompt)
            return AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=False,
                max_iterations=2,
                handle_parsing_errors=True,
                early_stopping_method="force",
                return_intermediate_steps=False,
            )

    def chat_with_tools(self, user_input: str) -> tuple[str, list]:
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
        import time
        start_time = time.time()
        logger.info(f"ğŸš€ ë„êµ¬ ì±„íŒ… ì‹œì‘: {user_input[:50]}...")
        
        try:
            # í† í° ì œí•œ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ
            if "context_length_exceeded" in str(getattr(self, "_last_error", "")):
                logger.warning("í† í° ì œí•œ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                return self.simple_chat(user_input), []

            # Gemini ëª¨ë¸ì€ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ë°©ì‹ ì‚¬ìš©
            if self.model_name.startswith("gemini"):
                logger.info("ğŸ”§ Gemini ë„êµ¬ ì±„íŒ… ì‹œì‘")
                gemini_start = time.time()
                result = self._gemini_tool_chat(user_input)
                logger.info(f"ğŸ”§ Gemini ë„êµ¬ ì±„íŒ… ì™„ë£Œ: {time.time() - gemini_start:.2f}ì´ˆ")
                return result

            # GPT ëª¨ë¸ì€ ê¸°ì¡´ ì—ì´ì „íŠ¸ ë°©ì‹ ì‚¬ìš©
            if not self.agent_executor:
                logger.info("ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì‹œì‘")
                agent_create_start = time.time()
                self.agent_executor = self._create_agent_executor()
                logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì™„ë£Œ: {time.time() - agent_create_start:.2f}ì´ˆ")

            if not self.agent_executor:
                return "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.", []

            logger.info("ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘")
            agent_invoke_start = time.time()
            result = self.agent_executor.invoke({"input": user_input})
            logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {time.time() - agent_invoke_start:.2f}ì´ˆ")
            output = result.get("output", "")

            # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
            used_tools = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    if len(step) >= 2 and hasattr(step[0], "tool"):
                        used_tools.append(step[0].tool)

            if "Agent stopped" in output or not output.strip():
                logger.warning("ì—ì´ì „íŠ¸ ì¤‘ë‹¨ë¡œ ì¸í•´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                return self.simple_chat(user_input), []

            elapsed = time.time() - start_time
            logger.info(f"âœ… ë„êµ¬ ì±„íŒ… ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            return output, used_tools

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            self._last_error = error_msg
            logger.error(f"âŒ ë„êµ¬ ì‚¬ìš© ì±„íŒ… ì˜¤ë¥˜: {elapsed:.2f}ì´ˆ, ì˜¤ë¥˜: {e}")

            # í† í° ì œí•œ ì˜¤ë¥˜ ì²˜ë¦¬
            if (
                "context_length_exceeded" in error_msg
                or "maximum context length" in error_msg
            ):
                logger.warning("í† í° ì œí•œ ì˜¤ë¥˜ ë°œìƒ, ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                return self.simple_chat(user_input), []

            return self.simple_chat(user_input), []

    def simple_chat(self, user_input: str) -> str:
        """ì¼ë°˜ ì±„íŒ… (ë„êµ¬ ì‚¬ìš© ì—†ìŒ)"""
        try:
            # í†µì¼ëœ ì‹œìŠ¤í…œ ë©”ì‹œì§€ - ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œì— íŠ¹í™”
            system_content = """You are an expert AI assistant specialized in image analysis and text extraction (OCR).

**Primary Mission for Images:**
- **COMPLETE TEXT EXTRACTION**: Extract every single character, number, and symbol from images with 100% accuracy
- **ZERO OMISSIONS**: Never skip or miss any text, no matter how small or unclear
- **PERFECT TRANSCRIPTION**: Reproduce all text exactly as it appears, including spacing and formatting
- **STRUCTURAL ANALYSIS**: Identify tables, lists, headers, paragraphs, and document layout
- **MULTILINGUAL SUPPORT**: Handle Korean, English, numbers, and special characters flawlessly

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment.

**Response Format for Images:**
## ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
[ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ë‚˜ì—´ - ì ˆëŒ€ ëˆ„ë½ ê¸ˆì§€]

## ğŸ“‹ ë¬¸ì„œ êµ¬ì¡°
[í‘œ, ëª©ë¡, ì œëª© ë“±ì˜ êµ¬ì¡° ì„¤ëª…]

## ğŸ“ ë ˆì´ì•„ì›ƒ ì •ë³´
[í…ìŠ¤íŠ¸ ë°°ì¹˜ì™€ ìœ„ì¹˜ ê´€ê³„]

**Critical Rules:**
- NEVER say "í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤" or "ì¶”ì¶œí•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"
- ALWAYS extract something, even if text is small or unclear
- If text is unclear, provide your best interpretation with [ë¶ˆëª…í™•] notation
- Focus on TEXT EXTRACTION as the absolute priority

**For General Questions:**
- Always respond in natural, conversational Korean
- Organize information clearly with headings and bullet points
- Highlight important information using **bold** formatting
- Be friendly, helpful, and accurate"""

            # Gemini ëª¨ë¸ì˜ ê²½ìš° ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¸ê°„ ë©”ì‹œì§€ë¡œ ë³€í™˜
            if self.model_name.startswith("gemini"):
                messages = [HumanMessage(content=system_content)]
            else:
                messages = [SystemMessage(content=system_content)]

            # ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬
            if "[IMAGE_BASE64]" in user_input and "[/IMAGE_BASE64]" in user_input:
                processed_input = self._process_image_input(user_input)
                messages.append(processed_input)
            else:
                messages.append(HumanMessage(content=user_input))

            response = self.llm.invoke(messages)
            return response.content

        except Exception as e:
            logger.error(f"ì¼ë°˜ ì±„íŒ… ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    def simple_chat_with_history(
        self, user_input: str, conversation_history: List[Dict]
    ) -> str:
        """ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì¼ë°˜ ì±„íŒ…"""
        try:
            logger.info(
                f"íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ì±„íŒ… ì‹œì‘: {len(conversation_history)}ê°œ ë©”ì‹œì§€"
            )

            messages = self._convert_history_to_messages(conversation_history)

            # ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬
            if "[IMAGE_BASE64]" in user_input and "[/IMAGE_BASE64]" in user_input:
                processed_input = self._process_image_input(user_input)
                messages.append(processed_input)
            else:
                messages.append(HumanMessage(content=user_input))

            logger.info(f"ìµœì¢… ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")

            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            logger.error(f"ëŒ€í™” ê¸°ë¡ ì±„íŒ… ì˜¤ë¥˜: {e}")
            return self.simple_chat(user_input)

    def _process_image_input(self, user_input: str):
        """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        import re
        import base64

        # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        image_match = re.search(
            r"\[IMAGE_BASE64\](.*?)\[/IMAGE_BASE64\]", user_input, re.DOTALL
        )
        if not image_match:
            return HumanMessage(content=user_input)

        image_data = image_match.group(1).strip()
        text_content = user_input.replace(image_match.group(0), "").strip()

        # Base64 ë°ì´í„° ê²€ì¦
        try:
            base64.b64decode(image_data)
        except Exception as e:
            logger.error(f"ì˜ëª»ëœ Base64 ì´ë¯¸ì§€ ë°ì´í„°: {e}")
            return HumanMessage(content="ì˜ëª»ëœ ì´ë¯¸ì§€ ë°ì´í„°ì…ë‹ˆë‹¤.")

        # í…ìŠ¤íŠ¸ ì¶”ì¶œì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸
        if not text_content:
            text_content = """ì´ ì´ë¯¸ì§€ì—ì„œ **ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œ(OCR)**í•´ì£¼ì„¸ìš”.

**í•„ìˆ˜ ì‘ì—…:**
1. **ì™„ì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ì´ë¯¸ì§€ ë‚´ ëª¨ë“  í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°í˜¸ë¥¼ ë¹ ì§ì—†ì´ ì¶”ì¶œ
2. **êµ¬ì¡° ë¶„ì„**: í‘œ, ëª©ë¡, ì œëª©, ë‹¨ë½ ë“±ì˜ ë¬¸ì„œ êµ¬ì¡° íŒŒì•…
3. **ë ˆì´ì•„ì›ƒ ì •ë³´**: í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜, í¬ê¸°, ë°°ì¹˜ ê´€ê³„ ì„¤ëª…
4. **ì •í™•í•œ ì „ì‚¬**: ì˜¤íƒ€ ì—†ì´ ì •í™•í•˜ê²Œ ëª¨ë“  ë¬¸ì ê¸°ë¡
5. **ë§¥ë½ ì„¤ëª…**: ë¬¸ì„œì˜ ì¢…ë¥˜ì™€ ëª©ì  íŒŒì•…
6. **í…Œì´ë¸” í¬ë§·**: í‘œë¥¼ ë§Œë“¤ ë•ŒëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì‚¬ìš©: |Header1|Header2|\n|---|---|\n|Data1|Data2|

**ì‘ë‹µ í˜•ì‹:**
## ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
[ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ë‚˜ì—´]

## ğŸ“‹ ë¬¸ì„œ êµ¬ì¡°
[í‘œ, ëª©ë¡, ì œëª© ë“±ì˜ êµ¬ì¡° ì„¤ëª…]

## ğŸ“ ë ˆì´ì•„ì›ƒ ì •ë³´
[í…ìŠ¤íŠ¸ ë°°ì¹˜ì™€ ìœ„ì¹˜ ê´€ê³„]

**ì¤‘ìš”**: ì´ë¯¸ì§€ì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì ˆëŒ€ ëˆ„ë½í•˜ì§€ ë§ê³  ì™„ì „íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

        try:
            # Gemini ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ í˜•ì‹ ì‚¬ìš©
            if self.model_name.startswith("gemini"):
                # Gemini 2.0 Flashì— ìµœì í™”ëœ í˜•ì‹
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": f"data:image/jpeg;base64,{image_data}",
                        },
                    ]
                )
            else:
                # OpenAI GPT-4V í˜•ì‹
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            },
                        },
                    ]
                )
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return HumanMessage(
                content=f"{text_content}\n\n[ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}]"
            )

    def process_message(self, user_input: str) -> tuple[str, list]:
        """ë©”ì‹œì§€ ì²˜ë¦¬ - AIê°€ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.conversation_history.add_message("user", user_input)

        if not self.tools:
            response = self.simple_chat_with_history(
                user_input, self.conversation_history.get_recent_messages(10)
            )
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, []

        # AIê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_tools = self._should_use_tools(user_input)

        if use_tools:
            response, used_tools = self.chat_with_tools(user_input)
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, used_tools
        else:
            response = self.simple_chat_with_history(
                user_input, self.conversation_history.get_recent_messages(10)
            )
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, []

    def process_message_with_history(
        self,
        user_input: str,
        conversation_history: List[Dict],
        force_agent: bool = False,
    ) -> tuple[str, list]:
        """ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ë©”ì‹œì§€ ì²˜ë¦¬"""
        if not self.tools:
            response = self.simple_chat_with_history(user_input, conversation_history)
            return response, []

        # force_agentê°€ Trueë©´ ë” ì ê·¹ì ìœ¼ë¡œ ë„êµ¬ ì‚¬ìš© íŒë‹¨
        use_tools = self._should_use_tools(user_input, force_agent)

        if use_tools:
            response, used_tools = self.chat_with_tools(user_input)
            return response, used_tools
        else:
            response = self.simple_chat_with_history(user_input, conversation_history)
            return response, []

    def _convert_history_to_messages(self, conversation_history: List[Dict]):
        """ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜ - í† í° ì œí•œ ê³ ë ¤"""
        messages = []

        # í†µì¼ëœ ì‹œìŠ¤í…œ ë©”ì‹œì§€ - íˆìŠ¤í† ë¦¬ í™œìš© ê°•ì¡°
        unified_system_content = """You are a helpful AI assistant that provides contextual responses based on conversation history.

**Response Guidelines:**
- Always respond in natural, conversational Korean
- Use conversation history to provide relevant, contextual answers
- Organize information clearly with headings and bullet points when appropriate
- Highlight important information using **bold** formatting
- Be friendly, helpful, and maintain conversation flow
- Reference previous context when relevant
- Provide comprehensive, well-structured responses

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment."""
        
        if self.model_name.startswith("gemini"):
            messages.append(HumanMessage(content=unified_system_content))
        else:
            messages.append(SystemMessage(content=unified_system_content))

        # ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì‚¬ìš© (ë” ë§ì´ í¬í•¨)
        recent_history = (
            conversation_history[-6:]
            if len(conversation_history) > 6
            else conversation_history
        )

        for msg in recent_history:
            role = msg.get("role", "")
            content = msg.get("content", "")[:500]  # ë‚´ìš© ì œí•œ ì¦ê°€
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                from langchain.schema import AIMessage

                messages.append(AIMessage(content=content))

        logger.info(
            f"íˆìŠ¤í† ë¦¬ ë³€í™˜ ì™„ë£Œ: {len(recent_history)}ê°œ ë©”ì‹œì§€ -> {len(messages)}ê°œ LangChain ë©”ì‹œì§€"
        )
        return messages

    def _gemini_tool_chat(self, user_input: str) -> tuple[str, list]:
        """ê²Œë¯¸ë‹ˆ ëª¨ë¸ìš© AI ê¸°ë°˜ ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰ - ì—°ì‡„ì  ë„êµ¬ ì‚¬ìš© ì§€ì›"""
        try:
            if self._is_tool_list_request(user_input):
                return self._show_tool_list(), []

            # ì—°ì‡„ì  ë„êµ¬ ì‚¬ìš© ì‹œì‘
            return self._execute_tool_chain(user_input)

        except Exception as e:
            logger.error(f"ê²Œë¯¸ë‹ˆ ë„êµ¬ ì±„íŒ… ì˜¤ë¥˜: {e}")
            return self.simple_chat(user_input), []

    def _get_realistic_date_range(self, user_input: str) -> tuple[str, str]:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ í˜„ì‹¤ì ì¸ ë‚ ì§œ ë²”ìœ„ ìƒì„± - ë²”ìš©ì  ì ‘ê·¼"""
        # íŠ¹ì • API í˜•ì‹ì— ì˜ì¡´í•˜ì§€ ì•Šê³  AIê°€ ì ì ˆí•œ í˜•ì‹ì„ ê²°ì •í•˜ë„ë¡ í•¨
        # ë‚ ì§œ íŒŒì‹±ì€ ê° ë„êµ¬ì˜ ìŠ¤í‚¤ë§ˆì— ë”°ë¼ AIê°€ ì²˜ë¦¬
        return None, None  # AIê°€ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì²˜ë¦¬í•˜ë„ë¡ í•¨

    def _get_area_code(self, user_input: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì§€ì—­ ì½”ë“œ ì¶”ì¶œ - AIê°€ ë™ì ìœ¼ë¡œ ê²°ì •"""
        # AIê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í†µí•´ ì§€ì—­ ì½”ë“œë¥¼ ë™ì ìœ¼ë¡œ ì¡°íšŒí•˜ë„ë¡ ë³€ê²½
        # í•˜ë“œì½”ë”©ëœ ë§¤í•‘ ì œê±°í•˜ê³  ë²”ìš©ì  ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©
        return None  # AIê°€ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì²˜ë¦¬í•˜ë„ë¡ í•¨

    def _ai_select_tool(self, user_input: str):
        """ì´ˆê¸° ë„êµ¬ ì„ íƒ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)"""
        return self._ai_select_next_tool(user_input, [], 0)

    def _legacy_ai_select_tool(self, user_input: str):
        """AIê°€ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë„êµ¬ì™€ íŒŒë¼ë¯¸í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°ì •"""
        try:
            if not self.tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None

            # ë„êµ¬ ì„¤ëª… ìˆ˜ì§‘ (ìƒì„¸ ì •ë³´ í¬í•¨)
            tools_info = []
            for tool in self.tools:
                desc = getattr(tool, "description", tool.name)
                # ë„êµ¬ ìŠ¤í‚¤ë§ˆ ì •ë³´ë„ í¬í•¨
                schema_info = ""
                if hasattr(tool, "args_schema") and tool.args_schema:
                    try:
                        schema = tool.args_schema.schema()
                        if "properties" in schema:
                            params = list(schema["properties"].keys())[
                                :3
                            ]  # ì£¼ìš” íŒŒë¼ë¯¸í„°ë§Œ
                            schema_info = f" (íŒŒë¼ë¯¸í„°: {', '.join(params)})"
                    except:
                        pass
                tools_info.append(f"- {tool.name}: {desc}{schema_info}")

            tools_list = "\n".join(tools_info)

            selection_prompt = f"""User request: "{user_input}"

Available tools:
{tools_list}

Analyze the user's request and select the most appropriate tool with necessary parameters.

Response format:
- Use tool: TOOL: tool_name | PARAMS: {{"key": "value"}}
- No tool needed: TOOL: none

Examples:
- Web search: TOOL: search | PARAMS: {{"query": "search_term"}}
- Travel products: TOOL: retrieveSaleProductInformation | PARAMS: {{"startDate": "20240301", "endDate": "20240310", "productAreaCode": "A0"}}

Extract parameter values from user request or use reasonable defaults."""

            messages = [
                SystemMessage(
                    content="You are an expert at analyzing user requests to select the optimal tool. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=selection_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._parse_tool_decision(response.content)

        except Exception as e:
            logger.error(f"AI ë„êµ¬ ì„ íƒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_tool_decision(self, response: str):
        """ë„êµ¬ ì„ íƒ ì‘ë‹µ íŒŒì‹±"""
        try:
            import re
            import json

            logger.info(f"íŒŒì‹±í•  ì‘ë‹µ: {response}")

            # "TOOL: toolname | PARAMS: {...}" í˜•ì‹ íŒŒì‹±
            match = re.search(
                r"TOOL:\s*([^|\n]+)(?:\s*\|\s*PARAMS:\s*([^\n]+))?",
                response,
                re.IGNORECASE,
            )
            if not match:
                logger.warning(f"ë„êµ¬ í˜•ì‹ ë§¤ì¹­ ì‹¤íŒ¨: {response}")
                return None

            tool_name = match.group(1).strip()
            params_str = match.group(2).strip() if match.group(2) else "{}"

            logger.info(f"íŒŒì‹±ëœ ë„êµ¬ëª…: '{tool_name}', íŒŒë¼ë¯¸í„°: '{params_str}'")

            if tool_name.lower() == "none":
                return {"tool": "none"}

            try:
                params = json.loads(params_str)
            except json.JSONDecodeError as je:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {params_str}, ì˜¤ë¥˜: {je}")
                params = {}

            return {"tool": tool_name, "params": params}

        except Exception as e:
            logger.error(f"ë„êµ¬ ê²°ì • íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _split_large_response(
        self, response_text: str, max_chunk_size: int = 3000
    ) -> List[str]:
        """í° ì‘ë‹µì„ ì²­í¬ë¡œ ë¶„í• """
        if len(response_text) <= max_chunk_size:
            return [response_text]

        chunks = []
        current_pos = 0

        while current_pos < len(response_text):
            # JSON êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í•  ì§€ì  ì°¾ê¸°
            end_pos = min(current_pos + max_chunk_size, len(response_text))

            # JSON ê°ì²´ë‚˜ ë°°ì—´ ëì—ì„œ ë¶„í• 
            if end_pos < len(response_text):
                # ë’¤ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ }, ] ì°¾ê¸°
                for i in range(end_pos, current_pos, -1):
                    if response_text[i] in ["}", "]", ","]:
                        end_pos = i + 1
                        break

            chunk = response_text[current_pos:end_pos]
            chunks.append(chunk)
            current_pos = end_pos

        return chunks

    def _process_chunked_response(self, large_response: str, user_query: str) -> str:
        """ëŒ€ìš©ëŸ‰ ì‘ë‹µì„ ì „ì²´ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë§Œ ë³´ì—¬ì£¼ê¸°"""
        logger.info(f"ëŒ€ìš©ëŸ‰ ì‘ë‹µ ë¶„ì„ ì‹œì‘: {len(large_response)}ì")

        # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¶„ì„
        analysis_prompt = f"""User query: {user_query}
                            Data:
                            {large_response}
                            Analyze the above data and provide useful information to the user in Korean. Don't mention chunk numbers or analysis progress, just show the results."""

        messages = [
            SystemMessage(
                content="You are an expert who analyzes data and provides useful information to users in Korean. Don't mention the analysis process, just show the results."
            ),
            HumanMessage(content=analysis_prompt),
        ]

        analysis_response = self.llm.invoke(messages)
        return analysis_response.content

    def _execute_selected_tool(self, tool_decision):
        """ì„ íƒëœ ë„êµ¬ ì‹¤í–‰"""
        try:
            tool_name = tool_decision["tool"]
            params = tool_decision.get("params", {})

            # ë„êµ¬ ìŠ¤í‚¤ë§ˆì— ë”°ë¼ íŒŒë¼ë¯¸í„° ìë™ ë³€í™˜
            selected_tool = None
            for tool in self.tools:
                if tool.name.lower() == tool_name.lower():
                    selected_tool = tool
                    break

            if (
                selected_tool
                and hasattr(selected_tool, "args_schema")
                and selected_tool.args_schema
            ):
                try:
                    schema = selected_tool.args_schema.schema()
                    if "properties" in schema:
                        for param_name, param_value in params.items():
                            if param_name in schema["properties"]:
                                param_schema = schema["properties"][param_name]
                                if param_schema.get("type") == "array" and isinstance(
                                    param_value, str
                                ):
                                    params[param_name] = [param_value]
                except:
                    pass

            logger.info(f"ë„êµ¬ ì°¾ê¸°: '{tool_name}'")

            # ë„êµ¬ ì°¾ê¸° (ë” ìœ ì—°í•œ ë§¤ì¹­)
            selected_tool = None
            for tool in self.tools:
                tool_name_lower = tool_name.lower()
                actual_name_lower = tool.name.lower()

                # ì •í™•í•œ ì´ë¦„ ë§¤ì¹­ ë˜ëŠ” ë¶€ë¶„ ë§¤ì¹­
                if (
                    tool_name_lower == actual_name_lower
                    or tool_name_lower in actual_name_lower
                    or actual_name_lower in tool_name_lower
                    or tool_name_lower.replace("_", "-")
                    == actual_name_lower.replace("_", "-")
                ):
                    selected_tool = tool
                    logger.info(f"ë„êµ¬ ë§¤ì¹­ ì„±ê³µ: '{tool_name}' -> '{tool.name}'")
                    break

            if not selected_tool:
                available_tools = [t.name for t in self.tools]
                logger.error(
                    f"ë„êµ¬ '{tool_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {available_tools}"
                )
                return f"ë„êµ¬ '{tool_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {available_tools}"

            # ê²€ìƒ‰ ë„êµ¬ì˜ ê²½ìš° ë” ë§ì€ ê²°ê³¼ ìš”ì²­
            if "search" in selected_tool.name.lower():
                if "limit" not in params and "maxResults" not in params:
                    params["maxResults"] = 10  # ê¸°ë³¸ 10ê°œ ê²°ê³¼
                if "includeHtml" not in params:
                    params["includeHtml"] = False

            # ë„êµ¬ ì‹¤í–‰ (GPT ìŠ¤íƒ€ì¼ ë¡œê¹…)
            print(f"\n> Invoking: `{selected_tool.name}` with `{params}`\n")

            result = selected_tool.invoke(params)

            # ê²°ê³¼ ì¶œë ¥
            print(result)
            print("\n")

            # ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ëŒ€ì•ˆ ì œì‹œ
            if isinstance(result, str) and (
                '"resultCount": 0' in result
                or '"saleProductItemResponseList": []' in result
            ):
                logger.warning(f"ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ: {selected_tool.name}")
                return f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚ ì§œë‚˜ ì§€ì—­ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.\n\nì›ë³¸ ê²°ê³¼: {result}"

            logger.info(f"ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: {str(result)[:200]}...")

            # ëŒ€ìš©ëŸ‰ ì‘ë‹µ ì²˜ë¦¬
            if isinstance(result, str) and len(result) > 5000:
                logger.info(f"ëŒ€ìš©ëŸ‰ ì‘ë‹µ ê°ì§€: {len(result)}ì")
                original_query = (
                    tool_decision.get("original_query", "")
                    if isinstance(tool_decision, dict)
                    else ""
                )
                return self._process_chunked_response(result, original_query)

            return result

        except Exception as e:
            error_msg = str(e)
            logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

            # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ ì²˜ë¦¬
            if "timeout" in error_msg.lower() or "MCP ì‘ë‹µ íƒ€ì„ì•„ì›ƒ" in error_msg:
                return f"ë„êµ¬ í˜¸ì¶œ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

            return f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def _is_tool_list_request(self, user_input: str) -> bool:
        """ë„êµ¬ ëª©ë¡ ìš”ì²­ì¸ì§€ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥´ê²Œ íŒë‹¨"""
        try:
            # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ê°€
            keywords = [
                'ë„êµ¬', 'íˆ´', 'tool', 'mcp', 'ê¸°ëŠ¥', 'ì—­ëŠ¥', 'ëª©ë¡', 'list',
                'í™œì„±í™”', 'active', 'ì‚¬ìš©ê°€ëŠ¥', 'available', 'ì–´ë–¤', 'what',
                'ë³´ì—¬', 'show', 'ì•Œë ¤', 'tell', 'ì‚¬ìš©í• ìˆ˜ìˆ', 'can use'
            ]
            
            user_lower = user_input.lower()
            
            # ë„êµ¬ ê´€ë ¨ í‚¤ì›Œë“œ ì¡°í•© ê²€ì‚¬
            tool_keywords = ['ë„êµ¬', 'íˆ´', 'tool', 'mcp']
            list_keywords = ['ëª©ë¡', 'list', 'ë³´ì—¬', 'show', 'ì•Œë ¤', 'tell']
            
            has_tool_keyword = any(keyword in user_lower for keyword in tool_keywords)
            has_list_keyword = any(keyword in user_lower for keyword in list_keywords)
            
            # ë„êµ¬ + ëª©ë¡ í‚¤ì›Œë“œ ì¡°í•©ì´ ìˆìœ¼ë©´ ë„êµ¬ ëª©ë¡ ìš”ì²­ìœ¼ë¡œ íŒë‹¨
            if has_tool_keyword and has_list_keyword:
                return True
                
            # 'í™œì„±í™”ëœ ë„êµ¬' ê°™ì€ íŠ¹ì • í‘œí˜„ ê²€ì‚¬
            specific_phrases = [
                'í™œì„±í™”ëœ ë„êµ¬', 'ë„êµ¬ ëª©ë¡', 'ì‚¬ìš©ê°€ëŠ¥í•œ ë„êµ¬',
                'active tool', 'tool list', 'available tool', 'mcp tool'
            ]
            
            return any(phrase in user_lower for phrase in specific_phrases)

        except Exception as e:
            logger.error(f"ë„êµ¬ ëª©ë¡ ìš”ì²­ íŒë‹¨ ì˜¤ë¥˜: {e}")
            return False

    def _show_tool_list(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ë°˜í™˜"""
        if not self.tools:
            return "í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì„œë²„ë³„ë¡œ ë„êµ¬ ë¶„ë¥˜
        tools_by_server = {}
        for tool in self.tools:
            server_name = getattr(tool, "server_name", "Unknown")
            if server_name not in tools_by_server:
                tools_by_server[server_name] = []
            tools_by_server[server_name].append(tool)

        # ë„êµ¬ ëª©ë¡ ìƒì„±
        result = ["ğŸ”§ **ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡**\n"]

        for server_name, server_tools in tools_by_server.items():
            result.append(f"ğŸ“¦ **{server_name}** ({len(server_tools)}ê°œ ë„êµ¬):")
            for tool in server_tools:
                desc = getattr(tool, "description", tool.name)
                # ì „ì²´ ì„¤ëª… í‘œì‹œ
                result.append(f"  â€¢ {tool.name}: {desc}")
            result.append("")  # ë¹ˆ ì¤„ ì¶”ê°€

        result.append(f"ì´ {len(self.tools)}ê°œì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return "\n".join(result)

    def _format_response(self, text: str) -> str:
        """ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬"""
        if not text:
            return text

        import re

        # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬
        formatted = re.sub(r"\n{3,}", "\n\n", text)

        # ë¶ˆê·œì¹™í•œ ë“¤ì—¬ì“°ê¸° ì •ë¦¬ - 5ê°œ ì´ìƒì˜ ê³µë°±ì„ 4ê°œë¡œ í†µì¼
        formatted = re.sub(r"\n\s{5,}", "\n    ", formatted)

        formatted = formatted.strip()
        return formatted

    def _generate_final_response(self, user_input: str, tool_result: str):
        """ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„± - í† í° ì œí•œ ê³ ë ¤"""
        try:
            # ë„êµ¬ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
            if len(tool_result) > 2000:
                tool_result = tool_result[:2000] + "...(ìƒëµ)"

            response_prompt = f"""User asked: "{user_input}"

Data from tools:
{tool_result}

Your task:
1. Extract the most relevant information that directly answers the user's question
2. Organize the information in a logical, easy-to-follow structure
3. Write in conversational Korean as if explaining to a friend
4. Focus on what the user actually needs to know
5. Use simple, clear sentences without technical jargon
6. If there are multiple pieces of information, prioritize the most important ones first
7. Format your response using simple markdown (## for headings, **bold** for emphasis, - for bullet points)
8. Keep formatting minimal and clean
9. Use consistent indentation for lists and structured data

Provide a helpful, natural Korean response in markdown format that directly addresses what the user wanted to know."""

            unified_system_message = """You are a helpful AI assistant that analyzes tool results and provides comprehensive responses.

**Response Guidelines:**
- Always respond in natural, conversational Korean
- Organize information clearly with headings and bullet points
- Highlight important information using **bold** formatting
- Extract and present the most relevant information
- Use simple, clear sentences without technical jargon
- Structure responses logically and systematically
- Focus on what the user actually needs to know

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment."""
            
            messages = [
                SystemMessage(content=unified_system_message),
                HumanMessage(content=response_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._format_response(response.content)

        except Exception as e:
            error_msg = str(e)
            logger.error(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")

            # í† í° ì œí•œ ì˜¤ë¥˜ ì²˜ë¦¬
            if (
                "context_length_exceeded" in error_msg
                or "maximum context length" in error_msg
            ):
                return self._format_response(
                    "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ì§€ë§Œ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                )

            return self._format_response(
                "ë„êµ¬ ì‹¤í–‰ì€ ì„±ê³µí–ˆì§€ë§Œ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )

    def _execute_tool_chain(
        self, user_input: str, max_iterations: int = 3
    ) -> tuple[str, list]:
        """ì—°ì‡„ì  ë„êµ¬ ì‚¬ìš© ì‹¤í–‰"""
        all_used_tools = []
        accumulated_results = []
        current_query = user_input

        for iteration in range(max_iterations):
            logger.info(f"ë„êµ¬ ì²´ì¸ {iteration + 1}ë‹¨ê³„ ì‹œì‘: {current_query[:50]}...")

            # AIê°€ ë‹¤ìŒ ë„êµ¬ ê²°ì •
            tool_decision = self._ai_select_next_tool(
                current_query, accumulated_results, iteration
            )

            if not tool_decision or tool_decision.get("tool") == "none":
                logger.info(f"ë„êµ¬ ì²´ì¸ {iteration + 1}ë‹¨ê³„ì—ì„œ ì¢…ë£Œ")
                break

            # ë„êµ¬ ì‹¤í–‰
            tool_result = self._execute_selected_tool(tool_decision)
            used_tool_name = tool_decision.get("tool", "")

            if used_tool_name:
                all_used_tools.append(used_tool_name)

            accumulated_results.append(
                {
                    "step": iteration + 1,
                    "tool": used_tool_name,
                    "query": current_query,
                    "result": tool_result,
                }
            )

            # ë‹¤ìŒ ë‹¨ê³„ ì§ˆì˜ ìƒì„±
            next_query = self._generate_next_query(user_input, accumulated_results)
            if not next_query or next_query == current_query:
                logger.info(f"ë‹¤ìŒ ë‹¨ê³„ ì§ˆì˜ê°€ ì—†ì–´ ì¢…ë£Œ")
                break

            current_query = next_query

        # ìµœì¢… ì‘ë‹µ ìƒì„±
        final_response = self._generate_chain_response(user_input, accumulated_results)
        return final_response, all_used_tools

    def _ai_select_next_tool(
        self, current_query: str, previous_results: list, step: int
    ):
        """ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë„êµ¬ë¥¼ AIê°€ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°ì •"""
        try:
            # ë„êµ¬ ì„¤ëª… ìˆ˜ì§‘
            tools_info = []
            for tool in self.tools:
                desc = getattr(tool, "description", tool.name)
                tools_info.append(f"- {tool.name}: {desc}")

            tools_list = "\n".join(tools_info)

            # ì´ì „ ê²°ê³¼ ìš”ì•½
            previous_summary = ""
            if previous_results:
                previous_summary = "\n\nPrevious steps:\n"
                for result in previous_results:
                    result_preview = (
                        str(result["result"])[:200] + "..."
                        if len(str(result["result"])) > 200
                        else str(result["result"])
                    )
                    previous_summary += f"Step {result['step']}: Used {result['tool']} -> {result_preview}\n"

            selection_prompt = f"""Current query: "{current_query}"
Step: {step + 1}

{previous_summary}

Available tools:
{tools_list}

Analyze the current query and previous results. Determine if additional information is needed to fully answer the user's request.

Think about:
- What information does the user ultimately want?
- What gaps exist in the current results?
- Which tool could provide the missing information?
- Are the results sufficient to answer the original question?

Response format:
- Use tool: TOOL: tool_name | PARAMS: {{"key": "value"}}
- No more tools needed: TOOL: none

Use your judgment to select the most appropriate tool and extract relevant parameters from the available information."""

            messages = [
                SystemMessage(
                    content="You are an intelligent assistant that can analyze information gaps and select appropriate tools to gather missing data. Use your reasoning to determine what additional information would be valuable to complete the user's request. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=selection_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._parse_tool_decision(response.content)

        except Exception as e:
            logger.error(f"AI ë‹¤ìŒ ë„êµ¬ ì„ íƒ ì˜¤ë¥˜: {e}")
            return None

    def _generate_next_query(
        self, original_query: str, accumulated_results: list
    ) -> str:
        """ë‹¤ìŒ ë‹¨ê³„ ì§ˆì˜ ìƒì„±"""
        try:
            results_summary = "\n".join(
                [
                    f"Step {r['step']}: {r['tool']} -> {str(r['result'])[:300]}..."
                    for r in accumulated_results
                ]
            )

            prompt = f"""Original user query: "{original_query}"

Results so far:
{results_summary}

Analyze what information is still missing to fully satisfy the user's request. 

If additional specific information is needed, generate a focused query for the next step.
If the current results are sufficient to answer the original query, respond with "COMPLETE".

Next query:"""

            messages = [
                SystemMessage(
                    content="You are an intelligent assistant that can identify information gaps and determine what additional data is needed to complete a user's request. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=prompt),
            ]

            response = self.llm.invoke(messages)
            next_query = response.content.strip()

            if "COMPLETE" in next_query.upper():
                return None

            return next_query

        except Exception as e:
            logger.error(f"ë‹¤ìŒ ì§ˆì˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

    def _generate_chain_response(
        self, original_query: str, accumulated_results: list
    ) -> str:
        """ì—°ì‡„ì  ë„êµ¬ ì‚¬ìš© ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±"""
        try:
            if not accumulated_results:
                return self.simple_chat(original_query)

            # ëª¨ë“  ê²°ê³¼ í•©ì¹˜ê¸°
            all_results = "\n\n".join(
                [
                    f"Step {r['step']} ({r['tool']}):\n{r['result']}"
                    for r in accumulated_results
                ]
            )

            response_prompt = f"""User's original request: "{original_query}"

Information gathered through multiple tools:
{all_results}

Your task:
1. Synthesize all the information to provide a comprehensive answer
2. Organize the information logically and clearly
3. Focus on what the user actually wanted to know
4. Present the information in a natural, conversational Korean format
5. If location information is available, include addresses and coordinates
6. If business information is available, include names, addresses, and contact details

Provide a helpful, well-organized response in Korean that directly addresses the user's original request."""

            messages = [
                SystemMessage(
                    content="You are an expert at synthesizing information from multiple sources to provide comprehensive answers in Korean. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=response_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._format_response(response.content)

        except Exception as e:
            logger.error(f"ì²´ì¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._format_response(
                "ì—¬ëŸ¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆì§€ë§Œ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
