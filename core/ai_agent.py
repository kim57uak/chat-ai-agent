from typing import List, Dict, Any, Optional
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.agents import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_perplexity import ChatPerplexity
from core.perplexity_wrapper import PerplexityWrapper
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain.tools import BaseTool
from tools.langchain.langchain_tools import MCPTool, MCPToolRegistry
from core.mcp_interface import MCPToolCaller
from mcp.servers.mcp import get_all_mcp_tools
from mcp.tools.tool_manager import tool_manager, ToolCategory
from core.conversation_history import ConversationHistory
from core.message_validator import MessageValidator
from core.enhanced_system_prompts import SystemPrompts
import logging
import time
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class AIAgent:
    """AI ÏóêÏù¥Ï†ÑÌä∏ - ÎèÑÍµ¨ ÏÇ¨Ïö© Ïó¨Î∂ÄÎ•º Í≤∞Ï†ïÌïòÍ≥† Ïã§Ìñâ"""

    def __init__(self, api_key: str, model_name: str = "gpt-3.5-turbo"):
        self.api_key = api_key
        self.model_name = model_name
        self.llm = self._create_llm()
        self.tools: List[MCPTool] = []
        self.agent_executor: Optional[AgentExecutor] = None
        self.conversation_history = ConversationHistory()

        # MCP ÎèÑÍµ¨ Î°úÎìú Î∞è Îì±Î°ù
        self._load_mcp_tools()
        # Í∏∞Ï°¥ ÌûàÏä§ÌÜ†Î¶¨ Î°úÎìú
        self.conversation_history.load_from_file()

    def _create_llm(self):
        """LLM Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±"""
        if self.model_name.startswith("gemini"):
            # Gemini Î™®Îç∏Ïóê Î©ÄÌã∞Î™®Îã¨ ÏßÄÏõê ÌôúÏÑ±Ìôî Î∞è Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏµúÏ†ÅÌôî
            return ChatGoogleGenerativeAI(
                model=self.model_name,
                google_api_key=self.api_key,
                temperature=0.1,  # Ï†ïÌôïÌïú ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÏùÑ ÏúÑÌï¥ ÎÇÆÏùÄ Ïò®ÎèÑ
                convert_system_message_to_human=True,  # ÏãúÏä§ÌÖú Î©îÏãúÏßÄÎ•º Ïù∏Í∞Ñ Î©îÏãúÏßÄÎ°ú Î≥ÄÌôò
                max_tokens=4096,  # Ï∂©Î∂ÑÌïú ÌÜ†ÌÅ∞ Ìï†Îãπ
            )
        elif (
            "sonar" in self.model_name
            or "r1-" in self.model_name
            or "perplexity" in self.model_name
        ):
            # Perplexity Î™®Îç∏Ïóê ÌäπÎ≥ÑÌïú ÎûòÌçº ÌÅ¥ÎûòÏä§ ÏÇ¨Ïö©
            logger.info(f"Perplexity Î™®Îç∏ ÏÇ¨Ïö©: {self.model_name}")
            return PerplexityWrapper(
                model=self.model_name,
                pplx_api_key=self.api_key,
                temperature=0.1,
                max_tokens=4096,
                request_timeout=120,
            )
        else:
            return ChatOpenAI(
                model=self.model_name,
                openai_api_key=self.api_key,
                temperature=0.1,
                max_tokens=4096,
            )

    def _load_mcp_tools(self):
        """MCP ÎèÑÍµ¨ Î°úÎìú Î∞è LangChain ÎèÑÍµ¨Î°ú Îì±Î°ù"""
        try:
            # Ïã§Ï†ú MCP ÎèÑÍµ¨ Ìò∏Ï∂úÏûê ÏÇ¨Ïö©
            from core.mcp_implementation import mcp_tool_caller

            tool_registry = MCPToolRegistry(mcp_tool_caller)

            all_mcp_tools = get_all_mcp_tools()
            if all_mcp_tools:
                # Î™®Îì† ÎèÑÍµ¨ Îì±Î°ù
                self.tools = tool_registry.register_mcp_tools(all_mcp_tools)
                tool_manager.register_tools(all_mcp_tools)
                logger.info(f"AI ÏóêÏù¥Ï†ÑÌä∏Ïóê {len(self.tools)}Í∞ú ÎèÑÍµ¨ Î°úÎìúÎê®")
            else:
                logger.warning("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú MCP ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§")
        except Exception as e:
            logger.error(f"MCP ÎèÑÍµ¨ Î°úÎìú Ïã§Ìå®: {e}")
            # Ìè¥Î∞±: Ïã§Ï†ú ÎèÑÍµ¨ Ìò∏Ï∂ú Í∏∞Îä•Ïù¥ ÏûàÎäî MCP ÎèÑÍµ¨ Ìò∏Ï∂úÏûê ÏÉùÏÑ±
            try:
                class SimpleMCPToolCaller(MCPToolCaller):
                    def call_tool(self, server_name, tool_name, arguments=None):
                        from mcp.servers.mcp import call_mcp_tool
                        return call_mcp_tool(server_name, tool_name, arguments)
                    
                    def get_all_tools(self):
                        return get_all_mcp_tools()
                
                # ÎèÑÍµ¨ Î†àÏßÄÏä§Ìä∏Î¶¨ ÏÉùÏÑ±
                mcp_caller = SimpleMCPToolCaller()
                tool_registry = MCPToolRegistry(mcp_caller)
                
                all_mcp_tools = get_all_mcp_tools()
                if all_mcp_tools:
                    # Î™®Îì† ÎèÑÍµ¨ Îì±Î°ù
                    self.tools = tool_registry.register_mcp_tools(all_mcp_tools)
                    tool_manager.register_tools(all_mcp_tools)
                    logger.info(f"Ìè¥Î∞± Î∞©ÏãùÏúºÎ°ú AI ÏóêÏù¥Ï†ÑÌä∏Ïóê {len(self.tools)}Í∞ú ÎèÑÍµ¨ Î°úÎìúÎê®")
                else:
                    logger.warning("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú MCP ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§")
            except Exception as fallback_error:
                logger.error(f"Ìè¥Î∞± MCP ÎèÑÍµ¨ Î°úÎìú Ïã§Ìå®: {fallback_error}")
                self.tools = []

    def _should_use_tools(self, user_input: str, force_agent: bool = False) -> bool:
        """AIÍ∞Ä ÏûêÏó∞Ïñ¥Î•º Ïù¥Ìï¥ÌïòÏó¨ ÎèÑÍµ¨ ÏÇ¨Ïö© Ïó¨Î∂ÄÎ•º ÏßÄÎä•Ï†ÅÏúºÎ°ú Í≤∞Ï†ï"""
        import time

        start_time = time.time()
        logger.info(f"ü§î ÎèÑÍµ¨ ÏÇ¨Ïö© ÌåêÎã® ÏãúÏûë: {user_input[:30]}...")

        # Perplexity Î™®Îç∏Ïùò Í≤ΩÏö∞ Ìï≠ÏÉÅ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÎèÑÎ°ù Í∞ïÏ†ú
        if (
            "sonar" in self.model_name
            or "r1-" in self.model_name
            or "perplexity" in self.model_name
        ):
            logger.info("üîß Perplexity Î™®Îç∏ÏùÄ Ìï≠ÏÉÅ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÎèÑÎ°ù Í∞ïÏ†úÌï©ÎãàÎã§")
            return True

        # Agent Î™®ÎìúÍ∞Ä Í∞ïÏ†úÎêú Í≤ΩÏö∞ Ìï≠ÏÉÅ ÎèÑÍµ¨ ÏÇ¨Ïö©
        if force_agent:
            logger.info("üîß Agent Î™®ÎìúÍ∞Ä Í∞ïÏ†úÎêòÏñ¥ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§")
            return True

        try:
            # ÎèÑÍµ¨ ÏÑ§Î™Ö ÏàòÏßë
            tool_descriptions = []
            for tool in self.tools[:8]:  # Ï£ºÏöî ÎèÑÍµ¨Îì§Îßå
                desc = getattr(tool, "description", tool.name)
                tool_descriptions.append(f"- {tool.name}: {desc[:100]}")

            tools_info = (
                "\n".join(tool_descriptions)
                if tool_descriptions
                else "ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ ÏóÜÏùå"
            )

            # Agent Î™®Îìú ÏÑ†ÌÉù Ïãú Îçî Ï†ÅÍ∑πÏ†ÅÏù∏ ÌåêÎã® Í∏∞Ï§Ä Ï†ÅÏö©
            agent_context = ""
            if force_agent:
                agent_context = "\n\nIMPORTANT: The user has specifically selected Agent mode, indicating they want to use available tools when possible. Be more inclined to use tools for information gathering, searches, or data processing tasks."

            decision_prompt = f"""User request: "{user_input}". You must end with "Action" or "Final Answer."

Available tools:
{tools_info}

Determine if this request requires using tools to provide accurate information.

Requires tools:
- Real-time information search (web search, news, weather, etc.)
- Database queries (travel products, flights, etc.)
- External API calls (maps, translation, etc.)
- File processing or calculations
- Current time or date-related information
- Specific data lookups or searches
- Location-based queries
- If you request something You don't know, utilize appropriate tools.

Does not require tools:
- A question about knowledge you already know.
- General conversation or questions
- Explanations or opinions
- Creative writing or idea suggestions
- General knowledge already known{agent_context}

Answer: YES or NO only."""

            messages = [
                SystemMessage(
                    content="You are an expert at analyzing user requests to determine if tools are needed. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=decision_prompt),
            ]

            llm_start = time.time()
            response = self.llm.invoke(messages)
            llm_elapsed = time.time() - llm_start
            decision = response.content.strip().upper()

            result = "YES" in decision
            mode_info = " (Agent Î™®Îìú)" if force_agent else " (Ask Î™®Îìú)"
            total_elapsed = time.time() - start_time
            logger.info(
                f"ü§î ÎèÑÍµ¨ ÏÇ¨Ïö© ÌåêÎã®{mode_info}: {decision} -> {result} (LLM: {llm_elapsed:.2f}Ï¥à, Ï¥ù: {total_elapsed:.2f}Ï¥à)"
            )
            return result

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"‚ùå ÎèÑÍµ¨ ÏÇ¨Ïö© ÌåêÎã® Ïò§Î•ò: {elapsed:.2f}Ï¥à, Ïò§Î•ò: {e}")
            return False

    def _create_agent_executor(self) -> AgentExecutor:
        """ÏóêÏù¥Ï†ÑÌä∏ Ïã§ÌñâÍ∏∞ ÏÉùÏÑ±"""
        if not self.tools:
            return None

        # OpenAI ÎèÑÍµ¨ ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ± (GPT Î™®Îç∏Ïö©)
        if not self.model_name.startswith("gemini") and not (
            "sonar" in self.model_name
            or "r1-" in self.model_name
            or "perplexity" in self.model_name
        ):
            from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

            system_message = """You are a helpful AI assistant that can use various tools to provide accurate information.

**Instructions:**
- Analyze user requests carefully to select the most appropriate tools
- Use tools to gather current, accurate information when needed
- Organize information in a clear, logical structure
- Respond in natural, conversational Korean
- Be friendly and helpful while maintaining accuracy
- If multiple tools are needed, use them systematically
- Focus on providing exactly what the user asked for

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment.

**Response Format:**
- Use clear headings and bullet points when appropriate
- Format information in a structured, readable way
- Always end your response with either an Action or Final Answer
- ONLY SHOW THE FINAL ANSWER TO THE USER - HIDE ALL THOUGHT PROCESSES

**IMPORTANT**: 
- Always end your response with either an Action or Final Answer
- ONLY SHOW THE FINAL ANSWER TO THE USER - DO NOT SHOW ANY THOUGHT PROCESSES, ACTIONS, OR OBSERVATIONSghlight important information
- Keep responses well-organized and easy to read"""

            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system_message),
                    ("human", "{input}"),
                    MessagesPlaceholder(variable_name="agent_scratchpad"),
                ]
            )

            agent = create_openai_tools_agent(self.llm, self.tools, prompt)
            return AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=True,
                max_iterations=2,
                handle_parsing_errors=True,
            )

        # Perplexity Î™®Îç∏Ïö© ReAct ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±
        elif (
            "sonar" in self.model_name
            or "r1-" in self.model_name
            or "perplexity" in self.model_name
        ):
            logger.info("Perplexity Î™®Îç∏Ïö© ReAct ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±")

            # Perplexity Î™®Îç∏ÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ (ÏòÅÏñ¥Î°ú ÏûëÏÑ±)
            perplexity_react_prompt = PromptTemplate.from_template(
                """
 You are an AI assistant that uses various tools to provide accurate information.

**Instructions:**
- Carefully analyze user requests to select the most appropriate tools
- Use tools to gather current, accurate information when needed
- Organize information in a clear, logical structure
- Respond in natural, conversational Korean
- Be friendly and helpful while maintaining accuracy
- If multiple tools are needed, use them systematically
- Focus on providing exactly what the user asked for
- Always parse MCP tool results accurately and present them to the user

Available tools:
{tools}

Tool names: {tool_names}

**IMPORTANT: YOU MUST FOLLOW THIS FORMAT EXACTLY**

Question: {input}. You must end with "Action" or "Final Answer."
Thought: I need to analyze this request and determine which tool(s) would be most helpful.
Action: tool_name
Action Input: {"param1": "value1", "param2": "value2"}
Observation: tool_execution_result
Thought: Based on the result, I will decide whether to use another tool.
Action: another_tool_name
Action Input: {"param1": "value1"}
Observation: another_tool_execution_result
Thought: Now I have enough information to provide a comprehensive answer.
Final Answer: My comprehensive response in Korean

**CRITICAL FORMAT RULES - FOLLOW THESE EXACTLY**:
- After EVERY "Thought:" line, you MUST IMMEDIATELY include either "Action:" or "Final Answer:"
- NEVER skip the "Action:" line after a "Thought:" line
- NEVER include any other text between "Thought:" and "Action:" lines
- ALWAYS follow "Action:" with "Action Input:" on the next line
- ALWAYS use valid JSON format for "Action Input:" parameters
- When finished, ALWAYS end with "Thought:" followed by "Final Answer:"

**REMEMBER**: The format must be EXACTLY as shown above. This is CRITICAL for the system to work properly.

{agent_scratchpad}

{agent_scratchpad}
                """
            )

            # ÌäπÎ≥ÑÌïú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï∂îÍ∞Ä
            try:
                from core.perplexity_agent_helper import create_perplexity_agent
                
                agent = create_perplexity_agent(
                    self.llm, self.tools, perplexity_react_prompt
                )
            except ImportError as e:
                logger.error(f"perplexity_agent_helper Î™®Îìà Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
                # Ìè¥Î∞±: Í∏∞Î≥∏ ReAct ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±
                agent = create_react_agent(self.llm, self.tools, perplexity_react_prompt)

            # AgentExecutor ÏÉùÏÑ± Ïãú ÌååÏã± Ïò§Î•ò Ï≤òÎ¶¨ Í∞ïÌôî
            executor = AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=True,  # ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ verbose ÌôúÏÑ±Ìôî
                max_iterations=3,  # Î∞òÎ≥µ ÌöüÏàò Ï¶ùÍ∞Ä
                handle_parsing_errors=True,
                early_stopping_method="force",
                return_intermediate_steps=True,
            )

            # Ï∂îÍ∞Ä ÏÑ§Ï†ï
            if hasattr(executor, "handle_parsing_errors"):
                executor.handle_parsing_errors = True

            return executor

        # ReAct ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ± (Gemini Îì± Îã§Î•∏ Î™®Îç∏Ïö©)
        else:
            react_prompt = PromptTemplate.from_template(
                """
You are a helpful AI assistant that can use various tools to provide accurate information.

**Instructions:**
- Analyze user requests carefully to select the most appropriate tools
- Use tools to gather current, accurate information when needed
- Organize information in a clear, logical structure
- Respond in natural, conversational Korean
- Be friendly and helpful while maintaining accuracy
- If multiple tools are needed, use them systematically
- Focus on providing exactly what the user asked for

Available tools:
{tools}

Tool names: {tool_names}

Follow this format exactly:

Question: {input}.You must end with "Action" or "Final Answer."
Thought: I need to analyze this request and determine which tool(s) would be most helpful.
Action: tool_name
Action Input: input_for_tool
Observation: tool_execution_result
Thought: Based on the result, I will provide a comprehensive answer in Korean with clear formatting.
Final Answer: [Provide a well-organized response in Korean with clear headings, bullet points, and highlighted important information]

{agent_scratchpad}
                """
            )

            agent = create_react_agent(self.llm, self.tools, react_prompt)
            return AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=False,
                max_iterations=2,
                handle_parsing_errors=True,
                early_stopping_method="force",
                return_intermediate_steps=False,
            )

    def chat_with_tools(self, user_input: str) -> tuple[str, list]:
        """ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìïú Ï±ÑÌåÖ"""
        import time

        start_time = time.time()
        logger.info(f"üöÄ ÎèÑÍµ¨ Ï±ÑÌåÖ ÏãúÏûë: {user_input[:50]}...")

        try:
            # ÌÜ†ÌÅ∞ Ï†úÌïú Ïò§Î•ò Î∞©ÏßÄÎ•º ÏúÑÌï¥ ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ï†úÌïú
            if "context_length_exceeded" in str(getattr(self, "_last_error", "")):
                logger.warning("ÌÜ†ÌÅ∞ Ï†úÌïú Ïò§Î•òÎ°ú Ïù∏Ìï¥ ÏùºÎ∞ò Ï±ÑÌåÖÏúºÎ°ú ÎåÄÏ≤¥")
                return self.simple_chat(user_input), []

            # Gemini Î™®Îç∏ÏùÄ ÏßÅÏ†ë ÎèÑÍµ¨ Ìò∏Ï∂ú Î∞©Ïãù ÏÇ¨Ïö©
            if self.model_name.startswith("gemini"):
                logger.info("üîß Gemini ÎèÑÍµ¨ Ï±ÑÌåÖ ÏãúÏûë")
                gemini_start = time.time()
                result = self._gemini_tool_chat(user_input)
                logger.info(
                    f"üîß Gemini ÎèÑÍµ¨ Ï±ÑÌåÖ ÏôÑÎ£å: {time.time() - gemini_start:.2f}Ï¥à"
                )
                return result

            # Perplexity Î™®Îç∏ÏùÄ ÏßÅÏ†ë ÎèÑÍµ¨ Ìò∏Ï∂ú Î∞©Ïãù ÏÇ¨Ïö©
            elif (
                "sonar" in self.model_name
                or "r1-" in self.model_name
                or "perplexity" in self.model_name
            ):
                logger.info("üîß Perplexity ÎèÑÍµ¨ Ï±ÑÌåÖ ÏãúÏûë")
                perplexity_start = time.time()
                result = self._perplexity_tool_chat(user_input)
                logger.info(
                    f"üîß Perplexity ÎèÑÍµ¨ Ï±ÑÌåÖ ÏôÑÎ£å: {time.time() - perplexity_start:.2f}Ï¥à"
                )
                return result

            # GPT Î™®Îç∏ÏùÄ Í∏∞Ï°¥ ÏóêÏù¥Ï†ÑÌä∏ Î∞©Ïãù ÏÇ¨Ïö©
            if not self.agent_executor:
                logger.info("üîß ÏóêÏù¥Ï†ÑÌä∏ Ïã§ÌñâÍ∏∞ ÏÉùÏÑ± ÏãúÏûë")
                agent_create_start = time.time()
                self.agent_executor = self._create_agent_executor()
                logger.info(
                    f"üîß ÏóêÏù¥Ï†ÑÌä∏ Ïã§ÌñâÍ∏∞ ÏÉùÏÑ± ÏôÑÎ£å: {time.time() - agent_create_start:.2f}Ï¥à"
                )

            if not self.agent_executor:
                return "ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§.", []

            logger.info("üîß ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ ÏãúÏûë")
            agent_invoke_start = time.time()
            result = self.agent_executor.invoke({"input": user_input})
            logger.info(
                f"üîß ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ ÏôÑÎ£å: {time.time() - agent_invoke_start:.2f}Ï¥à"
            )
            output = result.get("output", "")

            # ÏÇ¨Ïö©Îêú ÎèÑÍµ¨ Ï†ïÎ≥¥ Ï∂îÏ∂ú
            used_tools = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    if len(step) >= 2 and hasattr(step[0], "tool"):
                        used_tools.append(step[0].tool)

            if "Agent stopped" in output or not output.strip():
                logger.warning("ÏóêÏù¥Ï†ÑÌä∏ Ï§ëÎã® Í∞êÏßÄ, Í≤∞Í≥º ÌôïÏù∏ Ï§ë...")
                
                # Ï§ëÍ∞Ñ Îã®Í≥ÑÏóêÏÑú ÎèÑÍµ¨Í∞Ä ÏÇ¨Ïö©ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
                if "intermediate_steps" in result and result["intermediate_steps"]:
                    # ÎèÑÍµ¨Í∞Ä ÏÇ¨Ïö©ÎêòÏóàÏúºÎ©¥ ÎßàÏßÄÎßâ ÎèÑÍµ¨ Í≤∞Í≥º ÌôïÏù∏
                    last_step = result["intermediate_steps"][-1]
                    if len(last_step) >= 2:
                        tool_result = last_step[1]
                        logger.info(f"ÎèÑÍµ¨ Í≤∞Í≥º Î∞úÍ≤¨: {str(tool_result)[:100]}...")
                        
                        # ÎèÑÍµ¨ Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
                        final_response = self._generate_final_response(user_input, str(tool_result))
                        return final_response, [last_step[0].tool]
                
                # ÎèÑÍµ¨ Í≤∞Í≥ºÍ∞Ä ÏóÜÏúºÎ©¥ ÏùºÎ∞ò Ï±ÑÌåÖÏúºÎ°ú ÎåÄÏ≤¥
                logger.warning("Ïú†Ìö®Ìïú ÎèÑÍµ¨ Í≤∞Í≥º ÏóÜÏùå, ÏùºÎ∞ò Ï±ÑÌåÖÏúºÎ°ú ÎåÄÏ≤¥")
                return self.simple_chat(user_input), []

            elapsed = time.time() - start_time
            logger.info(f"‚úÖ ÎèÑÍµ¨ Ï±ÑÌåÖ ÏôÑÎ£å: {elapsed:.2f}Ï¥à")
            return output, used_tools

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            self._last_error = error_msg
            logger.error(f"‚ùå ÎèÑÍµ¨ ÏÇ¨Ïö© Ï±ÑÌåÖ Ïò§Î•ò: {elapsed:.2f}Ï¥à, Ïò§Î•ò: {e}")

            # ÌÜ†ÌÅ∞ Ï†úÌïú Ïò§Î•ò Ï≤òÎ¶¨
            if (
                "context_length_exceeded" in error_msg
                or "maximum context length" in error_msg
            ):
                logger.warning("ÌÜ†ÌÅ∞ Ï†úÌïú Ïò§Î•ò Î∞úÏÉù, ÏùºÎ∞ò Ï±ÑÌåÖÏúºÎ°ú ÎåÄÏ≤¥")
                return self.simple_chat(user_input), []

            return self.simple_chat(user_input), []

    def simple_chat(self, user_input: str) -> str:
        """ÏùºÎ∞ò Ï±ÑÌåÖ (ÎèÑÍµ¨ ÏÇ¨Ïö© ÏóÜÏùå)"""
        try:
            # ÌÜµÏùºÎêú ÏãúÏä§ÌÖú Î©îÏãúÏßÄ - Ïù¥ÎØ∏ÏßÄ ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÏóê ÌäπÌôî
            system_content = """You are an expert AI assistant specialized in image analysis and text extraction (OCR).

**Primary Mission for Images:**
- **COMPLETE TEXT EXTRACTION**: Extract every single character, number, and symbol from images with 100% accuracy
- **ZERO OMISSIONS**: Never skip or miss any text, no matter how small or unclear
- **PERFECT TRANSCRIPTION**: Reproduce all text exactly as it appears, including spacing and formatting
- **STRUCTURAL ANALYSIS**: Identify tables, lists, headers, paragraphs, and document layout
- **MULTILINGUAL SUPPORT**: Handle Korean, English, numbers, and special characters flawlessly

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment.

**Response Format for Images:**
## üìÑ Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏
[Î™®Îì† ÌÖçÏä§Ìä∏Î•º Ï†ïÌôïÌûà ÎÇòÏó¥ - Ï†àÎåÄ ÎàÑÎùΩ Í∏àÏßÄ]

## üìã Î¨∏ÏÑú Íµ¨Ï°∞
[Ìëú, Î™©Î°ù, Ï†úÎ™© Îì±Ïùò Íµ¨Ï°∞ ÏÑ§Î™Ö]

## üìç Î†àÏù¥ÏïÑÏõÉ Ï†ïÎ≥¥
[ÌÖçÏä§Ìä∏ Î∞∞ÏπòÏôÄ ÏúÑÏπò Í¥ÄÍ≥Ñ]

**Critical Rules:**
- NEVER say "ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§" or "Ï∂îÏ∂úÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§"
- ALWAYS extract something, even if text is small or unclear
- If text is unclear, provide your best interpretation with [Î∂àÎ™ÖÌôï] notation
- Focus on TEXT EXTRACTION as the absolute priority

**For General Questions:**
- Always respond in natural, conversational Korean
- Organize information clearly with headings and bullet points
- Highlight important information using **bold** formatting
- Be friendly, helpful, and accurate"""

            # Gemini Î™®Îç∏Ïùò Í≤ΩÏö∞ ÏãúÏä§ÌÖú Î©îÏãúÏßÄÎ•º Ïù∏Í∞Ñ Î©îÏãúÏßÄÎ°ú Î≥ÄÌôò
            if self.model_name.startswith("gemini"):
                messages = [HumanMessage(content=system_content)]
            else:
                messages = [SystemMessage(content=system_content)]

            # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
            if "[IMAGE_BASE64]" in user_input and "[/IMAGE_BASE64]" in user_input:
                processed_input = self._process_image_input(user_input)
                messages.append(processed_input)
            else:
                messages.append(HumanMessage(content=user_input))

            response = self.llm.invoke(messages)
            return response.content

        except Exception as e:
            logger.error(f"ÏùºÎ∞ò Ï±ÑÌåÖ Ïò§Î•ò: {e}")
            return f"Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî."

    def simple_chat_with_history(
        self, user_input: str, conversation_history: List[Dict]
    ) -> str:
        """ÎåÄÌôî Í∏∞Î°ùÏùÑ Ìè¨Ìï®Ìïú ÏùºÎ∞ò Ï±ÑÌåÖ"""
        try:
            logger.info(
                f"ÌûàÏä§ÌÜ†Î¶¨ÏôÄ Ìï®Íªò Ï±ÑÌåÖ ÏãúÏûë: {len(conversation_history)}Í∞ú Î©îÏãúÏßÄ"
            )

            messages = self._convert_history_to_messages(conversation_history)

            # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
            if "[IMAGE_BASE64]" in user_input and "[/IMAGE_BASE64]" in user_input:
                processed_input = self._process_image_input(user_input)
                messages.append(processed_input)
            else:
                messages.append(HumanMessage(content=user_input))

            logger.info(f"ÏµúÏ¢Ö Î©îÏãúÏßÄ Ïàò: {len(messages)}Í∞ú")

            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            logger.error(f"ÎåÄÌôî Í∏∞Î°ù Ï±ÑÌåÖ Ïò§Î•ò: {e}")
            return self.simple_chat(user_input)

    def _process_image_input(self, user_input: str):
        """Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÏó¨ LangChain Î©îÏãúÏßÄÎ°ú Î≥ÄÌôò"""
        import re
        import base64

        # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
        image_match = re.search(
            r"\[IMAGE_BASE64\](.*?)\[/IMAGE_BASE64\]", user_input, re.DOTALL
        )
        if not image_match:
            return HumanMessage(content=user_input)

        image_data = image_match.group(1).strip()
        text_content = user_input.replace(image_match.group(0), "").strip()

        # Base64 Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
        try:
            base64.b64decode(image_data)
        except Exception as e:
            logger.error(f"ÏûòÎ™ªÎêú Base64 Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞: {e}")
            return HumanMessage(content="ÏûòÎ™ªÎêú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§.")

        # ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÏóê ÌäπÌôîÎêú ÌîÑÎ°¨ÌîÑÌä∏
        if not text_content:
            text_content = """Ïù¥ Ïù¥ÎØ∏ÏßÄÏóêÏÑú **Î™®Îì† ÌÖçÏä§Ìä∏Î•º Ï†ïÌôïÌûà Ï∂îÏ∂ú(OCR)**Ìï¥Ï£ºÏÑ∏Ïöî.

**ÌïÑÏàò ÏûëÏóÖ:**
1. **ÏôÑÏ†ÑÌïú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú**: Ïù¥ÎØ∏ÏßÄ ÎÇ¥ Î™®Îì† ÌïúÍ∏Ä, ÏòÅÏñ¥, Ïà´Ïûê, Í∏∞Ìò∏Î•º Îπ†ÏßêÏóÜÏù¥ Ï∂îÏ∂ú
2. **Íµ¨Ï°∞ Î∂ÑÏÑù**: Ìëú, Î™©Î°ù, Ï†úÎ™©, Îã®ÎùΩ Îì±Ïùò Î¨∏ÏÑú Íµ¨Ï°∞ ÌååÏïÖ
3. **Î†àÏù¥ÏïÑÏõÉ Ï†ïÎ≥¥**: ÌÖçÏä§Ìä∏Ïùò ÏúÑÏπò, ÌÅ¨Í∏∞, Î∞∞Ïπò Í¥ÄÍ≥Ñ ÏÑ§Î™Ö
4. **Ï†ïÌôïÌïú Ï†ÑÏÇ¨**: Ïò§ÌÉÄ ÏóÜÏù¥ Ï†ïÌôïÌïòÍ≤å Î™®Îì† Î¨∏Ïûê Í∏∞Î°ù
5. **Îß•ÎùΩ ÏÑ§Î™Ö**: Î¨∏ÏÑúÏùò Ï¢ÖÎ•òÏôÄ Î™©Ï†Å ÌååÏïÖ
6. **ÌÖåÏù¥Î∏î Ìè¨Îß∑**: ÌëúÎ•º ÎßåÎì§ ÎïåÎäî ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãù ÏÇ¨Ïö©: |Header1|Header2|\n|---|---|\n|Data1|Data2|

**ÏùëÎãµ ÌòïÏãù:**
## üìÑ Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏
[Î™®Îì† ÌÖçÏä§Ìä∏Î•º Ï†ïÌôïÌûà ÎÇòÏó¥]

## üìã Î¨∏ÏÑú Íµ¨Ï°∞
[Ìëú, Î™©Î°ù, Ï†úÎ™© Îì±Ïùò Íµ¨Ï°∞ ÏÑ§Î™Ö]

## üìç Î†àÏù¥ÏïÑÏõÉ Ï†ïÎ≥¥
[ÌÖçÏä§Ìä∏ Î∞∞ÏπòÏôÄ ÏúÑÏπò Í¥ÄÍ≥Ñ]

**Ï§ëÏöî**: Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÏùΩÏùÑ Ïàò ÏûàÎäî Î™®Îì† ÌÖçÏä§Ìä∏Î•º Ï†àÎåÄ ÎàÑÎùΩÌïòÏßÄ ÎßêÍ≥† ÏôÑÏ†ÑÌûà Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî."""

        try:
            # Gemini Î™®Îç∏Ïùò Í≤ΩÏö∞ ÌäπÎ≥ÑÌïú ÌòïÏãù ÏÇ¨Ïö©
            if self.model_name.startswith("gemini"):
                # Gemini 2.0 FlashÏóê ÏµúÏ†ÅÌôîÎêú ÌòïÏãù
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": f"data:image/jpeg;base64,{image_data}",
                        },
                    ]
                )
            else:
                # OpenAI GPT-4V ÌòïÏãù
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            },
                        },
                    ]
                )
        except Exception as e:
            logger.error(f"Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïò§Î•ò: {e}")
            return HumanMessage(
                content=f"{text_content}\n\n[Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïò§Î•ò: {str(e)}]"
            )

    def process_message(self, user_input: str) -> tuple[str, list]:
        """Î©îÏãúÏßÄ Ï≤òÎ¶¨ - AIÍ∞Ä ÎèÑÍµ¨ ÏÇ¨Ïö© Ïó¨Î∂Ä Í≤∞Ï†ï"""
        # ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄÎ•º ÌûàÏä§ÌÜ†Î¶¨Ïóê Ï∂îÍ∞Ä
        self.conversation_history.add_message("user", user_input)

        if not self.tools:
            response = self.simple_chat_with_history(
                user_input, self.conversation_history.get_recent_messages(10)  # 5Í∞ú ÎåÄÌôî
            )
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, []

        # Perplexity Î™®Îç∏Ïùò Í≤ΩÏö∞ Ìï≠ÏÉÅ ÎèÑÍµ¨ ÏÇ¨Ïö©
        if (
            "sonar" in self.model_name
            or "r1-" in self.model_name
            or "perplexity" in self.model_name
        ):
            logger.info("üîß Perplexity Î™®Îç∏ÏùÄ Ìï≠ÏÉÅ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§")
            response, used_tools = self.chat_with_tools(user_input)
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, used_tools

        # AIÍ∞Ä Ïª®ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÏó¨ ÎèÑÍµ¨ ÏÇ¨Ïö© Ïó¨Î∂Ä Í≤∞Ï†ï
        use_tools = self._should_use_tools(user_input)

        if use_tools:
            response, used_tools = self.chat_with_tools(user_input)
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, used_tools
        else:
            response = self.simple_chat_with_history(
                user_input, self.conversation_history.get_recent_messages(10)  # 5Í∞ú ÎåÄÌôî
            )
            self.conversation_history.add_message("assistant", response)
            self.conversation_history.save_to_file()
            return response, []

    def process_message_with_history(
        self,
        user_input: str,
        conversation_history: List[Dict],
        force_agent: bool = False,
    ) -> tuple[str, list]:
        """ÎåÄÌôî Í∏∞Î°ùÏùÑ Ìè¨Ìï®Ìïú Î©îÏãúÏßÄ Ï≤òÎ¶¨"""
        if not self.tools:
            response = self.simple_chat_with_history(user_input, conversation_history)
            return response, []

        # force_agentÍ∞Ä TrueÎ©¥ Îçî Ï†ÅÍ∑πÏ†ÅÏúºÎ°ú ÎèÑÍµ¨ ÏÇ¨Ïö© ÌåêÎã®
        use_tools = self._should_use_tools(user_input, force_agent)

        if use_tools:
            response, used_tools = self.chat_with_tools(user_input)
            return response, used_tools
        else:
            response = self.simple_chat_with_history(user_input, conversation_history)
            return response, []

    def _convert_history_to_messages(self, conversation_history: List[Dict]):
        """ÎåÄÌôî Í∏∞Î°ùÏùÑ LangChain Î©îÏãúÏßÄÎ°ú Î≥ÄÌôò - ÌÜ†ÌÅ∞ Ï†úÌïú Í≥†Î†§"""
        # Perplexity API Î©îÏãúÏßÄ ÌòïÏãù Í≤ÄÏ¶ù
        validated_history = MessageValidator.validate_and_fix_messages(
            conversation_history
        )

        messages = []

        # ÌÜµÏùºÎêú ÏãúÏä§ÌÖú Î©îÏãúÏßÄ - ÌûàÏä§ÌÜ†Î¶¨ ÌôúÏö© Í∞ïÏ°∞
        unified_system_content = """You are a helpful AI assistant that provides contextual responses based on conversation history.

**Response Guidelines:**
- Always respond in natural, conversational Korean
- Use conversation history to provide relevant, contextual answers
- Organize information clearly with headings and bullet points when appropriate
- Highlight important information using **bold** formatting
- Be friendly, helpful, and maintain conversation flow
- Reference previous context when relevant
- Provide comprehensive, well-structured responses

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment."""

        if self.model_name.startswith("gemini"):
            messages.append(HumanMessage(content=unified_system_content))
        else:
            messages.append(SystemMessage(content=unified_system_content))

        # ÏµúÍ∑º 5Í∞ú ÎåÄÌôî Í∏∞Î°ù ÏÇ¨Ïö© (10Í∞ú Î©îÏãúÏßÄ = 5Í∞ú ÎåÄÌôî)
        recent_history = (
            validated_history[-10:] if len(validated_history) > 10 else validated_history
        )

        for msg in recent_history:
            role = msg.get("role", "")
            content = msg.get("content", "")[:500]  # ÎÇ¥Ïö© Ï†úÌïú Ï¶ùÍ∞Ä
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                from langchain.schema import AIMessage

                messages.append(AIMessage(content=content))

        logger.info(
            f"ÌûàÏä§ÌÜ†Î¶¨ Î≥ÄÌôò ÏôÑÎ£å: {len(recent_history)}Í∞ú Î©îÏãúÏßÄ -> {len(messages)}Í∞ú LangChain Î©îÏãúÏßÄ"
        )
        return messages

    def _gemini_tool_chat(self, user_input: str) -> tuple[str, list]:
        """Í≤åÎØ∏Îãà Î™®Îç∏Ïö© AI Í∏∞Î∞ò ÎèÑÍµ¨ ÏÑ†ÌÉù Î∞è Ïã§Ìñâ - Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏßÄÏõê"""
        try:
            if self._is_tool_list_request(user_input):
                return self._show_tool_list(), []

            # Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏãúÏûë
            return self._execute_tool_chain(user_input)

        except Exception as e:
            logger.error(f"Í≤åÎØ∏Îãà ÎèÑÍµ¨ Ï±ÑÌåÖ Ïò§Î•ò: {e}")
            return self.simple_chat(user_input), []

    def _get_realistic_date_range(self, user_input: str) -> tuple[str, str]:
        """ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏóêÏÑú ÌòÑÏã§Ï†ÅÏù∏ ÎÇ†Ïßú Î≤îÏúÑ ÏÉùÏÑ± - Î≤îÏö©Ï†Å Ï†ëÍ∑º"""
        # ÌäπÏ†ï API ÌòïÏãùÏóê ÏùòÏ°¥ÌïòÏßÄ ÏïäÍ≥† AIÍ∞Ä Ï†ÅÏ†àÌïú ÌòïÏãùÏùÑ Í≤∞Ï†ïÌïòÎèÑÎ°ù Ìï®
        # ÎÇ†Ïßú ÌååÏã±ÏùÄ Í∞Å ÎèÑÍµ¨Ïùò Ïä§ÌÇ§ÎßàÏóê Îî∞Îùº AIÍ∞Ä Ï≤òÎ¶¨
        return None, None  # AIÍ∞Ä Ï†ÅÏ†àÌïú ÎèÑÍµ¨Î•º ÏÑ†ÌÉùÌïòÏó¨ Ï≤òÎ¶¨ÌïòÎèÑÎ°ù Ìï®

    def _get_area_code(self, user_input: str) -> str:
        """ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏóêÏÑú ÏßÄÏó≠ ÏΩîÎìú Ï∂îÏ∂ú - AIÍ∞Ä ÎèôÏ†ÅÏúºÎ°ú Í≤∞Ï†ï"""
        # AIÍ∞Ä ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨Î•º ÌÜµÌï¥ ÏßÄÏó≠ ÏΩîÎìúÎ•º ÎèôÏ†ÅÏúºÎ°ú Ï°∞ÌöåÌïòÎèÑÎ°ù Î≥ÄÍ≤Ω
        # ÌïòÎìúÏΩîÎî©Îêú Îß§Ìïë Ï†úÍ±∞ÌïòÍ≥† Î≤îÏö©Ï†Å Ï†ëÍ∑º Î∞©Ïãù ÏÇ¨Ïö©
        return None  # AIÍ∞Ä Ï†ÅÏ†àÌïú ÎèÑÍµ¨Î•º ÏÑ†ÌÉùÌïòÏó¨ Ï≤òÎ¶¨ÌïòÎèÑÎ°ù Ìï®

    def _ai_select_tool(self, user_input: str):
        """Ï¥àÍ∏∞ ÎèÑÍµ¨ ÏÑ†ÌÉù (Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ Ïú†ÏßÄ)"""
        return self._ai_select_next_tool(user_input, [], 0)

    def _legacy_ai_select_tool(self, user_input: str):
        """AIÍ∞Ä ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÏùò ÎèÑÍµ¨ÏôÄ ÌååÎùºÎØ∏ÌÑ∞Î•º ÏßÄÎä•Ï†ÅÏúºÎ°ú Í≤∞Ï†ï"""
        try:
            if not self.tools:
                logger.warning("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§")
                return None

            # ÎèÑÍµ¨ ÏÑ§Î™Ö ÏàòÏßë (ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ìè¨Ìï®)
            tools_info = []
            for tool in self.tools:
                desc = getattr(tool, "description", tool.name)
                # ÎèÑÍµ¨ Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ÎèÑ Ìè¨Ìï®
                schema_info = ""
                if hasattr(tool, "args_schema") and tool.args_schema:
                    try:
                        schema = tool.args_schema.schema()
                        if "properties" in schema:
                            params = list(schema["properties"].keys())[
                                :3
                            ]  # Ï£ºÏöî ÌååÎùºÎØ∏ÌÑ∞Îßå
                            schema_info = f" (ÌååÎùºÎØ∏ÌÑ∞: {', '.join(params)})"
                    except:
                        pass
                tools_info.append(f"- {tool.name}: {desc}{schema_info}")

            tools_list = "\n".join(tools_info)

            selection_prompt = f"""User request: "{user_input}"

Available tools:
{tools_list}

Analyze the user's request and select the most appropriate tool with necessary parameters.

Response format:
- Use tool: TOOL: tool_name | PARAMS: {{"key": "value"}}
- No tool needed: TOOL: none

Examples:
- Web search: TOOL: search | PARAMS: {{"query": "search_term"}}
- Travel products: TOOL: retrieveSaleProductInformation | PARAMS: {{"startDate": "20240301", "endDate": "20240310", "productAreaCode": "A0"}}

Extract parameter values from user request or use reasonable defaults."""

            messages = [
                SystemMessage(
                    content="You are an expert at analyzing user requests to select the optimal tool. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=selection_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._parse_tool_decision(response.content)

        except Exception as e:
            logger.error(f"AI ÎèÑÍµ¨ ÏÑ†ÌÉù Ïò§Î•ò: {e}")
            return None

    def _parse_tool_decision(self, response: str):
        """ÎèÑÍµ¨ ÏÑ†ÌÉù ÏùëÎãµ ÌååÏã±"""
        try:
            import re
            import json

            logger.info(f"ÌååÏã±Ìï† ÏùëÎãµ: {response}")

            # "TOOL: toolname | PARAMS: {...}" ÌòïÏãù ÌååÏã±
            match = re.search(
                r"TOOL:\s*([^|\n]+)(?:\s*\|\s*PARAMS:\s*([^\n]+))?",
                response,
                re.IGNORECASE,
            )
            if not match:
                logger.warning(f"ÎèÑÍµ¨ ÌòïÏãù Îß§Ïπ≠ Ïã§Ìå®: {response}")
                return None

            tool_name = match.group(1).strip()
            params_str = match.group(2).strip() if match.group(2) else "{}"

            logger.info(f"ÌååÏã±Îêú ÎèÑÍµ¨Î™Ö: '{tool_name}', ÌååÎùºÎØ∏ÌÑ∞: '{params_str}'")

            if tool_name.lower() == "none":
                return {"tool": "none"}

            try:
                params = json.loads(params_str)
            except json.JSONDecodeError as je:
                logger.warning(f"JSON ÌååÏã± Ïã§Ìå®: {params_str}, Ïò§Î•ò: {je}")
                params = {}

            return {"tool": tool_name, "params": params}

        except Exception as e:
            logger.error(f"ÎèÑÍµ¨ Í≤∞Ï†ï ÌååÏã± Ïò§Î•ò: {e}")
            return None

    def _split_large_response(
        self, response_text: str, max_chunk_size: int = 3000
    ) -> List[str]:
        """ÌÅ∞ ÏùëÎãµÏùÑ Ï≤≠ÌÅ¨Î°ú Î∂ÑÌï†"""
        if len(response_text) <= max_chunk_size:
            return [response_text]

        chunks = []
        current_pos = 0

        while current_pos < len(response_text):
            # JSON Íµ¨Ï°∞Î•º Í≥†Î†§Ìïú Î∂ÑÌï† ÏßÄÏ†ê Ï∞æÍ∏∞
            end_pos = min(current_pos + max_chunk_size, len(response_text))

            # JSON Í∞ùÏ≤¥ÎÇò Î∞∞Ïó¥ ÎÅùÏóêÏÑú Î∂ÑÌï†
            if end_pos < len(response_text):
                # Îí§ÏóêÏÑú Í∞ÄÏû• Í∞ÄÍπåÏö¥ }, ] Ï∞æÍ∏∞
                for i in range(end_pos, current_pos, -1):
                    if response_text[i] in ["}", "]", ","]:
                        end_pos = i + 1
                        break

            chunk = response_text[current_pos:end_pos]
            chunks.append(chunk)
            current_pos = end_pos

        return chunks

    def _process_chunked_response(self, large_response: str, user_query: str) -> str:
        """ÎåÄÏö©Îüâ ÏùëÎãµÏùÑ Ï†ÑÏ≤¥ Î∂ÑÏÑùÌïòÏó¨ Í≤∞Í≥ºÎßå Î≥¥Ïó¨Ï£ºÍ∏∞"""
        logger.info(f"ÎåÄÏö©Îüâ ÏùëÎãµ Î∂ÑÏÑù ÏãúÏûë: {len(large_response)}Ïûê")

        # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º Ìïú Î≤àÏóê Î∂ÑÏÑù
        analysis_prompt = f"""User query: {user_query}
                            Data:
                            {large_response}
                            Analyze the above data and provide useful information to the user in Korean. Don't mention chunk numbers or analysis progress, just show the results."""

        messages = [
            SystemMessage(
                content="You are an expert who analyzes data and provides useful information to users in Korean. Don't mention the analysis process, just show the results."
            ),
            HumanMessage(content=analysis_prompt),
        ]

        analysis_response = self.llm.invoke(messages)
        return analysis_response.content

    def _execute_selected_tool(self, tool_decision):
        """ÏÑ†ÌÉùÎêú ÎèÑÍµ¨ Ïã§Ìñâ"""
        try:
            tool_name = tool_decision["tool"]
            params = tool_decision.get("params", {})

            # ÎèÑÍµ¨ Ïä§ÌÇ§ÎßàÏóê Îî∞Îùº ÌååÎùºÎØ∏ÌÑ∞ ÏûêÎèô Î≥ÄÌôò
            selected_tool = None
            for tool in self.tools:
                if tool.name.lower() == tool_name.lower():
                    selected_tool = tool
                    break

            if (
                selected_tool
                and hasattr(selected_tool, "args_schema")
                and selected_tool.args_schema
            ):
                try:
                    schema = selected_tool.args_schema.schema()
                    if "properties" in schema:
                        for param_name, param_value in params.items():
                            if param_name in schema["properties"]:
                                param_schema = schema["properties"][param_name]
                                if param_schema.get("type") == "array" and isinstance(
                                    param_value, str
                                ):
                                    params[param_name] = [param_value]
                except:
                    pass

            logger.info(f"ÎèÑÍµ¨ Ï∞æÍ∏∞: '{tool_name}'")

            # ÎèÑÍµ¨ Ï∞æÍ∏∞ (Îçî Ïú†Ïó∞Ìïú Îß§Ïπ≠)
            selected_tool = None
            for tool in self.tools:
                tool_name_lower = tool_name.lower()
                actual_name_lower = tool.name.lower()

                # Ï†ïÌôïÌïú Ïù¥Î¶Ñ Îß§Ïπ≠ ÎòêÎäî Î∂ÄÎ∂Ñ Îß§Ïπ≠
                if (
                    tool_name_lower == actual_name_lower
                    or tool_name_lower in actual_name_lower
                    or actual_name_lower in tool_name_lower
                    or tool_name_lower.replace("_", "-")
                    == actual_name_lower.replace("_", "-")
                ):
                    selected_tool = tool
                    logger.info(f"ÎèÑÍµ¨ Îß§Ïπ≠ ÏÑ±Í≥µ: '{tool_name}' -> '{tool.name}'")
                    break

            if not selected_tool:
                available_tools = [t.name for t in self.tools]
                logger.error(
                    f"ÎèÑÍµ¨ '{tool_name}'ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨: {available_tools}"
                )
                return f"ÎèÑÍµ¨ '{tool_name}'ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨: {available_tools}"

            # Í≤ÄÏÉâ ÎèÑÍµ¨Ïùò Í≤ΩÏö∞ Îçî ÎßéÏùÄ Í≤∞Í≥º ÏöîÏ≤≠
            if "search" in selected_tool.name.lower():
                if "limit" not in params and "maxResults" not in params:
                    params["maxResults"] = 10  # Í∏∞Î≥∏ 10Í∞ú Í≤∞Í≥º
                if "includeHtml" not in params:
                    params["includeHtml"] = False

            # ÎèÑÍµ¨ Ïã§Ìñâ (GPT Ïä§ÌÉÄÏùº Î°úÍπÖ)
            print(f"\n> Invoking: `{selected_tool.name}` with `{params}`\n")

            result = selected_tool.invoke(params)

            # Í≤∞Í≥º Ï∂úÎ†•
            print(result)
            print("\n")

            # Í≤∞Í≥ºÍ∞Ä ÎπÑÏñ¥ÏûàÎäî Í≤ΩÏö∞ ÎåÄÏïà Ï†úÏãú
            if isinstance(result, str) and (
                '"resultCount": 0' in result
                or '"saleProductItemResponseList": []' in result
            ):
                logger.warning(f"ÎèÑÍµ¨ Ïã§Ìñâ Í≤∞Í≥ºÍ∞Ä ÎπÑÏñ¥ÏûàÏùå: {selected_tool.name}")
                return f"Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. Îã§Î•∏ ÎÇ†ÏßúÎÇò ÏßÄÏó≠ÏúºÎ°ú Îã§Ïãú ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî.\n\nÏõêÎ≥∏ Í≤∞Í≥º: {result}"

            logger.info(f"ÎèÑÍµ¨ Ïã§Ìñâ Í≤∞Í≥º: {str(result)[:200]}...")

            # ÎåÄÏö©Îüâ ÏùëÎãµ Ï≤òÎ¶¨
            if isinstance(result, str) and len(result) > 5000:
                logger.info(f"ÎåÄÏö©Îüâ ÏùëÎãµ Í∞êÏßÄ: {len(result)}Ïûê")
                original_query = (
                    tool_decision.get("original_query", "")
                    if isinstance(tool_decision, dict)
                    else ""
                )
                return self._process_chunked_response(result, original_query)

            return result

        except Exception as e:
            error_msg = str(e)
            logger.error(f"ÎèÑÍµ¨ Ïã§Ìñâ Ïò§Î•ò: {e}")

            # ÌÉÄÏûÑÏïÑÏõÉ Ïò§Î•ò Ï≤òÎ¶¨
            if "timeout" in error_msg.lower() or "MCP ÏùëÎãµ ÌÉÄÏûÑÏïÑÏõÉ" in error_msg:
                return f"ÎèÑÍµ¨ Ìò∏Ï∂ú ÏãúÍ∞ÑÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."

            return f"ÎèÑÍµ¨ Ïã§Ìñâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}"

    def _is_tool_list_request(self, user_input: str) -> bool:
        """ÎèÑÍµ¨ Î™©Î°ù ÏöîÏ≤≠Ïù∏ÏßÄ AIÍ∞Ä ÌåêÎã®"""
        try:
            # ÎèÑÍµ¨ Î™©Î°ùÏù¥ ÎπÑÏñ¥ÏûàÏúºÎ©¥ Ìï≠ÏÉÅ False Î∞òÌôò
            if not self.tools:
                return False

            # Í∞ÑÎã®Ìïú ÌîÑÎ°¨ÌîÑÌä∏Î°ú AIÏóêÍ≤å ÌåêÎã® ÏöîÏ≤≠
            prompt = f"""User input: "{user_input}"

Determine if this user input is asking to see a list of available tools.
Examples of tool list requests:
- "What tools can I use?"
- "Show me the available tools"
- "List MCP tools"
- "What active tools are there?"

If this input is asking for a tool list, answer ONLY 'YES'. Otherwise, answer ONLY 'NO'."""

            messages = [
                SystemMessage(
                    content="You are a helpful assistant that determines if a user is asking to see a list of available tools."
                ),
                HumanMessage(content=prompt),
            ]

            # Í∞ÑÎã®Ìïú ÏùëÎãµÎßå ÌïÑÏöîÌïòÎØÄÎ°ú ÌÜ†ÌÅ∞ Ï†úÌïú
            response = self.llm.invoke(messages)
            result = response.content.strip().upper()

            logger.info(f"ÎèÑÍµ¨ Î™©Î°ù ÏöîÏ≤≠ ÌåêÎã®: {result}")
            return "YES" in result

        except Exception as e:
            logger.error(f"ÎèÑÍµ¨ Î™©Î°ù ÏöîÏ≤≠ ÌåêÎã® Ïò§Î•ò: {e}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú ÏïàÏ†ÑÌïòÍ≤å False Î∞òÌôò
            return False

    def _show_tool_list(self) -> str:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ Î™©Î°ùÏùÑ ÎèôÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÌïòÏó¨ Î∞òÌôò"""
        if not self.tools:
            return "ÌòÑÏû¨ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§."

        # ÏÑúÎ≤ÑÎ≥ÑÎ°ú ÎèÑÍµ¨ Î∂ÑÎ•ò
        tools_by_server = {}
        for tool in self.tools:
            server_name = getattr(tool, "server_name", "Unknown")
            if server_name not in tools_by_server:
                tools_by_server[server_name] = []
            tools_by_server[server_name].append(tool)

        # ÎèÑÍµ¨ Î™©Î°ù ÏÉùÏÑ±
        result = ["üîß **ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ Î™©Î°ù**\n"]

        for server_name, server_tools in tools_by_server.items():
            result.append(f"üì¶ **{server_name}** ({len(server_tools)}Í∞ú ÎèÑÍµ¨):")
            for tool in server_tools:
                desc = getattr(tool, "description", tool.name)
                # Ï†ÑÏ≤¥ ÏÑ§Î™Ö ÌëúÏãú
                result.append(f"  ‚Ä¢ {tool.name}: {desc}")
            result.append("")  # Îπà Ï§Ñ Ï∂îÍ∞Ä

        result.append(f"Ï¥ù {len(self.tools)}Í∞úÏùò ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.")
        return "\n".join(result)

    def _format_response(self, text: str) -> str:
        """ÏùëÎãµ ÌÖçÏä§Ìä∏Î•º ÏûêÏó∞Ïä§ÎüΩÍ≤å Ï†ïÎ¶¨"""
        if not text:
            return text

        import re

        # Í≥ºÎèÑÌïú Ï§ÑÎ∞îÍøà Ï†ïÎ¶¨
        formatted = re.sub(r"\n{3,}", "\n\n", text)

        # Î∂àÍ∑úÏπôÌïú Îì§Ïó¨Ïì∞Í∏∞ Ï†ïÎ¶¨ - 5Í∞ú Ïù¥ÏÉÅÏùò Í≥µÎ∞±ÏùÑ 4Í∞úÎ°ú ÌÜµÏùº
        formatted = re.sub(r"\n\s{5,}", "\n    ", formatted)

        formatted = formatted.strip()
        return formatted

    def _generate_final_response(self, user_input: str, tool_result: str):
        """ÎèÑÍµ¨ Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± - ÌÜ†ÌÅ∞ Ï†úÌïú Í≥†Î†§"""
        try:
            # ÎèÑÍµ¨ Í≤∞Í≥ºÍ∞Ä ÎÑàÎ¨¥ Í∏∏Î©¥ ÏöîÏïΩ
            if len(tool_result) > 2000:
                tool_result = tool_result[:2000] + "...(ÏÉùÎûµ)"

            response_prompt = f"""User asked: "{user_input}"

Data from tools:
{tool_result}

Your task:
1. Extract the most relevant information that directly answers the user's question
2. Organize the information in a logical, easy-to-follow structure
3. Write in conversational Korean as if explaining to a friend
4. Focus on what the user actually needs to know
5. Use simple, clear sentences without technical jargon
6. If there are multiple pieces of information, prioritize the most important ones first
7. Format your response using simple markdown (## for headings, **bold** for emphasis, - for bullet points)
8. Keep formatting minimal and clean
9. Use consistent indentation for lists and structured data

Provide a helpful, natural Korean response in markdown format that directly addresses what the user wanted to know."""

            unified_system_message = """You are a helpful AI assistant that analyzes tool results and provides comprehensive responses.

**Response Guidelines:**
- Always respond in natural, conversational Korean
- Organize information clearly with headings and bullet points
- Highlight important information using **bold** formatting
- Extract and present the most relevant information
- Use simple, clear sentences without technical jargon
- Structure responses logically and systematically
- Focus on what the user actually needs to know

**TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment."""

            messages = [
                SystemMessage(content=unified_system_message),
                HumanMessage(content=response_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._format_response(response.content)

        except Exception as e:
            error_msg = str(e)
            logger.error(f"ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ïò§Î•ò: {e}")

            # ÌÜ†ÌÅ∞ Ï†úÌïú Ïò§Î•ò Ï≤òÎ¶¨
            if (
                "context_length_exceeded" in error_msg
                or "maximum context length" in error_msg
            ):
                return self._format_response(
                    "Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ï∞æÏïòÏßÄÎßå ÎÇ¥Ïö©Ïù¥ ÎÑàÎ¨¥ Í∏∏Ïñ¥ ÏöîÏïΩÌï† Ïàò ÏóÜÏäµÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏ÏúºÎ°ú Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
                )

            return self._format_response(
                "ÎèÑÍµ¨ Ïã§ÌñâÏùÄ ÏÑ±Í≥µÌñàÏßÄÎßå ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
            )

    def _execute_tool_chain(
        self, user_input: str, max_iterations: int = 3
    ) -> tuple[str, list]:
        """Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© Ïã§Ìñâ"""
        all_used_tools = []
        accumulated_results = []
        current_query = user_input

        for iteration in range(max_iterations):
            logger.info(f"ÎèÑÍµ¨ Ï≤¥Ïù∏ {iteration + 1}Îã®Í≥Ñ ÏãúÏûë: {current_query[:50]}...")

            # AIÍ∞Ä Îã§Ïùå ÎèÑÍµ¨ Í≤∞Ï†ï
            tool_decision = self._ai_select_next_tool(
                current_query, accumulated_results, iteration
            )

            if not tool_decision or tool_decision.get("tool") == "none":
                logger.info(f"ÎèÑÍµ¨ Ï≤¥Ïù∏ {iteration + 1}Îã®Í≥ÑÏóêÏÑú Ï¢ÖÎ£å")
                break

            # ÎèÑÍµ¨ Ïã§Ìñâ
            tool_result = self._execute_selected_tool(tool_decision)
            used_tool_name = tool_decision.get("tool", "")

            if used_tool_name:
                all_used_tools.append(used_tool_name)

            accumulated_results.append(
                {
                    "step": iteration + 1,
                    "tool": used_tool_name,
                    "query": current_query,
                    "result": tool_result,
                }
            )

            # Îã§Ïùå Îã®Í≥Ñ ÏßàÏùò ÏÉùÏÑ±
            next_query = self._generate_next_query(user_input, accumulated_results)
            if not next_query or next_query == current_query:
                logger.info(f"Îã§Ïùå Îã®Í≥Ñ ÏßàÏùòÍ∞Ä ÏóÜÏñ¥ Ï¢ÖÎ£å")
                break

            current_query = next_query

        # ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
        final_response = self._generate_chain_response(user_input, accumulated_results)
        return final_response, all_used_tools

    def _ai_select_next_tool(
        self, current_query: str, previous_results: list, step: int
    ):
        """Îã§Ïùå Îã®Í≥ÑÏóêÏÑú ÏÇ¨Ïö©Ìï† ÎèÑÍµ¨Î•º AIÍ∞Ä ÏßÄÎä•Ï†ÅÏúºÎ°ú Í≤∞Ï†ï"""
        try:
            # ÎèÑÍµ¨ ÏÑ§Î™Ö ÏàòÏßë
            tools_info = []
            for tool in self.tools:
                desc = getattr(tool, "description", tool.name)
                tools_info.append(f"- {tool.name}: {desc}")

            tools_list = "\n".join(tools_info)

            # Ïù¥Ï†Ñ Í≤∞Í≥º ÏöîÏïΩ
            previous_summary = ""
            if previous_results:
                previous_summary = "\n\nPrevious steps:\n"
                for result in previous_results:
                    result_preview = (
                        str(result["result"])[:200] + "..."
                        if len(str(result["result"])) > 200
                        else str(result["result"])
                    )
                    previous_summary += f"Step {result['step']}: Used {result['tool']} -> {result_preview}\n"

            selection_prompt = f"""Current query: "{current_query}"
Step: {step + 1}

{previous_summary}

Available tools:
{tools_list}

Analyze the current query and previous results. Determine if additional information is needed to fully answer the user's request.

Think about:
- What information does the user ultimately want?
- What gaps exist in the current results?
- Which tool could provide the missing information?
- Are the results sufficient to answer the original question?

Response format:
- Use tool: TOOL: tool_name | PARAMS: {{"key": "value"}}
- No more tools needed: TOOL: none

Use your judgment to select the most appropriate tool and extract relevant parameters from the available information."""

            messages = [
                SystemMessage(
                    content="You are an intelligent assistant that can analyze information gaps and select appropriate tools to gather missing data. Use your reasoning to determine what additional information would be valuable to complete the user's request. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=selection_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._parse_tool_decision(response.content)

        except Exception as e:
            logger.error(f"AI Îã§Ïùå ÎèÑÍµ¨ ÏÑ†ÌÉù Ïò§Î•ò: {e}")
            return None

    def _generate_next_query(
        self, original_query: str, accumulated_results: list
    ) -> str:
        """Îã§Ïùå Îã®Í≥Ñ ÏßàÏùò ÏÉùÏÑ±"""
        try:
            results_summary = "\n".join(
                [
                    f"Step {r['step']}: {r['tool']} -> {str(r['result'])[:300]}..."
                    for r in accumulated_results
                ]
            )

            prompt = f"""Original user query: "{original_query}"

Results so far:
{results_summary}

Analyze what information is still missing to fully satisfy the user's request. 

If additional specific information is needed, generate a focused query for the next step.
If the current results are sufficient to answer the original query, respond with "COMPLETE".

Next query:"""

            messages = [
                SystemMessage(
                    content="You are an intelligent assistant that can identify information gaps and determine what additional data is needed to complete a user's request. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=prompt),
            ]

            response = self.llm.invoke(messages)
            next_query = response.content.strip()

            if "COMPLETE" in next_query.upper():
                return None

            return next_query

        except Exception as e:
            logger.error(f"Îã§Ïùå ÏßàÏùò ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return None

    def _generate_chain_response(
        self, original_query: str, accumulated_results: list
    ) -> str:
        """Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±"""
        try:
            if not accumulated_results:
                return self.simple_chat(original_query)

            # Î™®Îì† Í≤∞Í≥º Ìï©ÏπòÍ∏∞
            all_results = "\n\n".join(
                [
                    f"Step {r['step']} ({r['tool']}):\n{r['result']}"
                    for r in accumulated_results
                ]
            )

            response_prompt = f"""User's original request: "{original_query}"

Information gathered through multiple tools:
{all_results}

Your task:
1. Synthesize all the information to provide a comprehensive answer
2. Organize the information logically and clearly
3. Focus on what the user actually wanted to know
4. Present the information in a natural, conversational Korean format
5. If location information is available, include addresses and coordinates
6. If business information is available, include names, addresses, and contact details

Provide a helpful, well-organized response in Korean that directly addresses the user's original request."""

            messages = [
                SystemMessage(
                    content="You are an expert at synthesizing information from multiple sources to provide comprehensive answers in Korean. When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row."
                ),
                HumanMessage(content=response_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._format_response(response.content)

        except Exception as e:
            logger.error(f"Ï≤¥Ïù∏ ÏùëÎãµ ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return self._format_response(
                "Ïó¨Îü¨ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÎ≥¥Î•º ÏàòÏßëÌñàÏßÄÎßå ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
            )

    def _perplexity_tool_chat(self, user_input: str) -> tuple[str, list]:
        """ÌçºÌîåÎ†âÏãúÌã∞ Î™®Îç∏Ïö© AI Í∏∞Î∞ò ÎèÑÍµ¨ ÏÑ†ÌÉù Î∞è Ïã§Ìñâ - Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏßÄÏõê"""
        try:
            if self._is_tool_list_request(user_input):
                return self._show_tool_list(), []

            # ÏßÅÏ†ë ÏóêÏù¥Ï†ÑÌä∏ Ïã§ÌñâÍ∏∞ ÏÉùÏÑ± Î∞è Ïã§Ìñâ ÏãúÎèÑ
            try:
                logger.info("üîß ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ïã§ÌñâÍ∏∞ ÏÉùÏÑ± ÏãúÏûë")
                agent_create_start = time.time()
                agent_executor = self._create_agent_executor()
                logger.info(
                    f"üîß ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ïã§ÌñâÍ∏∞ ÏÉùÏÑ± ÏôÑÎ£å: {time.time() - agent_create_start:.2f}Ï¥à"
                )
                
                if not agent_executor:
                    return "ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§.", []
                
                logger.info("üîß ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ ÏãúÏûë")
                agent_invoke_start = time.time()
                result = agent_executor.invoke({"input": user_input})
                logger.info(
                    f"üîß ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ ÏôÑÎ£å: {time.time() - agent_invoke_start:.2f}Ï¥à"
                )
                
                output = result.get("output", "")
                
                # ÏÇ¨Ïö©Îêú ÎèÑÍµ¨ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                used_tools = []
                if "intermediate_steps" in result:
                    for step in result["intermediate_steps"]:
                        if len(step) >= 2 and hasattr(step[0], "tool"):
                            used_tools.append(step[0].tool)
                
                if "Agent stopped" in output or not output.strip():
                    logger.warning("ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ï§ëÎã® Í∞êÏßÄ, Í≤∞Í≥º ÌôïÏù∏ Ï§ë...")
                    
                    # Ï§ëÍ∞Ñ Îã®Í≥ÑÏóêÏÑú ÎèÑÍµ¨Í∞Ä ÏÇ¨Ïö©ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
                    if "intermediate_steps" in result and result["intermediate_steps"]:
                        # ÎèÑÍµ¨Í∞Ä ÏÇ¨Ïö©ÎêòÏóàÏúºÎ©¥ ÎßàÏßÄÎßâ ÎèÑÍµ¨ Í≤∞Í≥º ÌôïÏù∏
                        last_step = result["intermediate_steps"][-1]
                        if len(last_step) >= 2:
                            tool_result = last_step[1]
                            logger.info(f"ÎèÑÍµ¨ Í≤∞Í≥º Î∞úÍ≤¨: {str(tool_result)[:100]}...")
                            
                            # ÎèÑÍµ¨ Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
                            final_response = self._generate_perplexity_final_response(user_input, str(tool_result))
                            return final_response, [last_step[0].tool]
                
                # Ï†ïÏÉÅ Ï∂úÎ†•Ïù¥ ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú Î∞òÌôò
                if output.strip():
                    return output, used_tools
                
                # Ï∂úÎ†•Ïù¥ ÏóÜÏúºÎ©¥ Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏãúÎèÑ
                logger.warning("ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ï∂úÎ†• ÏóÜÏùå, Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏãúÎèÑ")
                return self._execute_perplexity_tool_chain(user_input)
                
            except Exception as agent_error:
                logger.error(f"ÌçºÌîåÎ†âÏãúÌã∞ ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ Ïò§Î•ò: {agent_error}, Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏãúÎèÑ")
                # ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ Ïã§Ìå® Ïãú Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© ÏãúÎèÑ
                return self._execute_perplexity_tool_chain(user_input)

        except Exception as e:
            logger.error(f"ÌçºÌîåÎ†âÏãúÌã∞ ÎèÑÍµ¨ Ï±ÑÌåÖ Ïò§Î•ò: {e}")
            return self.simple_chat(user_input), []

    def _execute_perplexity_tool_chain(
        self, user_input: str, max_iterations: int = 3
    ) -> tuple[str, list]:
        """ÌçºÌîåÎ†âÏãúÌã∞ Î™®Îç∏ÏùÑ ÏúÑÌïú Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© Ïã§Ìñâ"""
        all_used_tools = []
        accumulated_results = []
        current_query = user_input

        for iteration in range(max_iterations):
            logger.info(f"ÎèÑÍµ¨ Ï≤¥Ïù∏ {iteration + 1}Îã®Í≥Ñ ÏãúÏûë: {current_query[:50]}...")

            # AIÍ∞Ä Îã§Ïùå ÎèÑÍµ¨ Í≤∞Ï†ï
            tool_decision = self._ai_select_perplexity_tool(
                current_query, accumulated_results, iteration
            )

            if not tool_decision or tool_decision.get("tool") == "none":
                logger.info(f"ÎèÑÍµ¨ Ï≤¥Ïù∏ {iteration + 1}Îã®Í≥ÑÏóêÏÑú Ï¢ÖÎ£å")
                break

            # ÎèÑÍµ¨ Ïã§Ìñâ
            tool_result = self._execute_selected_tool(tool_decision)
            used_tool_name = tool_decision.get("tool", "")

            if used_tool_name:
                all_used_tools.append(used_tool_name)

            accumulated_results.append(
                {
                    "step": iteration + 1,
                    "tool": used_tool_name,
                    "query": current_query,
                    "result": tool_result,
                }
            )

            # Îã§Ïùå Îã®Í≥Ñ ÏßàÏùò ÏÉùÏÑ±
            next_query = self._generate_perplexity_next_query(
                user_input, accumulated_results
            )
            if not next_query or next_query == current_query:
                logger.info(f"Îã§Ïùå Îã®Í≥Ñ ÏßàÏùòÍ∞Ä ÏóÜÏñ¥ Ï¢ÖÎ£å")
                break

            current_query = next_query

        # ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
        final_response = self._generate_perplexity_chain_response(
            user_input, accumulated_results
        )
        return final_response, all_used_tools

    def _ai_select_perplexity_tool(
        self, current_query: str, previous_results: list, step: int
    ):
        """ÌçºÌîåÎ†âÏãúÌã∞ Î™®Îç∏ÏùÑ ÏúÑÌïú ÎèÑÍµ¨ ÏÑ†ÌÉù"""
        try:
            # ÎèÑÍµ¨ ÏÑ§Î™Ö ÏàòÏßë
            tools_info = []
            for tool in self.tools:
                desc = getattr(tool, "description", tool.name)
                tools_info.append(f"- {tool.name}: {desc}")

            tools_list = "\n".join(tools_info)

            # Ïù¥Ï†Ñ Í≤∞Í≥º ÏöîÏïΩ
            previous_summary = ""
            if previous_results:
                previous_summary = "\n\nÏù¥Ï†Ñ Îã®Í≥Ñ:\n"
                for result in previous_results:
                    result_preview = (
                        str(result["result"])[:200] + "..."
                        if len(str(result["result"])) > 200
                        else str(result["result"])
                    )
                    previous_summary += f"Îã®Í≥Ñ {result['step']}: {result['tool']} ÏÇ¨Ïö© -> {result_preview}\n"

            selection_prompt = f"""ÌòÑÏû¨ ÏßàÏùò: "{current_query}"
Îã®Í≥Ñ: {step + 1}

{previous_summary}

ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨:
{tools_list}

ÌòÑÏû¨ ÏßàÏùòÏôÄ Ïù¥Ï†Ñ Í≤∞Í≥ºÎ•º Î∂ÑÏÑùÌïòÏó¨ ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠Ïóê ÏôÑÏ†ÑÌûà ÎãµÎ≥ÄÌïòÎäî Îç∞ ÌïÑÏöîÌïú Ï∂îÍ∞Ä Ï†ïÎ≥¥Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî.

Í≥†Î†§Ìï† ÏÇ¨Ìï≠:
- ÏÇ¨Ïö©ÏûêÍ∞Ä ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú ÏõêÌïòÎäî Ï†ïÎ≥¥Îäî Î¨¥ÏóáÏù∏Í∞Ä?
- ÌòÑÏû¨ Í≤∞Í≥ºÏóêÏÑú Ïñ¥Îñ§ Ï†ïÎ≥¥Í∞Ä Î∂ÄÏ°±ÌïúÍ∞Ä?
- Ïñ¥Îñ§ ÎèÑÍµ¨Í∞Ä Î∂ÄÏ°±Ìïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï† Ïàò ÏûàÎäîÍ∞Ä?
- ÏõêÎûò ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÍ∏∞Ïóê Í≤∞Í≥ºÍ∞Ä Ï∂©Î∂ÑÌïúÍ∞Ä?

ÏùëÎãµ ÌòïÏãù:
- ÎèÑÍµ¨ ÏÇ¨Ïö©: TOOL: tool_name | PARAMS: {{"key": "value"}}
- Îçî Ïù¥ÏÉÅ ÎèÑÍµ¨Í∞Ä ÌïÑÏöîÌïòÏßÄ ÏïäÏùå: TOOL: none

Í∞ÄÏû• Ï†ÅÏ†àÌïú ÎèÑÍµ¨Î•º ÏÑ†ÌÉùÌïòÍ≥† ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ï†ïÎ≥¥ÏóêÏÑú Í¥ÄÎ†® ÌååÎùºÎØ∏ÌÑ∞Î•º Ï∂îÏ∂úÌïòÏÑ∏Ïöî."""

            # Perplexity Î™®Îç∏ÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï∂îÍ∞Ä
            mcp_system_prompt = SystemPrompts.get_perplexity_mcp_prompt()
            messages = [
                SystemMessage(content=mcp_system_prompt),
                HumanMessage(content=selection_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._parse_tool_decision(response.content)

        except Exception as e:
            logger.error(f"Perplexity ÎèÑÍµ¨ ÏÑ†ÌÉù Ïò§Î•ò: {e}")
            return None

    def _generate_perplexity_next_query(
        self, original_query: str, accumulated_results: list
    ) -> str:
        """ÌçºÌîåÎ†âÏãúÌã∞ Î™®Îç∏ÏùÑ ÏúÑÌïú Îã§Ïùå Îã®Í≥Ñ ÏßàÏùò ÏÉùÏÑ±"""
        try:
            results_summary = "\n".join(
                [
                    f"Îã®Í≥Ñ {r['step']}: {r['tool']} -> {str(r['result'])[:300]}..."
                    for r in accumulated_results
                ]
            )

            prompt = f"""ÏõêÎûò ÏÇ¨Ïö©Ïûê ÏßàÏùò: "{original_query}"

ÏßÄÍ∏àÍπåÏßÄÏùò Í≤∞Í≥º:
{results_summary}

ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠ÏùÑ ÏôÑÏ†ÑÌûà Ï∂©Ï°±ÏãúÌÇ§Í∏∞ ÏúÑÌï¥ Ïñ¥Îñ§ Ï†ïÎ≥¥Í∞Ä Ïó¨Ï†ÑÌûà Î∂ÄÏ°±ÌïúÏßÄ Î∂ÑÏÑùÌïòÏÑ∏Ïöî.

Ï∂îÍ∞ÄÏ†ÅÏù∏ Íµ¨Ï≤¥Ï†Å Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌïòÎã§Î©¥, Îã§Ïùå Îã®Í≥ÑÎ•º ÏúÑÌïú ÏßëÏ§ëÏ†ÅÏù∏ ÏßàÏùòÎ•º ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.
ÌòÑÏû¨ Í≤∞Í≥ºÍ∞Ä ÏõêÎûò ÏßàÏùòÏóê ÎãµÎ≥ÄÌïòÍ∏∞Ïóê Ï∂©Î∂ÑÌïòÎã§Î©¥, "COMPLETE"Î°ú ÏùëÎãµÌïòÏÑ∏Ïöî.

Îã§Ïùå ÏßàÏùò:"""

            # Perplexity Î™®Îç∏ÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï∂îÍ∞Ä
            mcp_system_prompt = SystemPrompts.get_perplexity_mcp_prompt()
            messages = [
                SystemMessage(content=mcp_system_prompt),
                HumanMessage(content=prompt),
            ]

            response = self.llm.invoke(messages)
            next_query = response.content.strip()

            if "COMPLETE" in next_query.upper():
                return None

            return next_query

        except Exception as e:
            logger.error(f"Perplexity Îã§Ïùå ÏßàÏùò ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return None

    def _generate_perplexity_final_response(self, user_input: str, tool_result: str) -> str:
        """ÎèÑÍµ¨ Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÌçºÌîåÎ†âÏãúÌã∞ Î™®Îç∏ÏùÑ ÏúÑÌïú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±"""
        try:
            # ÎèÑÍµ¨ Í≤∞Í≥ºÍ∞Ä ÎÑàÎ¨¥ Í∏∏Î©¥ ÏöîÏïΩ
            if len(tool_result) > 3000:
                tool_result = tool_result[:3000] + "...(ÏÉùÎûµ)"

            response_prompt = f"""ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏: "{user_input}"

ÎèÑÍµ¨ÏóêÏÑú Í∞ÄÏ†∏Ïò® Îç∞Ïù¥ÌÑ∞:
{tool_result}

ÏàòÌñâÌï† ÏûëÏóÖ:
1. ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏Ïóê ÏßÅÏ†ëÏ†ÅÏúºÎ°ú ÎãµÎ≥ÄÌïòÎäî Í∞ÄÏû• Í¥ÄÎ†®ÏÑ± ÎÜíÏùÄ Ï†ïÎ≥¥ Ï∂îÏ∂ú
2. Ï†ïÎ≥¥Î•º ÎÖºÎ¶¨Ï†ÅÏù¥Í≥† Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å Íµ¨ÏÑ±
3. ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥Î°ú ÏπúÍµ¨ÏóêÍ≤å ÏÑ§Î™ÖÌïòÎìØ ÏûëÏÑ±
4. ÏÇ¨Ïö©ÏûêÍ∞Ä Ïã§Ï†úÎ°ú ÏïåÍ≥† Ïã∂Ïñ¥ÌïòÎäî ÎÇ¥Ïö©Ïóê ÏßëÏ§ë
5. Í∏∞Ïà†Ï†Å Ïö©Ïñ¥ ÏóÜÏù¥ Í∞ÑÍ≤∞ÌïòÍ≥† Î™ÖÌôïÌïú Î¨∏Ïû• ÏÇ¨Ïö©
6. Ïó¨Îü¨ Ï†ïÎ≥¥Í∞Ä ÏûàÎäî Í≤ΩÏö∞ Í∞ÄÏû• Ï§ëÏöîÌïú Ï†ïÎ≥¥Î•º Ïö∞ÏÑ†ÏàúÏúÑÎ°ú Î∞∞Ïπò
7. Í∞ÑÎã®Ìïú ÎßàÌÅ¨Îã§Ïö¥(## Ï†úÎ™©, **Í∞ïÏ°∞**, - Í∏ÄÎ®∏Î¶¨ Í∏∞Ìò∏)Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏùëÎãµ ÌòïÏãùÌôî
8. ÌòïÏãùÏùÑ ÏµúÏÜåÌôîÌïòÍ≥† ÍπîÎÅîÌïòÍ≤å Ïú†ÏßÄ
9. Î™©Î°ùÍ≥º Íµ¨Ï°∞ÌôîÎêú Îç∞Ïù¥ÌÑ∞Ïóê ÏùºÍ¥ÄÎêú Îì§Ïó¨Ïì∞Í∏∞ ÏÇ¨Ïö©

ÏÇ¨Ïö©ÏûêÍ∞Ä ÏïåÍ≥† Ïã∂Ïñ¥ÌñàÎçò ÎÇ¥Ïö©Ïóê ÏßÅÏ†ëÏ†ÅÏúºÎ°ú ÎåÄÏùëÌïòÎäî ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏùò ÎèÑÏõÄÎêòÍ≥† ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ ÏùëÎãµÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî."""

            # Perplexity Î™®Îç∏ÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï∂îÍ∞Ä
            mcp_system_prompt = SystemPrompts.get_perplexity_mcp_prompt()
            messages = [
                SystemMessage(content=mcp_system_prompt),
                HumanMessage(content=response_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._format_response(response.content)

        except Exception as e:
            logger.error(f"Perplexity ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ïò§Î•ò: {e}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú ÏõêÎ≥∏ ÎèÑÍµ¨ Í≤∞Í≥º Î∞òÌôò
            return self._format_response(
                f"""ÎèÑÍµ¨ Í≤∞Í≥ºÎ•º Ï≤òÎ¶¨ÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÏõêÎ≥∏ Í≤∞Í≥ºÎ•º ÌëúÏãúÌï©ÎãàÎã§:

{tool_result}"""
            )

    def _generate_perplexity_chain_response(
        self, original_query: str, accumulated_results: list
    ) -> str:
        """ÌçºÌîåÎ†âÏãúÌã∞ Î™®Îç∏ÏùÑ ÏúÑÌïú Ïó∞ÏáÑÏ†Å ÎèÑÍµ¨ ÏÇ¨Ïö© Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±"""
        try:
            if not accumulated_results:
                return self.simple_chat(original_query)

            # Î™®Îì† Í≤∞Í≥º Ìï©ÏπòÍ∏∞
            all_results = "\n\n".join(
                [
                    f"Îã®Í≥Ñ {r['step']} ({r['tool']}):\n{r['result']}"
                    for r in accumulated_results
                ]
            )

            # Í≤∞Í≥ºÍ∞Ä ÎÑàÎ¨¥ Í∏∏Î©¥ ÏöîÏïΩ
            if len(all_results) > 3000:
                all_results = all_results[:3000] + "...(ÏÉùÎûµ)"

            response_prompt = f"""ÏÇ¨Ïö©ÏûêÏùò ÏõêÎûò ÏöîÏ≤≠: "{original_query}"

Ïó¨Îü¨ ÎèÑÍµ¨Î•º ÌÜµÌï¥ ÏàòÏßëÌïú Ï†ïÎ≥¥:
{all_results}

ÏàòÌñâÌï† ÏûëÏóÖ:
1. Î™®Îì† Ï†ïÎ≥¥Î•º Ï¢ÖÌï©ÌïòÏó¨ Ìè¨Í¥ÑÏ†ÅÏù∏ ÎãµÎ≥Ä Ï†úÍ≥µ
2. Ï†ïÎ≥¥Î•º ÎÖºÎ¶¨Ï†ÅÏù¥Í≥† Î™ÖÌôïÌïòÍ≤å Íµ¨ÏÑ±
3. ÏÇ¨Ïö©ÏûêÍ∞Ä Ïã§Ï†úÎ°ú ÏïåÍ≥† Ïã∂Ïñ¥ÌñàÎçò ÎÇ¥Ïö©Ïóê ÏßëÏ§ë
4. ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ ÌòïÏãùÏúºÎ°ú Ï†ïÎ≥¥ Ï†úÍ≥µ
5. ÏúÑÏπò Ï†ïÎ≥¥Í∞Ä ÏûàÎäî Í≤ΩÏö∞ Ï£ºÏÜåÏôÄ Ï¢åÌëú Ìè¨Ìï®
6. ÎπÑÏ¶àÎãàÏä§ Ï†ïÎ≥¥Í∞Ä ÏûàÎäî Í≤ΩÏö∞ Ïù¥Î¶Ñ, Ï£ºÏÜå, Ïó∞ÎùΩÏ≤ò Ìè¨Ìï®

ÏÇ¨Ïö©ÏûêÏùò ÏõêÎûò ÏöîÏ≤≠Ïóê ÏßÅÏ†ëÏ†ÅÏúºÎ°ú ÎåÄÏùëÌïòÎäî ÎèÑÏõÄÏù¥ ÎêòÍ≥† Ïûò Íµ¨ÏÑ±Îêú ÌïúÍµ≠Ïñ¥ ÏùëÎãµÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî."""

            # Perplexity Î™®Îç∏ÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï∂îÍ∞Ä
            mcp_system_prompt = SystemPrompts.get_perplexity_mcp_prompt()
            messages = [
                SystemMessage(content=mcp_system_prompt),
                HumanMessage(content=response_prompt),
            ]

            response = self.llm.invoke(messages)
            return self._format_response(response.content)

        except Exception as e:
            logger.error(f"Perplexity Ï≤¥Ïù∏ ÏùëÎãµ ÏÉùÏÑ± Ïò§Î•ò: {e}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú ÎßàÏßÄÎßâ ÎèÑÍµ¨ Í≤∞Í≥º Î∞òÌôò
            if accumulated_results:
                last_result = accumulated_results[-1]['result']
                return self._format_response(
                    f"""Ïó¨Îü¨ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÎ≥¥Î•º ÏàòÏßëÌñàÏßÄÎßå ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÎßàÏßÄÎßâ ÎèÑÍµ¨ Í≤∞Í≥ºÎ•º ÌëúÏãúÌï©ÎãàÎã§:

{last_result}"""
                )
            else:
                return self._format_response(
                    "Ïó¨Îü¨ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÎ≥¥Î•º ÏàòÏßëÌñàÏßÄÎßå ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
                )
