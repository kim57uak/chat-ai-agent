from core.ai_agent_refactored import AIAgent
from core.streaming_processor import StreamingChatProcessor, ChunkedResponseProcessor
from core.llm_factory import LLMFactoryProvider
import logging

logger = logging.getLogger(__name__)


class AIClient:
    """AI í´ë¼ì´ì–¸íŠ¸ - ì—ì´ì „íŠ¸ ê¸°ë°˜ ì±„íŒ…"""

    def __init__(self, api_key, model_name="gpt-3.5-turbo"):
        self.api_key = api_key
        self.model_name = model_name
        self.agent = AIAgent(api_key, model_name)
        self.conversation_history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥
        
        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.streaming_processor = StreamingChatProcessor()
        self.chunked_processor = ChunkedResponseProcessor()
        self.streaming_llm = None  # ìŠ¤íŠ¸ë¦¬ë°ìš© LLM

        # ì„¤ì • íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ ì„¤ì • ë¡œë“œ
        from core.file_utils import load_config

        config = load_config()
        conv_settings = config.get("conversation_settings", {})

        self.max_history_pairs = conv_settings.get("max_history_pairs", 5)
        self.max_tokens_estimate = conv_settings.get("max_tokens_estimate", 2000)
        self.enable_history = conv_settings.get("enable_history", True)
        self.token_optimization = conv_settings.get("token_optimization", True)

        # ìœ ì € í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self.user_prompt = self._load_user_prompt()

    def chat(self, messages):
        """í† í° ìµœì í™”ëœ ëŒ€í™” ê¸°ë¡ ì‚¬ìš© (í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í• ) - ì•ˆì •ì„± ê°œì„ """
        try:
            # ì…ë ¥ ê²€ì¦
            if not messages or not isinstance(messages, list):
                return "ìœ íš¨í•˜ì§€ ì•Šì€ ë©”ì‹œì§€ í˜•ì‹ì…ë‹ˆë‹¤."

            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_message = ""
            for msg in reversed(messages):
                if isinstance(msg, dict) and msg.get("role") == "user":
                    user_message = msg.get("content", "").strip()
                    if user_message:
                        break

            # ì´ë¯¸ì§€ ë°ì´í„° ê°ì§€ (ì¤„ë°”ê¿ˆ ë¬´ì‹œ)
            cleaned_message = user_message.replace("\n", "")
            has_start_tag = "[IMAGE_BASE64]" in cleaned_message
            has_end_tag = "[/IMAGE_BASE64]" in cleaned_message
            has_image_data = has_start_tag and has_end_tag
            
            if has_image_data:
                # ì´ë¯¸ì§€ OCRì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                ocr_prompt = """Please **extract all text accurately (OCR)** from this image.

**Required Tasks:**
1. **Complete Text Extraction**: Extract all Korean, English, numbers, and symbols without omission
2. **Structure Analysis**: Identify document structures like tables, lists, headings, paragraphs
3. **Layout Information**: Describe text position, size, and arrangement relationships
4. **Accurate Transcription**: Record all characters precisely without typos
5. **Context Description**: Identify document type and purpose
6. **Table Format**: When creating tables, use markdown format: |Header1|Header2|\n|---|---|\n|Data1|Data2|

**TABLE FORMAT RULES**: When creating tables from extracted data, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment.

**Response Format:**
## ğŸ“„ Extracted Text
[List all text accurately]

## ğŸ“‹ Document Structure
[Describe structure of tables, lists, headings, etc.]

## ğŸ“ Layout Information
[Text arrangement and positional relationships]

**Important**: Please extract all readable text from the image completely without any omissions."""
                
                # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ì— OCR í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                for i in range(len(messages) - 1, -1, -1):
                    if messages[i].get("role") == "user":
                        content = messages[i]["content"]
                        cleaned_content = content.replace("\n", "")
                        has_image_in_content = "[IMAGE_BASE64]" in cleaned_content
                        if has_image_in_content:
                            messages[i]["content"] = f"{ocr_prompt}\n\n{content}"
                        break
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ì˜ ê²½ìš° ê¸°ì¡´ ìœ ì € í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                user_prompt = self.get_current_user_prompt()
                if user_prompt and user_message:
                    enhanced_message = f"{user_prompt}\n\n{user_message}"
                    for i in range(len(messages) - 1, -1, -1):
                        if messages[i].get("role") == "user":
                            messages[i]["content"] = enhanced_message
                            break

            if not user_message:
                return "ì²˜ë¦¬í•  ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.info(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì‹œì‘: {user_message[:50]}...")

            # ëŒ€í™” ê¸°ë¡ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            if not self.enable_history:
                return self._process_with_quota_handling(user_message, [])

            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ ë° ìµœì í™”
            self.conversation_history = [
                msg for msg in messages if isinstance(msg, dict)
            ]
            optimized_history = (
                self._optimize_conversation_history()
                if self.token_optimization
                else self.conversation_history.copy()
            )

            return self._process_with_quota_handling(user_message, optimized_history)

        except Exception as e:
            error_msg = str(e)
            logger.error(f"AI í´ë¼ì´ì–¸íŠ¸ ì±„íŒ… ì˜¤ë¥˜: {error_msg}")
            return f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg[:100]}..."

    def _process_with_quota_handling(
        self, user_message: str, history: list, force_agent: bool = False
    ):
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í•  ì²˜ë¦¬ - ë‹¨ìˆœí™”"""
        import time

        try:
            start_time = time.time()
            mode_info = " (Agent ëª¨ë“œ)" if force_agent else " (Ask ëª¨ë“œ)"
            logger.info(
                f"AI ìš”ì²­ ì‹œì‘{mode_info}: {self.model_name} (íˆìŠ¤í† ë¦¬: {len(history)}ê°œ)"
            )

            response, used_tools = self.agent.process_message_with_history(
                user_message, history, force_agent
            )

            elapsed_time = time.time() - start_time

            if used_tools:
                logger.info(
                    f"ë„êµ¬ ì‚¬ìš© ì‘ë‹µ ì™„ë£Œ{mode_info}: {self.model_name} ({elapsed_time:.1f}ì´ˆ) - ë„êµ¬: {used_tools}"
                )
            else:
                logger.info(
                    f"ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ì™„ë£Œ{mode_info}: {self.model_name} ({elapsed_time:.1f}ì´ˆ)"
                )

            return response, used_tools

        except Exception as e:
            error_msg = str(e)

            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜ ê°ì§€
            if "429" in error_msg and "quota" in error_msg.lower():
                logger.warning(
                    f"í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€, ì²­í¬ ë¶„í•  ì²˜ë¦¬ ì‹œë„: {self.model_name}"
                )
                return self._handle_quota_exceeded(user_message, history)

            # ì—°ê²° ì˜¤ë¥˜ ê°ì§€
            elif any(
                keyword in error_msg.lower()
                for keyword in ["connection", "timeout", "network"]
            ):
                logger.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {self.model_name} - {error_msg}")
                return "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", []

            else:
                logger.error(f"AI í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {self.model_name} - {error_msg}")
                raise e

    def _handle_quota_exceeded(self, user_message: str, history: list):
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í•  ì²˜ë¦¬"""
        # ê¸´ ë©”ì‹œì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        if len(user_message) > 1000:
            chunks = self._split_message_into_chunks(user_message, 800)
            responses = []

            for i, chunk in enumerate(chunks):
                try:
                    # ê° ì²­í¬ë¥¼ ê°„ë‹¨í•œ íˆìŠ¤í† ë¦¬ë¡œ ì²˜ë¦¬
                    simple_history = history[-2:] if history else []  # ìµœê·¼ 2ê°œë§Œ
                    chunk_response, _ = self.agent.process_message_with_history(
                        f"[{i+1}/{len(chunks)}] {chunk}",
                        simple_history,
                        force_agent=True,
                    )
                    responses.append(chunk_response)

                except Exception as chunk_error:
                    logger.error(f"ì²­í¬ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {chunk_error}")
                    responses.append(
                        f"[ì²­í¬ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(chunk_error)[:100]}...]"
                    )

            return "\n\n".join(responses), []

        else:
            # ì§§ì€ ë©”ì‹œì§€ëŠ” íˆìŠ¤í† ë¦¬ ì—†ì´ ì²˜ë¦¬
            try:
                response, used_tools = self.agent.process_message(user_message)
                logger.info(f"í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ íˆìŠ¤í† ë¦¬ ì—†ì´ ì²˜ë¦¬: {self.model_name}")
                return response, used_tools
            except Exception as fallback_error:
                return (
                    f"í• ë‹¹ëŸ‰ ì´ˆê³¼ ë° ëŒ€ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {str(fallback_error)[:200]}...",
                    [],
                )

    def _split_message_into_chunks(self, message: str, chunk_size: int = 800):
        """ë©”ì‹œì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if len(message) <= chunk_size:
            return [message]

        chunks = []
        sentences = message.split(". ")
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk + sentence) <= chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def agent_chat(self, user_input: str):
        """ì—ì´ì „íŠ¸ ì±„íŒ… (í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í• ) - ì•ˆì •ì„± ê°œì„ """
        try:
            # ì…ë ¥ ê²€ì¦
            if not user_input or not isinstance(user_input, str):
                return "ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤.", []

            user_input = user_input.strip()
            if not user_input:
                return "ë¹ˆ ë©”ì‹œì§€ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            logger.info(f"ì—ì´ì „íŠ¸ ì±„íŒ… ìš”ì²­: {user_input[:50]}...")

            # ëŒ€í™” ê¸°ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ
            if not self.conversation_history:
                from core.conversation_history import ConversationHistory

                conv_hist = ConversationHistory()
                conv_hist.load_from_file()
                recent_messages = conv_hist.get_recent_messages(10)
                self.conversation_history = recent_messages
                logger.info(f"íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë“œ: {len(recent_messages)}ê°œ")

            optimized_history = self._optimize_conversation_history()
            logger.info(f"ìµœì í™”ëœ íˆìŠ¤í† ë¦¬ ì‚¬ìš©: {len(optimized_history)}ê°œ")

            result = self._process_with_quota_handling(
                user_input, optimized_history, force_agent=True
            )

            # ê²°ê³¼ê°€ íŠœí”Œì¸ì§€ í™•ì¸
            if isinstance(result, tuple):
                return result
            else:
                return result, []

        except Exception as e:
            error_msg = str(e)
            logger.error(f"ì—ì´ì „íŠ¸ ì±„íŒ… ì˜¤ë¥˜: {error_msg}")
            return (
                f"ì—ì´ì „íŠ¸ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg[:100]}...",
                [],
            )

    def simple_chat(self, user_input: str):
        """ë‹¨ìˆœ ì±„íŒ… (ë„êµ¬ ì‚¬ìš© ì•ˆí•¨)"""
        try:
            # ì´ë¯¸ì§€ ë°ì´í„° ê°ì§€ ì‹œ OCR ìµœì í™” (ì¤„ë°”ê¿ˆ ë¬´ì‹œ)
            cleaned_input = user_input.replace("\n", "")
            has_start_tag = "[IMAGE_BASE64]" in cleaned_input
            has_end_tag = "[/IMAGE_BASE64]" in cleaned_input
            has_image_data = has_start_tag and has_end_tag
            
            if has_image_data:
                # ì´ë¯¸ì§€ OCRì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                ocr_prompt = """Please **extract all text accurately (OCR)** from this image.

**Required Tasks:**
1. **Complete Text Extraction**: Extract all Korean, English, numbers, and symbols without omission
2. **Structure Analysis**: Identify document structures like tables, lists, headings, paragraphs
3. **Layout Information**: Describe text position, size, and arrangement relationships
4. **Accurate Transcription**: Record all characters precisely without typos
5. **Context Description**: Identify document type and purpose
6. **Table Format**: When creating tables, use markdown format: |Header1|Header2|\n|---|---|\n|Data1|Data2|

**TABLE FORMAT RULES**: When creating tables from extracted data, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|. Never use tabs or spaces for table alignment.

**Response Format:**
## ğŸ“„ Extracted Text
[List all text accurately]

## ğŸ“‹ Document Structure
[Describe structure of tables, lists, headings, etc.]

## ğŸ“ Layout Information
[Text arrangement and positional relationships]

**Important**: Please extract all readable text from the image completely without any omissions."""
                
                enhanced_input = f"{ocr_prompt}\n\n{user_input}"
                return self.agent.simple_chat(enhanced_input)
            else:
                optimized_history = self._optimize_conversation_history()
                return self.agent.simple_chat_with_history(user_input, optimized_history)
        except Exception as e:
            logger.error(f"ë‹¨ìˆœ ì±„íŒ… ì˜¤ë¥˜: {e}")
            return f"ì˜¤ë¥˜: {e}"

    def _optimize_conversation_history(self):
        """ëŒ€í™” ê¸°ë¡ ìµœì í™” - í† í° ì‚¬ìš©ëŸ‰ ì ˆì•½"""
        if not self.conversation_history:
            return []

        # 1. ìµœê·¼ Nê°œ ëŒ€í™” ìŒë§Œ ìœ ì§€
        if len(self.conversation_history) > self.max_history_pairs * 2:
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ëŠ” í•­ìƒ ìœ ì§€
            recent_history = self.conversation_history[-(self.max_history_pairs * 2) :]
        else:
            recent_history = self.conversation_history.copy()

        # 2. í† í° ìˆ˜ ì¶”ì • ë° ì œí•œ
        optimized_history = self._limit_by_tokens(recent_history)

        return optimized_history

    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ëŒ€ëµ ì¶”ì • (í•œê¸€ 1ì = 1.5í† í°, ì˜ì–´ 1ë‹¨ì–´ = 1.3í† í°)"""
        if not text:
            return 0

        # í•œê¸€ê³¼ ì˜ì–´ ë¶„ë¦¬ ê³„ì‚°
        korean_chars = sum(1 for c in text if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3)
        english_words = len([w for w in text.split() if w.isascii()])
        other_chars = len(text) - korean_chars

        estimated_tokens = int(
            korean_chars * 1.5 + english_words * 1.3 + other_chars * 0.8
        )
        return max(estimated_tokens, len(text) // 4)  # ìµœì†Œê°’ ë³´ì¥

    def _limit_by_tokens(self, history: list) -> list:
        """í† í° ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ ì œí•œ"""
        if not history:
            return []

        total_tokens = 0
        limited_history = []

        # ë§ˆì§€ë§‰ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬
        for msg in reversed(history):
            content = msg.get("content", "")
            msg_tokens = self._estimate_tokens(content)

            if total_tokens + msg_tokens > self.max_tokens_estimate:
                break

            limited_history.insert(0, msg)
            total_tokens += msg_tokens

        logger.info(
            f"ëŒ€í™” ê¸°ë¡ ìµœì í™”: {len(history)}ê°œ -> {len(limited_history)}ê°œ (ì˜ˆìƒ í† í°: {total_tokens})"
        )
        return limited_history

    def _load_user_prompt(self):
        """ìœ ì € í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        try:
            from core.file_utils import load_config

            config = load_config()
            user_prompts = config.get(
                "user_prompt",
                {
                    "gpt": "Please follow these rules: 1. Structured responses 2. Prioritize readability 3. Clear categorization 4. Key summaries 5. Use Korean language 6. **TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|\n|Data4|Data5|Data6|. Never use tabs or spaces for table alignment. Always include the header separator row with dashes. 7. When asked about 'MCP tools', 'active tools', or 'available tools', use the get_all_mcp_tools function to show current MCP server tools",
                    "gemini": "Please follow these rules: 1. Structured responses 2. Prioritize readability 3. Clear categorization 4. Key summaries 5. Use Korean language 6. **TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|\n|Data4|Data5|Data6|. Never use tabs or spaces for table alignment. Always include the header separator row with dashes. 7. When asked about 'MCP tools', 'active tools', or 'available tools', use the get_all_mcp_tools function to show current MCP server tools",
                },
            )
            print(f"[ë””ë²„ê·¸] ë¡œë“œëœ ìœ ì € í”„ë¡¬í”„íŠ¸: {user_prompts}")
            return user_prompts
        except Exception as e:
            print(f"[ë””ë²„ê·¸] ìœ ì € í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {
                "gpt": "Please follow these rules: 1. Structured responses 2. Prioritize readability 3. Clear categorization 4. Key summaries 5. Use Korean language 6. **TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|\n|Data4|Data5|Data6|. Never use tabs or spaces for table alignment. Always include the header separator row with dashes. 7. When asked about 'MCP tools', 'active tools', or 'available tools', use the get_all_mcp_tools function to show current MCP server tools",
                "gemini": "Please follow these rules: 1. Structured responses 2. Prioritize readability 3. Clear categorization 4. Key summaries 5. Use Korean language 6. **TABLE FORMAT RULES**: When creating tables, ALWAYS use proper markdown table format with pipe separators and header separator row. Format: |Header1|Header2|Header3|\n|---|---|---|\n|Data1|Data2|Data3|\n|Data4|Data5|Data6|. Never use tabs or spaces for table alignment. Always include the header separator row with dashes. 7. When asked about 'MCP tools', 'active tools', or 'available tools', use the get_all_mcp_tools function to show current MCP server tools",
            }

    def get_current_user_prompt(self):
        """í˜„ì¬ ëª¨ë¸ì— ë§ëŠ” ìœ ì € í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        if "gemini" in self.model_name.lower():
            return self.user_prompt.get("gemini", "")
        else:
            return self.user_prompt.get("gpt", "")

    def update_user_prompt(self, prompt_text, model_type="both"):
        """ìœ ì € í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
        if model_type == "both":
            self.user_prompt["gpt"] = prompt_text
            self.user_prompt["gemini"] = prompt_text
        elif model_type == "gpt":
            self.user_prompt["gpt"] = prompt_text
        elif model_type == "gemini":
            self.user_prompt["gemini"] = prompt_text

        # ì„¤ì • íŒŒì¼ì— ì €ì¥
        self._save_user_prompt()

    def _save_user_prompt(self):
        """ìœ ì € í”„ë¡¬í”„íŠ¸ ì €ì¥"""
        try:
            from core.file_utils import load_config, save_config

            config = load_config()
            config["user_prompt"] = self.user_prompt
            save_config(config)
            logger.info(f"ìœ ì € í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ: {self.user_prompt}")
        except Exception as e:
            logger.error(f"ìœ ì € í”„ë¡¬í”„íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            print(f"ìœ ì € í”„ë¡¬í”„íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def streaming_chat(
        self, 
        user_input: str, 
        on_token=None, 
        on_complete=None, 
        on_error=None
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… - ëŒ€ìš©ëŸ‰ ì‘ë‹µ ì§€ì›"""
        try:
            # ìŠ¤íŠ¸ë¦¬ë°ìš© LLM ìƒì„± (í•œ ë²ˆë§Œ)
            if not self.streaming_llm:
                self.streaming_llm = LLMFactoryProvider.create_llm(
                    self.api_key, self.model_name, streaming=True
                )
                logger.info(f"ìŠ¤íŠ¸ë¦¬ë° LLM ìƒì„±: {self.model_name}")
            
            # ëŒ€í™” ê¸°ë¡ ìµœì í™”
            optimized_history = self._optimize_conversation_history()
            
            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘
            self.streaming_processor.process_streaming_chat(
                user_input=user_input,
                llm=self.streaming_llm,
                conversation_history=optimized_history,
                on_token=on_token,
                on_complete=on_complete,
                on_error=on_error
            )
            
        except Exception as e:
            error_msg = f"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì˜¤ë¥˜: {str(e)}"
            logger.error(error_msg)
            if on_error:
                on_error(error_msg)
    
    def cancel_streaming(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œ"""
        self.streaming_processor.cancel_current_stream()
        self.chunked_processor.cancel()
        logger.info("ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œ ìš”ì²­")
    
    def process_large_response(
        self, 
        response: str, 
        on_chunk=None, 
        on_complete=None
    ):
        """ëŒ€ìš©ëŸ‰ ì‘ë‹µ ì²˜ë¦¬ - ì²­í¬ ë‹¨ìœ„ ì „ì†¡"""
        self.chunked_processor.process_large_response(
            response=response,
            on_chunk=on_chunk,
            on_complete=on_complete
        )


# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì „ì—­ í´ë¼ì´ì–¸íŠ¸
mcp_client = None


def get_mcp_client():
    global mcp_client
    return mcp_client


def set_mcp_client(client):
    global mcp_client
    mcp_client = client
