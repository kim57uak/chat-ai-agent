from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple, Optional
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from core.enhanced_system_prompts import SystemPrompts
from core.file_utils import load_config
import logging
import re
import base64

logger = logging.getLogger(__name__)


class ChatProcessor(ABC):
    """ì±„íŒ… ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> str:
        pass


class SimpleChatProcessor(ChatProcessor):
    """ë‹¨ìˆœ ì±„íŒ… ì²˜ë¦¬ê¸°"""
    
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> str:
        """ì¼ë°˜ ì±„íŒ… (ë„êµ¬ ì‚¬ìš© ì—†ìŒ)"""
        try:
            # AI-driven system prompt selection based on content type
            if self._contains_image_data(user_input):
                system_content = SystemPrompts.get_image_analysis_prompt()
            else:
                system_content = SystemPrompts.get_general_chat_prompt()

            # Gemini ëª¨ë¸ì˜ ê²½ìš° ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¸ê°„ ë©”ì‹œì§€ë¡œ ë³€í™˜
            model_name = getattr(llm, 'model_name', str(llm))
            if 'gemini' in model_name.lower():
                messages = [HumanMessage(content=system_content)]
            else:
                messages = [SystemMessage(content=system_content)]

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if conversation_history:
                messages.extend(self._convert_history_to_messages(conversation_history, model_name))

                # ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬
            if self._contains_image_data(user_input):
                processed_input = self._process_image_input(user_input, model_name)
                messages.append(processed_input)
            else:
                messages.append(HumanMessage(content=user_input))

            response = llm.invoke(messages)
            response_content = response.content
            
            # ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©
            limited_response = self._limit_response_length(response_content)
            
            return limited_response

        except Exception as e:
            logger.error(f"ì¼ë°˜ ì±„íŒ… ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def _convert_history_to_messages(self, conversation_history: List[Dict], model_name: str):
        """ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        messages = []
        
        # ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì‚¬ìš©
        recent_history = (
            conversation_history[-6:]
            if len(conversation_history) > 6
            else conversation_history
        )

        for msg in recent_history:
            role = msg.get("role", "")
            content = msg.get("content", "")[:500]  # ë‚´ìš© ì œí•œ
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))

        return messages
    
    def _contains_image_data(self, user_input: str) -> bool:
        """Check if input contains image data"""
        return "[IMAGE_BASE64]" in user_input and "[/IMAGE_BASE64]" in user_input
    
    def _process_image_input(self, user_input: str, model_name: str):
        """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        image_match = re.search(
            r"\[IMAGE_BASE64\](.*?)\[/IMAGE_BASE64\]", user_input, re.DOTALL
        )
        if not image_match:
            return HumanMessage(content=user_input)

        image_data = image_match.group(1).strip()
        text_content = user_input.replace(image_match.group(0), "").strip()

        # Base64 ë°ì´í„° ê²€ì¦
        try:
            base64.b64decode(image_data)
        except Exception as e:
            logger.error(f"ì˜ëª»ëœ Base64 ì´ë¯¸ì§€ ë°ì´í„°: {e}")
            return HumanMessage(content="ì˜ëª»ëœ ì´ë¯¸ì§€ ë°ì´í„°ì…ë‹ˆë‹¤.")

        # AI-driven image analysis prompt
        if not text_content:
            text_content = """Analyze this image comprehensively and extract all information.

**Analysis Tasks:**
1. **Complete Text Extraction**: Extract all visible text with perfect accuracy
2. **Content Understanding**: Identify the type and purpose of the document/image
3. **Structure Analysis**: Describe layout, organization, and visual hierarchy
4. **Context Interpretation**: Explain what the image represents and its significance

**Response Requirements:**
- Extract ALL text without any omissions
- Organize information logically and clearly
- Use appropriate formatting (tables, lists, headings) based on content
- Provide context and interpretation where helpful
- Respond in Korean unless the content suggests otherwise

**Quality Standards:**
- Accuracy: 100% faithful text extraction
- Completeness: Cover all visible information
- Clarity: Well-organized, easy to understand presentation
- Intelligence: Apply appropriate formatting based on content type"""

        try:
            # Gemini ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ í˜•ì‹ ì‚¬ìš©
            if 'gemini' in model_name.lower():
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": f"data:image/jpeg;base64,{image_data}",
                        },
                    ]
                )
            else:
                # OpenAI GPT-4V í˜•ì‹
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            },
                        },
                    ]
                )
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return HumanMessage(
                content=f"{text_content}\n\n[ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}]"
            )
    
    def _limit_response_length(self, response: str) -> str:
        """ì‘ë‹µ ê¸¸ì´ ì œí•œ"""
        try:
            config = load_config()
            response_settings = config.get("response_settings", {})
            
            if not response_settings.get("enable_length_limit", True):
                return response
            
            max_length = response_settings.get("max_response_length", 8000)
            
            if len(response) <= max_length:
                return response
            
            logger.warning(f"ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©: {len(response)}ì -> {max_length}ì")
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ìë¥´ê¸°
            truncated = response[:max_length]
            last_period = truncated.rfind('.')
            last_newline = truncated.rfind('\n')
            
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°
            cut_point = max(last_period, last_newline)
            if cut_point > max_length * 0.8:  # 80% ì´ìƒì—ì„œ ì°¾ì€ ê²½ìš°ë§Œ ì‚¬ìš©
                truncated = response[:cut_point + 1]
            
            return truncated + "\n\n[ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.]"
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ê¸¸ì´ ì œí•œ ì˜¤ë¥˜: {e}")
            return response


class ToolChatProcessor(ChatProcessor):
    """ë„êµ¬ ì‚¬ìš© ì±„íŒ… ì²˜ë¦¬ê¸°"""
    
    def __init__(self, tools: List[Any], agent_executor_factory):
        self.tools = tools
        self.agent_executor_factory = agent_executor_factory
        self.agent_executor = None
    
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> Tuple[str, List]:
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
        import time
        start_time = time.time()
        logger.info(f"ğŸš€ ë„êµ¬ ì±„íŒ… ì‹œì‘: {user_input[:50]}...")
        
        try:
            # í† í° ì œí•œ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ
            if "context_length_exceeded" in str(getattr(self, "_last_error", "")):
                logger.warning("í† í° ì œí•œ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []

            # Gemini ëª¨ë¸ì€ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ë°©ì‹ ì‚¬ìš©
            model_name = getattr(llm, 'model_name', str(llm))
            if 'gemini' in model_name.lower():
                logger.info("ğŸ”§ Gemini ë„êµ¬ ì±„íŒ… ì‹œì‘")
                gemini_start = time.time()
                result = self._gemini_tool_chat(user_input, llm)
                logger.info(f"ğŸ”§ Gemini ë„êµ¬ ì±„íŒ… ì™„ë£Œ: {time.time() - gemini_start:.2f}ì´ˆ")
                return result

            # GPT ëª¨ë¸ì€ ê¸°ì¡´ ì—ì´ì „íŠ¸ ë°©ì‹ ì‚¬ìš©
            if not self.agent_executor:
                logger.info("ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì‹œì‘")
                agent_create_start = time.time()
                self.agent_executor = self.agent_executor_factory.create_agent_executor(llm, self.tools)
                logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì™„ë£Œ: {time.time() - agent_create_start:.2f}ì´ˆ")

            if not self.agent_executor:
                return "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.", []

            logger.info("ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘")
            agent_invoke_start = time.time()
            result = self.agent_executor.invoke({"input": user_input})
            logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {time.time() - agent_invoke_start:.2f}ì´ˆ")
            output = result.get("output", "")

            # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
            used_tools = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    if len(step) >= 2 and hasattr(step[0], "tool"):
                        used_tools.append(step[0].tool)

            if "Agent stopped" in output or not output.strip():
                logger.warning("ì—ì´ì „íŠ¸ ì¤‘ë‹¨ë¡œ ì¸í•´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []

            elapsed = time.time() - start_time
            logger.info(f"âœ… ë„êµ¬ ì±„íŒ… ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            # ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©
            limited_output = self._limit_response_length(output)
            
            return limited_output, used_tools

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            self._last_error = error_msg
            logger.error(f"âŒ ë„êµ¬ ì‚¬ìš© ì±„íŒ… ì˜¤ë¥˜: {elapsed:.2f}ì´ˆ, ì˜¤ë¥˜: {e}")

            # í† í° ì œí•œ ì˜¤ë¥˜ ì²˜ë¦¬
            if (
                "context_length_exceeded" in error_msg
                or "maximum context length" in error_msg
            ):
                logger.warning("í† í° ì œí•œ ì˜¤ë¥˜ ë°œìƒ, ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []

            simple_processor = SimpleChatProcessor()
            return simple_processor.process_chat(user_input, llm), []
    
    def _gemini_tool_chat(self, user_input: str, llm: Any) -> Tuple[str, List]:
        """Gemini ëª¨ë¸ìš© ë„êµ¬ ì±„íŒ… (ê°„ë‹¨í•œ êµ¬í˜„)"""
        # ì‹¤ì œ êµ¬í˜„ì€ ê¸°ì¡´ ë¡œì§ì„ ì°¸ì¡°í•˜ì—¬ ì‘ì„±
        return f"Gemini ë„êµ¬ ì±„íŒ… ê²°ê³¼: {user_input}", []
    
    def _limit_response_length(self, response: str) -> str:
        """ì‘ë‹µ ê¸¸ì´ ì œí•œ"""
        try:
            config = load_config()
            response_settings = config.get("response_settings", {})
            
            if not response_settings.get("enable_length_limit", True):
                return response
            
            max_length = response_settings.get("max_response_length", 8000)
            
            if len(response) <= max_length:
                return response
            
            logger.warning(f"ë„êµ¬ ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©: {len(response)}ì -> {max_length}ì")
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ìë¥´ê¸°
            truncated = response[:max_length]
            last_period = truncated.rfind('.')
            last_newline = truncated.rfind('\n')
            
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°
            cut_point = max(last_period, last_newline)
            if cut_point > max_length * 0.8:  # 80% ì´ìƒì—ì„œ ì°¾ì€ ê²½ìš°ë§Œ ì‚¬ìš©
                truncated = response[:cut_point + 1]
            
            return truncated + "\n\n[ë„êµ¬ ì‚¬ìš© ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.]"
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ì‘ë‹µ ê¸¸ì´ ì œí•œ ì˜¤ë¥˜: {e}")
            return response