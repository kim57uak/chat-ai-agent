from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple, Optional
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from core.enhanced_system_prompts import SystemPrompts
from core.file_utils import load_config
import logging
import re
import base64

logger = logging.getLogger(__name__)


class ChatProcessor(ABC):
    """ì±„íŒ… ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> str:
        pass


class SimpleChatProcessor(ChatProcessor):
    """ë‹¨ìˆœ ì±„íŒ… ì²˜ë¦¬ê¸°"""
    
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> str:
        """ì¼ë°˜ ì±„íŒ… (ë„êµ¬ ì‚¬ìš© ì—†ìŒ)"""
        try:
            # AI-driven system prompt selection based on content type
            if self._contains_image_data(user_input):
                system_content = SystemPrompts.get_image_analysis_prompt()
            else:
                system_content = SystemPrompts.get_general_chat_prompt()

            # Gemini ëª¨ë¸ì˜ ê²½ìš° ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¸ê°„ ë©”ì‹œì§€ë¡œ ë³€í™˜
            model_name = getattr(llm, 'model_name', str(llm))
            if 'gemini' in model_name.lower():
                messages = [HumanMessage(content=system_content)]
            else:
                messages = [SystemMessage(content=system_content)]

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìˆœì„œ ì¤‘ìš”)
            if conversation_history:
                history_messages = self._convert_history_to_messages(conversation_history, model_name)
                messages.extend(history_messages)
                logger.info(f"ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš©: {len(history_messages)}ê°œ ë©”ì‹œì§€")
            else:
                logger.info("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—†ìŒ")

            # ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬
            if self._contains_image_data(user_input):
                processed_input = self._process_image_input(user_input, model_name)
                messages.append(processed_input)
            else:
                messages.append(HumanMessage(content=user_input))

            logger.info(f"ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")
            response = llm.invoke(messages)
            response_content = response.content
            
            return response_content

        except Exception as e:
            logger.error(f"ì¼ë°˜ ì±„íŒ… ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def _convert_history_to_messages(self, conversation_history: List[Dict], model_name: str):
        """ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        messages = []
        
        # ëŒ€í™” ê¸°ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if not conversation_history:
            return messages
        
        # ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì‚¬ìš© (ë” ë§ì´ í¬í•¨)
        recent_history = (
            conversation_history[-10:]
            if len(conversation_history) > 10
            else conversation_history
        )

        for msg in recent_history:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            # ë‚´ìš©ì´ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if not content or not content.strip():
                continue
                
            # ë‚´ìš© ì œí•œì„ ë” ëŠ˜ë¦¬ê³  ì¤‘ìš”í•œ ë‚´ìš©ì€ ìœ ì§€
            if len(content) > 1000:
                content = content[:1000] + "..."
            
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))

        logger.info(f"ëŒ€í™” ê¸°ë¡ ë³€í™˜ ì™„ë£Œ: {len(recent_history)}ê°œ -> {len(messages)}ê°œ ë©”ì‹œì§€")
        return messages
    
    def _contains_image_data(self, user_input: str) -> bool:
        """Check if input contains image data"""
        return "[IMAGE_BASE64]" in user_input and "[/IMAGE_BASE64]" in user_input
    
    def _process_image_input(self, user_input: str, model_name: str):
        """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        image_match = re.search(
            r"\[IMAGE_BASE64\](.*?)\[/IMAGE_BASE64\]", user_input, re.DOTALL
        )
        if not image_match:
            return HumanMessage(content=user_input)

        image_data = image_match.group(1).strip()
        text_content = user_input.replace(image_match.group(0), "").strip()

        # Base64 ë°ì´í„° ê²€ì¦
        try:
            base64.b64decode(image_data)
        except Exception as e:
            logger.error(f"ì˜ëª»ëœ Base64 ì´ë¯¸ì§€ ë°ì´í„°: {e}")
            return HumanMessage(content="ì˜ëª»ëœ ì´ë¯¸ì§€ ë°ì´í„°ì…ë‹ˆë‹¤.")

        # AI-driven image analysis prompt
        if not text_content:
            text_content = """Analyze this image comprehensively and extract all information.

**Analysis Tasks:**
1. **Complete Text Extraction**: Extract all visible text with perfect accuracy
2. **Content Understanding**: Identify the type and purpose of the document/image
3. **Structure Analysis**: Describe layout, organization, and visual hierarchy
4. **Context Interpretation**: Explain what the image represents and its significance

**Response Requirements:**
- Extract ALL text without any omissions
- Organize information logically and clearly
- Use appropriate formatting (tables, lists, headings) based on content
- Provide context and interpretation where helpful
- Respond in Korean unless the content suggests otherwise

**Quality Standards:**
- Accuracy: 100% faithful text extraction
- Completeness: Cover all visible information
- Clarity: Well-organized, easy to understand presentation
- Intelligence: Apply appropriate formatting based on content type"""

        try:
            # Gemini ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ í˜•ì‹ ì‚¬ìš©
            if 'gemini' in model_name.lower():
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": f"data:image/jpeg;base64,{image_data}",
                        },
                    ]
                )
            else:
                # OpenAI GPT-4V í˜•ì‹
                return HumanMessage(
                    content=[
                        {"type": "text", "text": text_content},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            },
                        },
                    ]
                )
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return HumanMessage(
                content=f"{text_content}\n\n[ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}]"
            )
    
    def _limit_response_length(self, response: str) -> str:
        """ì‘ë‹µ ê¸¸ì´ ì œí•œ"""
        try:
            config = load_config()
            response_settings = config.get("response_settings", {})
            
            if not response_settings.get("enable_length_limit", True):
                return response
            
            max_length = response_settings.get("max_response_length", 8000)
            
            if len(response) <= max_length:
                return response
            
            logger.warning(f"ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©: {len(response)}ì -> {max_length}ì")
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ìë¥´ê¸°
            truncated = response[:max_length]
            last_period = truncated.rfind('.')
            last_newline = truncated.rfind('\n')
            
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°
            cut_point = max(last_period, last_newline)
            if cut_point > max_length * 0.8:  # 80% ì´ìƒì—ì„œ ì°¾ì€ ê²½ìš°ë§Œ ì‚¬ìš©
                truncated = response[:cut_point + 1]
            
            return truncated + "\n\n[ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.]"
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ê¸¸ì´ ì œí•œ ì˜¤ë¥˜: {e}")
            return response


class ToolChatProcessor(ChatProcessor):
    """ë„êµ¬ ì‚¬ìš© ì±„íŒ… ì²˜ë¦¬ê¸°"""
    
    def __init__(self, tools: List[Any], agent_executor_factory):
        self.tools = tools
        self.agent_executor_factory = agent_executor_factory
        self.agent_executor = None
    
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> Tuple[str, List]:
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
        import time
        start_time = time.time()
        logger.info(f"ğŸš€ ë„êµ¬ ì±„íŒ… ì‹œì‘: {user_input[:50]}...")
        
        try:
            # í† í° ì œí•œ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ
            if "context_length_exceeded" in str(getattr(self, "_last_error", "")):
                logger.warning("í† í° ì œí•œ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []

            # Gemini ëª¨ë¸ì€ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ë°©ì‹ ì‚¬ìš©
            model_name = getattr(llm, 'model_name', str(llm))
            if 'gemini' in model_name.lower():
                logger.info("ğŸ”§ Gemini ë„êµ¬ ì±„íŒ… ì‹œì‘")
                gemini_start = time.time()
                result = self._gemini_tool_chat(user_input, llm)
                logger.info(f"ğŸ”§ Gemini ë„êµ¬ ì±„íŒ… ì™„ë£Œ: {time.time() - gemini_start:.2f}ì´ˆ")
                return result

            # GPT ëª¨ë¸ì€ ê¸°ì¡´ ì—ì´ì „íŠ¸ ë°©ì‹ ì‚¬ìš©
            if not self.agent_executor:
                logger.info("ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì‹œì‘")
                agent_create_start = time.time()
                self.agent_executor = self.agent_executor_factory.create_agent_executor(llm, self.tools)
                logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì™„ë£Œ: {time.time() - agent_create_start:.2f}ì´ˆ")

            if not self.agent_executor:
                return "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.", []

            logger.info("ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘")
            agent_invoke_start = time.time()
            result = self.agent_executor.invoke({"input": user_input})
            logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {time.time() - agent_invoke_start:.2f}ì´ˆ")
            output = result.get("output", "")

            # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
            used_tools = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    if len(step) >= 2 and hasattr(step[0], "tool"):
                        used_tools.append(step[0].tool)

            if "Agent stopped" in output or not output.strip():
                logger.warning("ì—ì´ì „íŠ¸ ì¤‘ë‹¨ë¡œ ì¸í•´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []

            elapsed = time.time() - start_time
            logger.info(f"âœ… ë„êµ¬ ì±„íŒ… ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            return output, used_tools

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            self._last_error = error_msg
            logger.error(f"âŒ ë„êµ¬ ì‚¬ìš© ì±„íŒ… ì˜¤ë¥˜: {elapsed:.2f}ì´ˆ, ì˜¤ë¥˜: {e}")

            # í† í° ì œí•œ ì˜¤ë¥˜ ì²˜ë¦¬
            if (
                "context_length_exceeded" in error_msg
                or "maximum context length" in error_msg
            ):
                logger.warning("í† í° ì œí•œ ì˜¤ë¥˜ ë°œìƒ, ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []

            simple_processor = SimpleChatProcessor()
            return simple_processor.process_chat(user_input, llm), []
    
    def _gemini_tool_chat(self, user_input: str, llm: Any) -> Tuple[str, List]:
        """Gemini ëª¨ë¸ìš© ë„êµ¬ ì±„íŒ… - ì‹¤ì œ ë„êµ¬ ì‚¬ìš©"""
        try:
            # ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not self.agent_executor:
                logger.info("ğŸ”§ Geminiìš© ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„±")
                self.agent_executor = self.agent_executor_factory.create_agent_executor(llm, self.tools)
            
            if not self.agent_executor:
                logger.warning("ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì‹¤íŒ¨")
                return "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.", []
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            logger.info(f"ğŸ”§ Gemini ì—ì´ì „íŠ¸ ì‹¤í–‰: {user_input[:50]}...")
            result = self.agent_executor.invoke({"input": user_input})
            output = result.get("output", "")
            
            # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
            used_tools = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    if len(step) >= 2 and hasattr(step[0], "tool"):
                        used_tools.append(step[0].tool)
            
            # ë„êµ¬ê°€ ì‚¬ìš©ëœ ê²½ìš° ê²°ê³¼ ì²˜ë¦¬
            if len(used_tools) > 0:
                # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ìˆ˜ì§‘
                tool_results = []
                if "intermediate_steps" in result:
                    for step in result["intermediate_steps"]:
                        if len(step) >= 2:
                            tool_results.append(step[1])
                
                # ì—ì´ì „íŠ¸ ì‘ë‹µì´ ë¶€ì ì ˆí•œ ê²½ìš° ë„êµ¬ ê²°ê³¼ë¥¼ ì§ì ‘ ì •ë¦¬
                if not output.strip() or "Agent stopped" in output or "ë„êµ¬ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤" in output:
                    logger.info(f"Gemini ì—ì´ì „íŠ¸ ì‘ë‹µ ë¶€ì ì ˆ, ë„êµ¬ ê²°ê³¼ ì§ì ‘ ì •ë¦¬: {len(used_tools)}ê°œ ë„êµ¬")
                    
                    # ë„êµ¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±
                    if tool_results:
                        self.llm = llm  # AI í¬ë§·íŒ…ì„ ìœ„í•´ LLM ì €ì¥
                        formatted_results = self._format_tool_results(used_tools, tool_results, user_input)
                        if formatted_results:
                            return formatted_results, used_tools
                    
                    # ë„êµ¬ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
                    tool_names = [getattr(tool, '__name__', str(tool)) for tool in used_tools]
                    output = f"ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tool_names)}"
            # ë„êµ¬ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ê³  ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì—ì´ì „íŠ¸ê°€ ì¤‘ë‹¨ëœ ê²½ìš°
            elif not output.strip() or "Agent stopped" in output:
                logger.warning("Gemini ì—ì´ì „íŠ¸ ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì¤‘ë‹¨ë¨, ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []
            
            logger.info(f"âœ… Gemini ë„êµ¬ ì±„íŒ… ì„±ê³µ: {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©")
            return output, used_tools
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Gemini ë„êµ¬ ì±„íŒ… ì˜¤ë¥˜: {e}")
            
            # ReAct í˜•ì‹ ì˜¤ë¥˜ ì²˜ë¦¬
            if "Invalid Format" in error_msg and len(self.tools) > 0:
                # í˜•ì‹ ì˜¤ë¥˜ì§€ë§Œ ë„êµ¬ê°€ ìˆëŠ” ê²½ìš°, ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                logger.info("ReAct í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ, ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„")
                try:
                    # ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    simple_prompt = f"""Please use the following tools to answer the user's question:

{[str(tool) for tool in self.tools]}

Question: {user_input}

Please respond in the following format:
1. Select the appropriate tool to use
2. Explain the parameters to pass to the tool
3. Interpret the results and provide a final answer in Korean"""
                    messages = [HumanMessage(content=simple_prompt)]
                    response = llm.invoke(messages)
                    return response.content, []
                except Exception as inner_e:
                    logger.error(f"âŒ ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ ì‹¤íŒ¨: {inner_e}")
            
            # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ í´ë°±
            simple_processor = SimpleChatProcessor()
            return simple_processor.process_chat(user_input, llm), []
    
    def _format_tool_results(self, used_tools: List, tool_results: List, user_input: str) -> str:
        """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ AIê°€ ì§€ëŠ¥ì ìœ¼ë¡œ í¬ë§·íŒ…"""
        try:
            if not tool_results:
                return None
            
            # AIì—ê²Œ ê²°ê³¼ í¬ë§·íŒ… ìš”ì²­
            tool_names = [getattr(tool, '__name__', str(tool)) for tool in used_tools]
            results_text = "\n\n".join([f"Tool {i+1} Result: {str(result)}" for i, result in enumerate(tool_results)])
            
            format_prompt = f"""Please format the following tool execution results in a user-friendly way in Korean:

User Question: {user_input}
Used Tools: {', '.join(tool_names)}
Tool Results:
{results_text}

Please:
1. Analyze the results and provide a clear summary
2. Use appropriate emojis and formatting
3. Structure the information logically
4. Keep it concise but informative
5. Respond in Korean

Format the response as if you're directly answering the user's question based on these tool results."""
            
            # SimpleChatProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ AIê°€ ê²°ê³¼ í¬ë§·íŒ…
            from core.llm_factory import LLMFactoryProvider
            formatting_llm = LLMFactoryProvider.create_llm("dummy", "gpt-3.5-turbo")
            
            if hasattr(self, 'llm') and self.llm:
                formatting_llm = self.llm
            
            messages = [HumanMessage(content=format_prompt)]
            response = formatting_llm.invoke(messages)
            
            return response.content
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ í¬ë§·íŒ…
            tool_names = [getattr(tool, '__name__', str(tool)) for tool in used_tools]
            return f"âœ… ì‘ì—… ì™„ë£Œ (ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tool_names)})\n\n" + "\n\n".join([str(result) for result in tool_results])
    
    def _limit_response_length(self, response: str) -> str:
        """ì‘ë‹µ ê¸¸ì´ ì œí•œ"""
        try:
            config = load_config()
            response_settings = config.get("response_settings", {})
            
            if not response_settings.get("enable_length_limit", True):
                return response
            
            max_length = response_settings.get("max_response_length", 8000)
            
            if len(response) <= max_length:
                return response
            
            logger.warning(f"ë„êµ¬ ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©: {len(response)}ì -> {max_length}ì")
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ìë¥´ê¸°
            truncated = response[:max_length]
            last_period = truncated.rfind('.')
            last_newline = truncated.rfind('\n')
            
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°
            cut_point = max(last_period, last_newline)
            if cut_point > max_length * 0.8:  # 80% ì´ìƒì—ì„œ ì°¾ì€ ê²½ìš°ë§Œ ì‚¬ìš©
                truncated = response[:cut_point + 1]
            
            return truncated + "\n\n[ë„êµ¬ ì‚¬ìš© ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.]"
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ì‘ë‹µ ê¸¸ì´ ì œí•œ ì˜¤ë¥˜: {e}")
            return response