# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ ì„í¬íŠ¸
from abc import ABC, abstractmethod
from core.chat_processor_refactored import (
    ChatProcessor as BaseChatProcessor, 
    RefactoredSimpleChatProcessor, 
    RefactoredToolChatProcessor
)
from langchain.schema import HumanMessage
from core.file_utils import load_config
from typing import List, Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)


class ChatProcessor(BaseChatProcessor):
    """ì±„íŒ… ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤ - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€"""
    pass


class SimpleChatProcessor(RefactoredSimpleChatProcessor):
    """ë‹¨ìˆœ ì±„íŒ… ì²˜ë¦¬ê¸° - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€"""
    pass
    
    pass


class ToolChatProcessor(RefactoredToolChatProcessor):
    """ë„êµ¬ ì‚¬ìš© ì±„íŒ… ì²˜ë¦¬ê¸° - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€"""
    
    def __init__(self, tools: List[Any], agent_executor_factory):
        super().__init__(tools, agent_executor_factory)
        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„± ìœ ì§€
        self.tools = tools
        self.agent_executor_factory = agent_executor_factory
        self.agent_executor = None
    
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€
    def process_chat(self, user_input: str, llm: Any, conversation_history: List[Dict] = None) -> Tuple[str, List]:
        return super().process_chat(user_input, llm, conversation_history)
    
    def _gemini_tool_chat(self, user_input: str, llm: Any) -> Tuple[str, List]:
        """Gemini ëª¨ë¸ìš© ë„êµ¬ ì±„íŒ… - ì‹¤ì œ ë„êµ¬ ì‚¬ìš©"""
        try:
            # ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not self.agent_executor:
                logger.info("ğŸ”§ Geminiìš© ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„±")
                self.agent_executor = self.agent_executor_factory.create_agent_executor(llm, self.tools)
            
            if not self.agent_executor:
                logger.warning("ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± ì‹¤íŒ¨")
                return "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.", []
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            logger.info(f"ğŸ”§ Gemini ì—ì´ì „íŠ¸ ì‹¤í–‰: {user_input[:50]}...")
            result = self.agent_executor.invoke({"input": user_input})
            output = result.get("output", "")
            
            # ì‚¬ìš©ëœ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
            used_tools = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    if len(step) >= 2 and hasattr(step[0], "tool"):
                        used_tools.append(step[0].tool)
            
            # ë„êµ¬ê°€ ì‚¬ìš©ëœ ê²½ìš° ê²°ê³¼ ì²˜ë¦¬
            if len(used_tools) > 0:
                # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ìˆ˜ì§‘
                tool_results = []
                if "intermediate_steps" in result:
                    for step in result["intermediate_steps"]:
                        if len(step) >= 2:
                            tool_results.append(step[1])
                
                # ì—ì´ì „íŠ¸ ì‘ë‹µì´ ë¶€ì ì ˆí•œ ê²½ìš° ë„êµ¬ ê²°ê³¼ë¥¼ ì§ì ‘ ì •ë¦¬
                if not output.strip() or "Agent stopped" in output or "ë„êµ¬ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤" in output:
                    logger.info(f"Gemini ì—ì´ì „íŠ¸ ì‘ë‹µ ë¶€ì ì ˆ, ë„êµ¬ ê²°ê³¼ ì§ì ‘ ì •ë¦¬: {len(used_tools)}ê°œ ë„êµ¬")
                    
                    # ë„êµ¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±
                    if tool_results:
                        self.llm = llm  # AI í¬ë§·íŒ…ì„ ìœ„í•´ LLM ì €ì¥
                        formatted_results = self._format_tool_results(used_tools, tool_results, user_input)
                        if formatted_results:
                            return formatted_results, used_tools
                    
                    # ë„êµ¬ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
                    tool_names = [getattr(tool, '__name__', str(tool)) for tool in used_tools]
                    output = f"ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tool_names)}"
            # ë„êµ¬ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ê³  ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì—ì´ì „íŠ¸ê°€ ì¤‘ë‹¨ëœ ê²½ìš°
            elif not output.strip() or "Agent stopped" in output:
                logger.warning("Gemini ì—ì´ì „íŠ¸ ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì¤‘ë‹¨ë¨, ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ëŒ€ì²´")
                simple_processor = SimpleChatProcessor()
                return simple_processor.process_chat(user_input, llm), []
            
            logger.info(f"âœ… Gemini ë„êµ¬ ì±„íŒ… ì„±ê³µ: {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©")
            return output, used_tools
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Gemini ë„êµ¬ ì±„íŒ… ì˜¤ë¥˜: {e}")
            
            # ReAct í˜•ì‹ ì˜¤ë¥˜ ì²˜ë¦¬
            if "Invalid Format" in error_msg and len(self.tools) > 0:
                # í˜•ì‹ ì˜¤ë¥˜ì§€ë§Œ ë„êµ¬ê°€ ìˆëŠ” ê²½ìš°, ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                logger.info("ReAct í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ, ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„")
                try:
                    # ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    simple_prompt = f"""Please use the following tools to answer the user's question:

{[str(tool) for tool in self.tools]}

Question: {user_input}

Please respond in the following format:
1. Select the appropriate tool to use
2. Explain the parameters to pass to the tool
3. Interpret the results and provide a final answer in Korean"""
                    messages = [HumanMessage(content=simple_prompt)]
                    response = llm.invoke(messages)
                    return response.content, []
                except Exception as inner_e:
                    logger.error(f"âŒ ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ ì‹¤íŒ¨: {inner_e}")
            
            # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ í´ë°±
            simple_processor = SimpleChatProcessor()
            return simple_processor.process_chat(user_input, llm), []
    
    def _format_tool_results(self, used_tools: List, tool_results: List, user_input: str) -> str:
        """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ AIê°€ ì§€ëŠ¥ì ìœ¼ë¡œ í¬ë§·íŒ…"""
        try:
            if not tool_results:
                return None
            
            # AIì—ê²Œ ê²°ê³¼ í¬ë§·íŒ… ìš”ì²­
            tool_names = [getattr(tool, '__name__', str(tool)) for tool in used_tools]
            results_text = "\n\n".join([f"Tool {i+1} Result: {str(result)}" for i, result in enumerate(tool_results)])
            
            format_prompt = f"""Please format the following tool execution results in a user-friendly way in Korean:

User Question: {user_input}
Used Tools: {', '.join(tool_names)}
Tool Results:
{results_text}

Please:
1. Analyze the results and provide a clear summary
2. Use appropriate emojis and formatting
3. Structure the information logically
4. Keep it concise but informative
5. Respond in Korean

Format the response as if you're directly answering the user's question based on these tool results."""
            
            # SimpleChatProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ AIê°€ ê²°ê³¼ í¬ë§·íŒ…
            from core.llm_factory import LLMFactoryProvider
            formatting_llm = LLMFactoryProvider.create_llm("dummy", "gpt-3.5-turbo")
            
            if hasattr(self, 'llm') and self.llm:
                formatting_llm = self.llm
            
            messages = [HumanMessage(content=format_prompt)]
            response = formatting_llm.invoke(messages)
            
            return response.content
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ í¬ë§·íŒ…
            tool_names = [getattr(tool, '__name__', str(tool)) for tool in used_tools]
            return f"âœ… ì‘ì—… ì™„ë£Œ (ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tool_names)})\n\n" + "\n\n".join([str(result) for result in tool_results])
    
    def _limit_response_length(self, response: str) -> str:
        """ì‘ë‹µ ê¸¸ì´ ì œí•œ"""
        try:
            config = load_config()
            response_settings = config.get("response_settings", {})
            
            if not response_settings.get("enable_length_limit", True):
                return response
            
            max_length = response_settings.get("max_response_length", 8000)
            
            if len(response) <= max_length:
                return response
            
            logger.warning(f"ë„êµ¬ ì‘ë‹µ ê¸¸ì´ ì œí•œ ì ìš©: {len(response)}ì -> {max_length}ì")
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ìë¥´ê¸°
            truncated = response[:max_length]
            last_period = truncated.rfind('.')
            last_newline = truncated.rfind('\n')
            
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°
            cut_point = max(last_period, last_newline)
            if cut_point > max_length * 0.8:  # 80% ì´ìƒì—ì„œ ì°¾ì€ ê²½ìš°ë§Œ ì‚¬ìš©
                truncated = response[:cut_point + 1]
            
            return truncated + "\n\n[ë„êµ¬ ì‚¬ìš© ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.]"
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ì‘ë‹µ ê¸¸ì´ ì œí•œ ì˜¤ë¥˜: {e}")
            return response