from typing import List, Dict, Any, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import BaseMessage, HumanMessage, SystemMessage, AIMessage
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from .base_model_strategy import BaseModelStrategy
from ui.prompts import prompt_manager, ModelType
from core.token_logger import TokenLogger
from core.token_tracker import token_tracker, StepType
from core.parsers.custom_react_parser import CustomReActParser
import logging

logger = logging.getLogger(__name__)


class GeminiStrategy(BaseModelStrategy):
    """Google Gemini ëª¨ë¸ ì „ëžµ"""
    
    def create_llm(self):
        """Gemini LLM ìƒì„±"""
        # Pro ëª¨ë¸ì˜ ê²½ìš° ë” ê¸´ íƒ€ìž„ì•„ì›ƒê³¼ ë” ë§Žì€ ìž¬ì‹œë„
        if "pro" in self.model_name.lower():
            max_tokens = 32768
            max_retries = 5
            request_timeout = 60
        else:
            max_tokens = 16384
            max_retries = 3
            request_timeout = 30
        
        llm = ChatGoogleGenerativeAI(
            model=self.model_name,
            google_api_key=self.api_key,
            temperature=0.1,
            convert_system_message_to_human=True,
            max_tokens=max_tokens,
            max_retries=max_retries,
            request_timeout=request_timeout,
        )
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì½œë°± ì„¤ì •
        self._setup_token_tracking_callbacks(llm)
        return llm
    
    def _setup_token_tracking_callbacks(self, llm):
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì½œë°± ì„¤ì • - ë‹¨ìˆœí™”"""
        # Pydantic ëª¨ë¸ì˜ ì œì•½ì‚¬í•­ìœ¼ë¡œ ì¸í•´ ë‹¨ìˆœížˆ ì „ëžµ ê°ì²´ì— ì°¸ì¡°ë§Œ ì €ìž¥
        self._tracked_llm = llm
        pass
    
    def create_messages(self, user_input: str, system_prompt: str = None, conversation_history: List[Dict] = None) -> List[BaseMessage]:
        """Gemini ë©”ì‹œì§€ í˜•ì‹ ìƒì„± - ëŒ€í™” ížˆìŠ¤í† ë¦¬ í¬í•¨"""
        messages = []
        
        # ì‚¬ìš©ìž ìž…ë ¥ì—ì„œ ì–¸ì–´ ê°ì§€ (ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©)
        user_language = self.detect_user_language(user_input)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        if system_prompt:
            enhanced_prompt = self.enhance_prompt_with_format(system_prompt)
        else:
            enhanced_prompt = self.get_default_system_prompt()
        
        # ì–¸ì–´ë³„ ì‘ë‹µ ì§€ì¹¨ ì¶”ê°€
        if user_language == "ko":
            enhanced_prompt += "\n\n**ì¤‘ìš”**: ì‚¬ìš©ìžê°€ í•œêµ­ì–´ë¡œ ì§ˆë¬¸í–ˆìœ¼ë¯€ë¡œ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
        else:
            enhanced_prompt += "\n\n**Important**: The user asked in English, so please respond in English."
        
        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if conversation_history:
            history_context = self._format_conversation_history(conversation_history)
            if user_language == "ko":
                enhanced_prompt += (
                    f"\n\n## ðŸ’¬ ì´ì „ ëŒ€í™”:\n"
                    f"{history_context}\n\n"
                    f"â„¹ï¸ ì´ ëŒ€í™” ë‚´ì—­ì„ ê³ ë ¤í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”."
                )
            else:
                enhanced_prompt += (
                    f"\n\n## ðŸ’¬ Previous Conversation:\n"
                    f"{history_context}\n\n"
                    f"â„¹ï¸ Please consider this conversation history when responding."
                )
        
        messages.append(HumanMessage(content=enhanced_prompt))
        
        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ ì‹¤ì œ ë©”ì‹œì§€ë¡œ ë³€í™˜
        if conversation_history:
            for msg in conversation_history:
                role = msg.get("role", "")
                content = msg.get("content", "")
                
                if role == "user" and isinstance(content, str) and content.strip():
                    messages.append(HumanMessage(content=content))
                elif role in ["assistant", "agent"] and isinstance(content, str) and content.strip():
                    messages.append(AIMessage(content=content))
        
        # í˜„ìž¬ ì‚¬ìš©ìž ìž…ë ¥ ì¶”ê°€
        messages.append(HumanMessage(content=user_input))
        return messages
    
    def _format_conversation_history(self, conversation_history: List[Dict]) -> str:
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ ê°€ë…ì„± ì¢‹ê²Œ í¬ë§·íŒ…"""
        formatted_history = []
        for i, msg in enumerate(conversation_history, 1):
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                formatted_history.append(f"**[{i}] ðŸ‘¤ User:** {content}")
            elif role in ["assistant", "agent"]:
                formatted_history.append(f"**[{i}] ðŸ¤– Assistant:** {content}")
        
        return "\n\n".join(formatted_history)
    
    def process_image_input(self, user_input: str) -> BaseMessage:
        """Gemini ì´ë¯¸ì§€ ìž…ë ¥ ì²˜ë¦¬"""
        import re
        import base64
        
        image_match = re.search(
            r"\[IMAGE_BASE64\](.*?)\[/IMAGE_BASE64\]", user_input, re.DOTALL
        )
        if not image_match:
            return HumanMessage(content=user_input)
        
        image_data = image_match.group(1).strip()
        text_content = user_input.replace(image_match.group(0), "").strip()
        
        try:
            base64.b64decode(image_data)
        except Exception as e:
            logger.error(f"Invalid Base64 image data: {e}")
            return HumanMessage(content="Invalid image data.")
        
        if not text_content:
            text_content = self._get_ocr_prompt()
        
        # Gemini 2.0 Flashì— ìµœì í™”ëœ í˜•ì‹
        return HumanMessage(
            content=[
                {"type": "text", "text": text_content},
                {
                    "type": "image_url",
                    "image_url": f"data:image/jpeg;base64,{image_data}",
                },
            ]
        )
    
    def should_use_tools(self, user_input: str, force_agent: bool = False) -> bool:
        """Geminiê°€ ìžì—°ì–´ë¥¼ ì´í•´í•˜ì—¬ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°ì •"""
        import time
        
        start_time = time.time()
        logger.info(f"ðŸ¤” Gemini ë„êµ¬ ì‚¬ìš© íŒë‹¨ ì‹œìž‘: {user_input[:30]}...")
        
        # Agent ëª¨ë“œê°€ ê°•ì œëœ ê²½ìš° í•­ìƒ ë„êµ¬ ì‚¬ìš©
        if force_agent:
            logger.info("ðŸ”§ Agent ëª¨ë“œê°€ ê°•ì œë˜ì–´ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
            return True
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì •ë³´ ìˆ˜ì§‘
            available_tools = []
            if hasattr(self, 'tools') and self.tools:
                for tool in self.tools[:5]:  # ì£¼ìš” ë„êµ¬ 5ê°œë§Œ
                    tool_desc = getattr(tool, 'description', tool.name)
                    available_tools.append(f"- {tool.name}: {tool_desc[:80]}")
            
            tools_info = "\n".join(available_tools) if available_tools else "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì—†ìŒ"
            
            # Agent ëª¨ë“œ ì„ íƒ ì‹œ ë” ì ê·¹ì ì¸ íŒë‹¨ ê¸°ì¤€ ì ìš©
            agent_context = ""
            if force_agent:
                agent_context = "\n\nIMPORTANT: The user has specifically selected Agent mode, indicating they want to use available tools when possible. Be more inclined to use tools for information gathering, searches, or data processing tasks."
            
            # ì¤‘ì•™ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            available_tools = getattr(self, 'tools', [])
            decision_prompt = prompt_manager.get_tool_decision_prompt(
                ModelType.GOOGLE.value, user_input, available_tools
            )
            
            # Agent ëª¨ë“œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if force_agent:
                decision_prompt += (
                    "\n\n## ðŸ”§ Agent Mode Active\n"
                    "User selected Agent mode - be more inclined to use tools for:\n"
                    "- Information gathering\n"
                    "- Searches and data processing\n"
                    "- External API calls"
                )
            
            # Gemini ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                HumanMessage(content=(
                    "# Tool Decision Expert\n\n"
                    "You analyze user requests to determine if tools are needed.\n\n"
                    "â„¹ï¸ **Important:** When creating tables, use proper markdown format with pipe separators."
                )),
                HumanMessage(content=decision_prompt),
            ]
            
            # ë„êµ¬ ê²°ì • ë‹¨ê³„ ì‹œìž‘
            token_tracker.start_step(StepType.TOOL_DECISION, "Gemini Tool Decision")
            
            llm_start = time.time()
            response = self.llm.invoke(messages)
            llm_elapsed = time.time() - llm_start
            decision = response.content.strip().upper()
            
            # ì‘ë‹µ ê°ì²´ ì €ìž¥ (í† í° ì¶”ì¶œìš©)
            self._last_response = response
            
            # ë„êµ¬ ê²°ì • ë‹¨ê³„ ì¢…ë£Œ
            input_text = "\n".join([str(msg.content) for msg in messages])
            token_tracker.end_step(
                StepType.TOOL_DECISION,
                "Gemini Tool Decision",
                input_text=input_text,
                output_text=decision,
                response_obj=response,
                additional_info={"force_agent": force_agent}
            )
            
            # ê¸°ì¡´ ë¡œê¹…
            TokenLogger.log_messages_token_usage(
                self.model_name, messages, decision, "tool_decision"
            )
            
            result = "YES" in decision
            mode_info = " (Agent ëª¨ë“œ)" if force_agent else " (Ask ëª¨ë“œ)"
            total_elapsed = time.time() - start_time
            logger.info(
                f"ðŸ¤” Gemini ë„êµ¬ ì‚¬ìš© íŒë‹¨{mode_info}: {decision} -> {result} (LLM: {llm_elapsed:.2f}ì´ˆ, ì´: {total_elapsed:.2f}ì´ˆ)"
            )
            return result
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"âŒ Gemini ë„êµ¬ ì‚¬ìš© íŒë‹¨ ì˜¤ë¥˜: {elapsed:.2f}ì´ˆ, ì˜¤ë¥˜: {e}")
            return False
    
    def create_agent_executor(self, tools: List) -> Optional[AgentExecutor]:
        """Gemini ReAct ì—ì´ì „íŠ¸ ìƒì„± (ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ë¶„ê¸°)"""
        if not tools:
            return None
        
        # ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ë‚ ì§œ ì •ë³´ í¬í•¨)
        base_system_prompt = prompt_manager.get_system_prompt(ModelType.GOOGLE.value, use_tools=True)
        
        # ëª¨ë¸ë³„ ReAct í˜•ì‹ ê°€ì´ë“œ ì¶”ê°€
        if "pro" in self.model_name.lower():
            react_guide = (
                "\n\n## ðŸš¨ CRITICAL: Every response MUST start with a keyword\n\n"
                "### Required Keywords:\n"
                "- `Thought:` [your reasoning]\n"
                "- `Action:` [exact_tool_name]\n"
                "- `Action Input:` [json_parameters]\n"
                "- `Final Answer:` [complete response with tables/content]\n\n"
                "### âœ… Correct Examples:\n"
                "```\n"
                "Final Answer: Here are the search results.\n"
                "| Product | Price |\n"
                "```\n\n"
                "### ðŸ“ Response Length Control:\n"
                "- Keep Final Answer under 16384 tokens to prevent parsing errors\n"
                "- If data is large, provide summary with key statistics\n"
                "- Always prioritize essential information over details\n\n"
                "**Rule: Follow ReAct format - Thought â†’ Action â†’ Action Input â†’ Final Answer**"
            )
        else:
            react_guide = (
                "\n\n## Google Gemini ReAct Format\n\n"
                "### Step 1 - Tool Use:\n"
                "`Thought:` [your reasoning]\n"
                "`Action:` [exact_tool_name]\n"
                "`Action Input:` [json_params]\n\n"
                "### Step 2 - After Observation:\n"
                "`Thought:` [analyze the observation result]\n"
                "`Final Answer:` [response based on observation]\n\n"
                "### ðŸš¨ Critical Rules:\n"
                "- Never skip Observation\n"
                "- Never mix steps\n"
                "- Use exact tool names from schema\n"
                "- Wait for system Observation before Final Answer\n"
                "- Keep Final Answer under 16384 tokens (summarize if needed)"
            )
        
        agent_system_prompt = base_system_prompt + react_guide
        
        # GeminiëŠ” ReAct ì—ì´ì „íŠ¸ë§Œ ì§€ì›
        if "pro" in self.model_name.lower():
            # Pro ëª¨ë¸ìš© ê°•ë ¥í•œ íŒŒì‹± í…œí”Œë¦¿
            react_prompt = PromptTemplate.from_template(
                f"""# ðŸš¨ Parsing Rule: Start every response with a keyword

{agent_system_prompt}

## Available Tools:
{{tools}}

## Tool Names:
{{tool_names}}

## Process:
1. **First**: `Thought:` â†’ `Action:` â†’ `Action Input:`
2. **Wait for Observation**
3. **Then**: `Thought:` â†’ `Final Answer:`

---
**Question:** {{input}}
{{agent_scratchpad}}"""
            )
        else:
            # Flash ë° ê¸°íƒ€ ëª¨ë¸ìš© ê¸°ì¡´ í…œí”Œë¦¿ (ê°€ë…ì„± ê°œì„ )
            react_prompt = PromptTemplate.from_template(
                f"""# Google Gemini ReAct Agent

{agent_system_prompt}

## Available Tools:
{{tools}}

## Tool Names:
{{tool_names}}

## Example Process:
**Step 1:**
```
Thought: I need to use a tool
Action: [exact_tool_name_from_list]
Action Input: {{{{"param": "value"}}}}
```

**Step 2 (After Observation):**
```
Thought: [analyze the observation result]
Final Answer: [response based on observation]
```

âš ï¸ **Important:** Never output content directly! Always use Final Answer format!

---
**Question:** {{input}}
{{agent_scratchpad}}"""
            )

        # ì»¤ìŠ¤í…€ íŒŒì„œ ì‚¬ìš©
        custom_parser = CustomReActParser()
        agent = create_react_agent(self.llm, tools, react_prompt, output_parser=custom_parser)
        
        # Pro ëª¨ë¸ì˜ ê²½ìš° ë” ê¸´ ì‹œê°„ê³¼ ë” ë§Žì€ ë°˜ë³µ í—ˆìš©
        if "pro" in self.model_name.lower():
            max_iterations = 8
            max_execution_time = 60
        else:
            max_iterations = 5
            max_execution_time = 30
        
        return AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            max_iterations=max_iterations,
            max_execution_time=max_execution_time,
            handle_parsing_errors=True,  # ì»¤ìŠ¤í…€ íŒŒì„œê°€ ì²˜ë¦¬
            early_stopping_method="force",
            return_intermediate_steps=True,
        )
    
    def _get_ocr_prompt(self) -> str:
        """OCR ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return prompt_manager.get_ocr_prompt()