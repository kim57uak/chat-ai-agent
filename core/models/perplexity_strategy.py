from typing import List, Dict, Any, Optional
from langchain.schema import BaseMessage, HumanMessage, SystemMessage, AIMessage
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from .base_model_strategy import BaseModelStrategy
from core.perplexity_llm import PerplexityLLM
from core.perplexity_wrapper import PerplexityWrapper
from core.perplexity_output_parser import PerplexityOutputParser
from ui.prompts import prompt_manager, ModelType
from core.token_logger import TokenLogger
from core.logging import get_logger

logger = get_logger("perplexity_strategy")


class PerplexityStrategy(BaseModelStrategy):
    """Perplexity ëª¨ë¸ ì „ëµ - ë‹¨ìˆœí™”ëœ êµ¬í˜„"""
    
    def create_llm(self):
        """Perplexity LLM ìƒì„±"""
        params = self.get_model_parameters()
        wrapper = PerplexityWrapper(
            pplx_api_key=self.api_key,
            model=self.model_name
        )
        # íŒŒë¼ë¯¸í„°ë¥¼ wrapperì— ì €ì¥í•˜ì—¬ generate í˜¸ì¶œ ì‹œ ì‚¬ìš©
        wrapper._model_params = params
        return wrapper
    
    def create_messages(self, user_input: str, system_prompt: str = None, conversation_history: List[Dict] = None) -> List[BaseMessage]:
        """Perplexity ë©”ì‹œì§€ í˜•ì‹ ìƒì„± - ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨"""
        messages = []
        
        # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì–¸ì–´ ê°ì§€
        user_language = self.detect_user_language(user_input)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì–¸ì–´ ì§€ì¹¨ í¬í•¨)
        if system_prompt:
            enhanced_prompt = self.enhance_prompt_with_format(system_prompt)
        else:
            enhanced_prompt = self.get_perplexity_system_prompt(user_language)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ ê°•ì¡° ì¶”ê°€
        if conversation_history:
            history_context = self._format_conversation_history(conversation_history)
            enhanced_prompt += f"\n\n=== CONVERSATION HISTORY (HIGHEST PRIORITY) ===\n{history_context}\n\n**ABSOLUTE PRIORITY RULE**: \n1. ALWAYS check this conversation history FIRST before searching\n2. If the answer exists in this history, use it directly\n3. Only search for NEW information not in this conversation\n4. When referencing history, say \"Based on our conversation...\"\n5. Remember names, preferences, and context from above messages\n==========================================="
            
            logger.info(f"Perplexityì— ëŒ€í™” íˆìŠ¤í† ë¦¬ {len(conversation_history)}ê°œ ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€")
        
        # PerplexityëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬
        messages.append(SystemMessage(content=enhanced_prompt))
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì‹¤ì œ ë©”ì‹œì§€ë¡œ ë³€í™˜ - êµëŒ€ íŒ¨í„´ ë³´ì¥
        if conversation_history:
            last_role = None
            for i, msg in enumerate(conversation_history):
                role = msg.get("role", "")
                content = msg.get("content", "")
                
                if not content.strip():
                    continue
                
                # ì—°ì†ëœ ê°™ì€ role ë°©ì§€
                if role == "user" and last_role != "user":
                    messages.append(HumanMessage(content=content))
                    logger.debug(f"  íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ {i+1}: user - {content[:50]}...")
                    last_role = "user"
                elif role in ["assistant", "agent"] and last_role != "assistant":
                    messages.append(AIMessage(content=content))
                    logger.debug(f"  íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ {i+1}: assistant - {content[:50]}...")
                    last_role = "assistant"
                elif role == "user" and last_role == "user":
                    # ì—°ì†ëœ user ë©”ì‹œì§€ëŠ” ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ë³‘í•©
                    if messages and isinstance(messages[-1], HumanMessage):
                        messages[-1].content += f"\n\n{content}"
                        logger.debug(f"  ì—°ì† user ë©”ì‹œì§€ ë³‘í•©: {content[:30]}...")
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€ - ë§ˆì§€ë§‰ì´ userë©´ ë³‘í•©
        if messages and isinstance(messages[-1], HumanMessage):
            messages[-1].content += f"\n\n{user_input}"
            logger.info(f"  í˜„ì¬ ì…ë ¥ì„ ë§ˆì§€ë§‰ user ë©”ì‹œì§€ì— ë³‘í•©")
        else:
            messages.append(HumanMessage(content=user_input))
        
        logger.info(f"Perplexityì— ìµœì¢… ì „ë‹¬ë˜ëŠ” ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        return messages
    
    def _format_conversation_history(self, conversation_history: List[Dict]) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ… - ê°•í™”ëœ ë²„ì „"""
        if not conversation_history:
            return "No previous conversation."
        
        formatted_history = []
        formatted_history.append("ğŸ’¬ **CONVERSATION CONTEXT (Remember this!):**")
        formatted_history.append("=" * 50)
        
        for i, msg in enumerate(conversation_history, 1):
            role = msg.get("role", "")
            content = msg.get("content", "").strip()
            
            if not content:
                continue
                
            if role == "user":
                formatted_history.append(f"[{i}] ğŸ‘¤ **User said**: {content}")
            elif role in ["assistant", "agent"]:
                formatted_history.append(f"[{i}] ğŸ¤– **I replied**: {content}")
        
        formatted_history.append("=" * 50)
        formatted_history.append("ğŸ“ **IMPORTANT**: This conversation history contains personal context, names, preferences, and previous discussions. Use this information to provide contextual responses.")
        
        return "\n".join(formatted_history)
    
    def process_image_input(self, user_input: str) -> BaseMessage:
        """PerplexityëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ ë¯¸ì§€ì›"""
        # ì´ë¯¸ì§€ íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
        import re
        cleaned_input = re.sub(r'\[IMAGE_BASE64\].*?\[/IMAGE_BASE64\]', '', user_input, flags=re.DOTALL)
        return HumanMessage(content=cleaned_input.strip() or "ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def should_use_tools(self, user_input: str) -> bool:
        """ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì • - AI ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜"""
        try:
            available_tools = getattr(self, 'tools', [])
            if not available_tools:
                return False
            
            # ì¤‘ì•™ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            decision_prompt = prompt_manager.get_tool_decision_prompt(
                ModelType.PERPLEXITY.value, user_input, available_tools
            )
            
            response = self.llm._call(decision_prompt)
            decision = response.strip().upper()
            
            TokenLogger.log_token_usage(
                self.model_name, decision_prompt, decision, "tool_decision"
            )
            
            result = "YES" in decision
            logger.info(f"ğŸ¤” Perplexity ë„êµ¬ ì‚¬ìš© íŒë‹¨: '{user_input}' -> {decision} -> {result}")
            return result
            
        except Exception as e:
            logger.error(f"Perplexity ë„êµ¬ ì‚¬ìš© íŒë‹¨ ì˜¤ë¥˜: {e}")
            return True
    
    def create_agent_executor(self, tools: List) -> Optional[AgentExecutor]:
        """Perplexity ReAct ì—ì´ì „íŠ¸ ìƒì„± - ë‹¨ìˆœí™”ëœ êµ¬í˜„"""
        if not tools:
            return None
        
        # Common í”„ë¡¬í”„íŠ¸ì—ì„œ ë„êµ¬ ì‚¬ìš© ê·œì¹™ ê°€ì ¸ì˜¤ê¸°
        tool_usage_rules = prompt_manager.get_tool_prompt(ModelType.PERPLEXITY.value)
        execution_rules = prompt_manager.get_custom_prompt(ModelType.COMMON.value, "execution_rules")
        
        react_prompt = PromptTemplate.from_template(
            f"""You are an expert data analyst that uses tools to gather information and provides comprehensive analysis.

**CRITICAL: THOROUGH OBSERVATION ANALYSIS REQUIRED**

{tool_usage_rules}

{execution_rules}

**EXACT FORMAT (MANDATORY):**

Thought: [Analyze what information is needed for the user's question]
Action: [exact_tool_name]
Action Input: {{{{"exact_parameter_name_from_schema": "value"}}}}

(System provides Observation with tool results)

Thought: [CRITICAL ANALYSIS STEP: Read the ENTIRE Observation carefully. Extract ALL key information including names, numbers, dates, details. Identify patterns and relationships. Organize findings logically. This analysis determines the quality of your Final Answer.]
Final Answer: [Comprehensive Korean response based EXCLUSIVELY on Observation data. Include specific details, exact numbers, names, and all relevant information from the tool results. Structure the response clearly with proper formatting. Do NOT add external knowledge - use ONLY the data provided in the Observation.]

**ANALYSIS REQUIREMENTS:**
1. **COMPLETE DATA PROCESSING**: Read every part of the Observation
2. **EXTRACT SPECIFICS**: Include exact names, numbers, dates from results
3. **LOGICAL ORGANIZATION**: Structure information clearly
4. **COMPREHENSIVE COVERAGE**: Address all aspects relevant to user's question
5. **DATA-ONLY RESPONSES**: Base answer EXCLUSIVELY on Observation data

**PARSING RULES:**
- Use EXACT keywords: "Thought:", "Action:", "Action Input:", "Final Answer:"
- Action Input MUST be valid JSON with EXACT parameter names from inputSchema
- NEVER use generic names like "param", "value" - check the tool's inputSchema
- NEVER mix Action and Final Answer
- Analyze Observation thoroughly before Final Answer

Tools: {{tools}}
Tool names: {{tool_names}}

Question: {{input}}
Thought:{{agent_scratchpad}}"""
        )

        try:
            # ì»¤ìŠ¤í…€ íŒŒì„œ ì‚¬ìš©
            custom_parser = PerplexityOutputParser()
            
            # ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ì»¤ìŠ¤í…€ íŒŒì„œ ì‚¬ìš©
            agent = create_react_agent(self.llm, tools, react_prompt, output_parser=custom_parser)
            
            return AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=True,
                max_iterations=5,
                max_execution_time=60,
                handle_parsing_errors="CRITICAL: Follow the exact format. After receiving Observation, analyze ALL the data thoroughly in your Thought, then provide a comprehensive Final Answer based ONLY on the Observation data. Include specific details from the results.",
                early_stopping_method="force",
                return_intermediate_steps=True,
            )
        except Exception as e:
            logger.error(f"Perplexity agent creation failed: {e}")
            return None
    
    def get_perplexity_system_prompt(self, user_language: str = None) -> str:
        """Perplexity ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ë„êµ¬ ì¸ì‹ ê°•í™”"""
        base_prompt = prompt_manager.get_system_prompt(ModelType.PERPLEXITY.value, use_tools=True, user_language=user_language)
        
        # ë„êµ¬ê°€ ìˆì„ ë•Œ ë„êµ¬ ì¸ì‹ ê°•í™” í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        if hasattr(self, 'tools') and self.tools:
            tool_awareness = prompt_manager.get_custom_prompt(ModelType.PERPLEXITY.value, "tool_awareness")
            agent_system = prompt_manager.get_agent_system_prompt(ModelType.PERPLEXITY.value)
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ìƒì„±
            tool_list = []
            for tool in self.tools[:10]:
                tool_desc = getattr(tool, 'description', tool.name)
                tool_list.append(f"- {tool.name}: {tool_desc[:80]}")
            
            tools_summary = "\n".join(tool_list) if tool_list else "No tools available"
            
            enhanced_prompt = f"""{base_prompt}

{tool_awareness}

**AVAILABLE MCP TOOLS:**
{tools_summary}

{agent_system}

**CRITICAL REMINDER**: When user asks for data, search, current information, or external resources, IMMEDIATELY use the appropriate MCP tool. These tools provide real-time, accurate data that surpasses your training knowledge."""
            
            return enhanced_prompt
        else:
            return base_prompt
    
    def supports_streaming(self) -> bool:
        """PerplexityëŠ” ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›"""
        return False