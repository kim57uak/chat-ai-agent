# ë²¡í„°í™” ì„±ëŠ¥ ë³‘ëª© ë¶„ì„ ë° ê°œì„ 

## ğŸŒ í˜„ì¬ ë³‘ëª© í¬ì¸íŠ¸

### 1. **ìˆœì°¨ ì²˜ë¦¬** (ê°€ì¥ í° ë³‘ëª©)
```python
# í˜„ì¬: íŒŒì¼ì„ í•˜ë‚˜ì”© ì²˜ë¦¬
for file in files:  # âŒ ìˆœì°¨
    process_file(file)
```
- 100ê°œ íŒŒì¼ Ã— 5ì´ˆ = **500ì´ˆ (8ë¶„)**

### 2. **ê°œë³„ ì„ë² ë”© í˜¸ì¶œ**
```python
# í˜„ì¬: ì²­í¬ë§ˆë‹¤ ê°œë³„ í˜¸ì¶œ
for chunk in chunks:  # âŒ ë¹„íš¨ìœ¨
    vector = embeddings.embed_documents([chunk])
```
- ë„¤íŠ¸ì›Œí¬/ëª¨ë¸ ì˜¤ë²„í—¤ë“œ Ã— ì²­í¬ ìˆ˜

### 3. **ëª¨ë¸ ì¬ì´ˆê¸°í™”**
```python
# í˜„ì¬: ë§¤ë²ˆ ëª¨ë¸ ë¡œë“œ
embeddings = EmbeddingFactory.create_embeddings()  # âŒ 2-3ì´ˆ
```
- ì´ë¯¸ í•´ê²°ë¨ (ì„ë² ë”© í’€ ì‚¬ìš©)

### 4. **DB ì“°ê¸° ìµœì í™” ë¶€ì¡±**
```python
# í˜„ì¬: ì²­í¬ë§ˆë‹¤ ê°œë³„ ì‚½ì…
for chunk in chunks:  # âŒ ëŠë¦¼
    db.insert(chunk)
```

## âœ… ê°œì„  ë°©ì•ˆ

### 1. ë³‘ë ¬ ì²˜ë¦¬ (4ë°° í–¥ìƒ)
```python
# ê°œì„ : 4ê°œ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(process_file, f) for f in files]
```
- 100ê°œ íŒŒì¼ Ã· 4 = **125ì´ˆ (2ë¶„)**

### 2. ë°°ì¹˜ ì„ë² ë”© (10ë°° í–¥ìƒ)
```python
# ê°œì„ : 32ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
batch_size = 32
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    vectors = embeddings.embed_documents(batch)  # âœ… ë°°ì¹˜
```
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
- GPU í™œìš©ë„ ì¦ê°€

### 3. ë²¡í„° DB ë°°ì¹˜ ì‚½ì…
```python
# ê°œì„ : í•œ ë²ˆì— ì‚½ì…
db.add_documents(all_chunks, embeddings=all_vectors)  # âœ… ë°°ì¹˜
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | 100ê°œ íŒŒì¼ | ê°œì„ ìœ¨ |
|------|-----------|--------|
| **í˜„ì¬ (ìˆœì°¨)** | 500ì´ˆ (8ë¶„) | - |
| **ë³‘ë ¬ (4 workers)** | 125ì´ˆ (2ë¶„) | 4ë°° âš¡ |
| **ë³‘ë ¬ + ë°°ì¹˜ ì„ë² ë”©** | 50ì´ˆ (1ë¶„) | 10ë°° âš¡âš¡ |
| **ë³‘ë ¬ + ë°°ì¹˜ + DB ìµœì í™”** | 30ì´ˆ | 16ë°° âš¡âš¡âš¡ |

## ğŸš€ ìµœì í™” êµ¬í˜„

### ì˜µì…˜ 1: ê°„ë‹¨í•œ ë³‘ë ¬í™” (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

```python
# core/rag/batch/batch_processor.py ìˆ˜ì •
from concurrent.futures import ThreadPoolExecutor

class BatchProcessor:
    def __init__(self, storage_manager, embeddings, max_workers: int = 4):
        self.max_workers = max_workers  # âœ… ë³‘ë ¬ ì²˜ë¦¬
    
    def process_files(self, files, topic_id):
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self._process_file, f, topic_id): f 
                      for f in files}
            
            for future in as_completed(futures):
                result = future.result()
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
```

### ì˜µì…˜ 2: ë°°ì¹˜ ì„ë² ë”© ì¶”ê°€ (ì¤‘ê°„ ë‚œì´ë„)

```python
def _embed_batch(self, texts: List[str], batch_size: int = 32):
    """ë°°ì¹˜ ì„ë² ë”©"""
    all_vectors = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        vectors = self.embeddings.embed_documents(batch)  # âœ… ë°°ì¹˜
        all_vectors.extend(vectors)
    return all_vectors
```

### ì˜µì…˜ 3: ì™„ì „ ìµœì í™” (ê³ ê¸‰)

```python
# docs/VECTORIZATION_PERFORMANCE.md ì°¸ì¡°
# BatchProcessorOptimized í´ë˜ìŠ¤ ì‚¬ìš©
```

## ğŸ¯ ê¶Œì¥ ì ìš© ìˆœì„œ

### Phase 1: ë³‘ë ¬ ì²˜ë¦¬ (ì¦‰ì‹œ)
```python
# config.json ë˜ëŠ” rag_config.json
{
  "batch_upload": {
    "max_workers": 4  # âœ… CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ
  }
}
```

### Phase 2: ë°°ì¹˜ ì„ë² ë”© (1ì£¼ì¼ ë‚´)
```python
# embeddings.embed_documents() í˜¸ì¶œ ì‹œ
# ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ë¬¶ì–´ì„œ í˜¸ì¶œ
```

### Phase 3: DB ìµœì í™” (2ì£¼ì¼ ë‚´)
```python
# LanceDB add_documents() ìµœì í™”
# íŠ¸ëœì­ì…˜ ë°°ì¹˜ ì²˜ë¦¬
```

## ğŸ’¡ ì¶”ê°€ ìµœì í™” íŒ

### 1. íŒŒì¼ í¬ê¸° í•„í„°ë§
```python
# ë„ˆë¬´ í° íŒŒì¼ ê±´ë„ˆë›°ê¸°
if file.stat().st_size > 50 * 1024 * 1024:  # 50MB
    logger.warning(f"Skipping large file: {file.name}")
    continue
```

### 2. ìºì‹±
```python
# ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°
if file_hash in processed_hashes:
    logger.info(f"Skipping cached: {file.name}")
    continue
```

### 3. ì²­í‚¹ ì „ëµ ìµœì í™”
```python
# ì‘ì€ ì²­í¬ = ë” ë§ì€ ì„ë² ë”© í˜¸ì¶œ
# í° ì²­í¬ = ê²€ìƒ‰ ì •í™•ë„ ì €í•˜
# ìµœì : 500-1000 í† í°
```

### 4. GPU í™œìš©
```python
# sentence-transformersëŠ” ìë™ìœ¼ë¡œ GPU ì‚¬ìš©
# CUDA ì„¤ì¹˜ í™•ì¸
import torch
print(torch.cuda.is_available())  # Trueë©´ GPU ì‚¬ìš©
```

## ğŸ”§ ì„¤ì • ì˜ˆì‹œ

### ë¹ ë¥¸ ì²˜ë¦¬ (ì •í™•ë„ ì•½ê°„ í¬ìƒ)
```json
{
  "batch_upload": {
    "max_workers": 8,
    "batch_size": 64,
    "max_file_size_mb": 10
  },
  "chunking": {
    "window_size": 1000,
    "overlap_ratio": 0.1
  }
}
```

### ê· í˜• (ê¶Œì¥)
```json
{
  "batch_upload": {
    "max_workers": 4,
    "batch_size": 32,
    "max_file_size_mb": 50
  },
  "chunking": {
    "window_size": 500,
    "overlap_ratio": 0.2
  }
}
```

### ê³ í’ˆì§ˆ (ëŠë¦¼)
```json
{
  "batch_upload": {
    "max_workers": 2,
    "batch_size": 16,
    "max_file_size_mb": 100
  },
  "chunking": {
    "window_size": 300,
    "overlap_ratio": 0.3
  }
}
```

## ğŸ“ˆ ì‹¤ì œ ì¸¡ì • ë°©ë²•

```python
import time

start = time.time()
processor.process_files(files, topic_id)
elapsed = time.time() - start

print(f"ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ")
print(f"íŒŒì¼ë‹¹ í‰ê· : {elapsed/len(files):.2f}ì´ˆ")
print(f"ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: {len(files)/(elapsed/3600):.0f}ê°œ")
```

## ğŸ“ ê²°ë¡ 

**ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ **:
1. âœ… `max_workers=4` ì„¤ì • (ë³‘ë ¬ ì²˜ë¦¬)
2. âœ… ì„ë² ë”© í’€ ì‚¬ìš© (ì´ë¯¸ ì ìš©ë¨)
3. âœ… íŒŒì¼ í¬ê¸° ì œí•œ

**ì˜ˆìƒ íš¨ê³¼**:
- 100ê°œ íŒŒì¼: 8ë¶„ â†’ **2ë¶„** (4ë°° í–¥ìƒ)
- ì‚¬ìš©ì ê²½í—˜: ëŒ€í­ ê°œì„ 
- ì¶”ê°€ ë¹„ìš©: ì—†ìŒ
