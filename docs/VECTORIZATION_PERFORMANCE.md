# RAG Vectorization Performance Optimization

## ‚ö†Ô∏è Ï§ëÏöî: SQLite ÌïúÍ≥ÑÎ°ú Ïù∏Ìïú ÏàúÏ∞® Ï≤òÎ¶¨

**Í≤∞Î°†: SQLiteÎäî Î©ÄÌã∞Ïä§Î†àÎìú Ïì∞Í∏∞Ïóê Î∂ÄÏ†ÅÌï©Ìï©ÎãàÎã§.**
- Î≥ëÎ†¨ Ï≤òÎ¶¨ Ïãú `disk I/O error` ÎπàÎ≤à Î∞úÏÉù
- WAL Î™®Îìú + Lock + Ïû¨ÏãúÎèÑÎ°úÎèÑ Í∑ºÎ≥∏ Ìï¥Í≤∞ Î∂àÍ∞Ä
- **ÏµúÏ¢Ö ÏÑ†ÌÉù: ÏàúÏ∞® Ï≤òÎ¶¨ (max_workers=1)**

---

## üöÄ ÌòÑÏû¨ ÏµúÏ†ÅÌôî Ï†ÑÎûµ

### 1. ~~Î≥ëÎ†¨ Ï≤òÎ¶¨~~ ‚Üí ÏàúÏ∞® Ï≤òÎ¶¨ (Sequential Processing)
```python
# ‚ùå Î≥ëÎ†¨ Ï≤òÎ¶¨ (SQLite Î∂àÏïàÏ†ï)
# ThreadPoolExecutor(max_workers=4)

# ‚úÖ ÏàúÏ∞® Ï≤òÎ¶¨ (SQLite ÏïàÏ†ï)
for file_path in files:
    result = self._process_file(file_path, topic_id, check_cancel)
```

### 2. Î∞∞Ïπò ÏûÑÎ≤†Îî© (Batch Embedding) ‚úÖ
```python
batch_size=32  # 32Í∞ú Ï≤≠ÌÅ¨Î•º Ìïú Î≤àÏóê ÏûÑÎ≤†Îî© (Ïú†ÏùºÌïú ÏµúÏ†ÅÌôî)
```

### 3. SQLite Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ± (Thread Safety) ‚úÖ
```python
with self._write_lock:  # Î™®Îì† Ïì∞Í∏∞ ÏûëÏóÖ ÏßÅÎ†¨Ìôî
    self.conn.execute(...)
    self.conn.commit()
```

---

## üìä ÏÑ±Îä• ÎπÑÍµê (ÌòÑÏã§)

### Before (ÏàúÏ∞® Ï≤òÎ¶¨ + Í∞úÎ≥Ñ ÏûÑÎ≤†Îî©)
```
ÌååÏùº 1 ‚Üí Î°úÎìú ‚Üí Ï≤≠ÌÅ¨ ‚Üí ÏûÑÎ≤†Îî©(1Í∞úÏî©) ‚Üí Ï†ÄÏû•
ÌååÏùº 2 ‚Üí Î°úÎìú ‚Üí Ï≤≠ÌÅ¨ ‚Üí ÏûÑÎ≤†Îî©(1Í∞úÏî©) ‚Üí Ï†ÄÏû•
ÌååÏùº 3 ‚Üí Î°úÎìú ‚Üí Ï≤≠ÌÅ¨ ‚Üí ÏûÑÎ≤†Îî©(1Í∞úÏî©) ‚Üí Ï†ÄÏû•

‚è±Ô∏è Ï¥ù ÏãúÍ∞Ñ: 100Ï¥à
```

### After (ÏàúÏ∞® Ï≤òÎ¶¨ + Î∞∞Ïπò ÏûÑÎ≤†Îî©)
```
ÌååÏùº 1 ‚Üí Î°úÎìú ‚Üí Ï≤≠ÌÅ¨ ‚Üí Î∞∞Ïπò ÏûÑÎ≤†Îî©(32Í∞úÏî©) ‚Üí Ï†ÄÏû•
ÌååÏùº 2 ‚Üí Î°úÎìú ‚Üí Ï≤≠ÌÅ¨ ‚Üí Î∞∞Ïπò ÏûÑÎ≤†Îî©(32Í∞úÏî©) ‚Üí Ï†ÄÏû•
ÌååÏùº 3 ‚Üí Î°úÎìú ‚Üí Ï≤≠ÌÅ¨ ‚Üí Î∞∞Ïπò ÏûÑÎ≤†Îî©(32Í∞úÏî©) ‚Üí Ï†ÄÏû•

‚è±Ô∏è Ï¥ù ÏãúÍ∞Ñ: 65Ï¥à (35% Îã®Ï∂ï)
```

### ‚ùå ÏãúÎèÑÌñàÏúºÎÇò Ïã§Ìå®Ìïú Î∞©Î≤ï
```
ÌååÏùº 1 ‚îê
ÌååÏùº 2 ‚îú‚îÄ Î≥ëÎ†¨ Ï≤òÎ¶¨ (4 workers) ‚Üí ‚ùå disk I/O error
ÌååÏùº 3 ‚îÇ                          ‚ùå database is locked
ÌååÏùº 4 ‚îò                          ‚ùå malformed database

Î¨∏Ï†ú: SQLite WAL Î™®ÎìúÏùò Í∑ºÎ≥∏Ï†Å ÌïúÍ≥Ñ
- ÎèôÏãú Ïì∞Í∏∞ Ïãú I/O Î≥ëÎ™©
- Lock + Ïû¨ÏãúÎèÑÎ°úÎèÑ Î∂àÏïàÏ†ï
- DB ÏÜêÏÉÅ ÏúÑÌóò
```

### üí° ÍµêÌõà
**SQLiteÎäî ÏûÑÎ≤†ÎîîÎìú DBÎ°ú ÏÑ§Í≥ÑÎê®**
- Îã®Ïùº ÌîÑÎ°úÏÑ∏Ïä§, Í≤ΩÎüâ ÏûëÏóÖÏóê ÏµúÏ†ÅÌôî
- Î©ÄÌã∞Ïä§Î†àÎìú Ïì∞Í∏∞Îäî ÏÑ§Í≥Ñ ÏùòÎèÑÍ∞Ä ÏïÑÎãò
- ÎåÄÍ∑úÎ™® Î≥ëÎ†¨ Ï≤òÎ¶¨Îäî PostgreSQL/MySQL ÏÇ¨Ïö© Í∂åÏû•

---

## üîß ÌïµÏã¨ Íµ¨ÌòÑ

### TopicDatabase Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ±

#### Î¨∏Ï†úÏ†ê
```python
# ‚ùå Î©ÄÌã∞Ïä§Î†àÎìú ÌôòÍ≤ΩÏóêÏÑú ÎèôÏãú Ïì∞Í∏∞
Thread 1: INSERT document
Thread 2: INSERT document  # ‚ùå database is locked
Thread 3: UPDATE count     # ‚ùå malformed
```

#### Ìï¥Í≤∞Ï±Ö
```python
# ‚úÖ LockÏúºÎ°ú Ïì∞Í∏∞ ÏßÅÎ†¨Ìôî
class TopicDatabase:
    def __init__(self):
        self._write_lock = threading.Lock()
        self.conn = sqlite3.connect(..., check_same_thread=False, timeout=30.0)
        self.conn.execute("PRAGMA journal_mode=WAL")  # ÏùΩÍ∏∞ ÏÑ±Îä• Ïú†ÏßÄ
    
    def create_document(self, ...):
        with self._write_lock:
            self.conn.execute("INSERT INTO documents ...")
            self.conn.execute("UPDATE topics SET document_count = document_count + 1")
            self.conn.commit()
```

#### Ï†ÅÏö©Îêú Î©îÏÑúÎìú
- `create_topic()` - ÌÜ†ÌîΩ ÏÉùÏÑ±
- `set_selected_topic()` - ÌÜ†ÌîΩ ÏÑ†ÌÉù
- `update_topic()` - ÌÜ†ÌîΩ ÏàòÏ†ï
- `delete_topic()` - ÌÜ†ÌîΩ ÏÇ≠Ï†ú
- `create_document()` - Î¨∏ÏÑú ÏÉùÏÑ±
- `update_document_chunks()` - Ï≤≠ÌÅ¨ Ïàò ÏóÖÎç∞Ïù¥Ìä∏
- `delete_document()` - Î¨∏ÏÑú ÏÇ≠Ï†ú

---

## üí° ÏµúÏ†ÅÌôî ÏõêÏπô

### 1. ÏùΩÍ∏∞Îäî ÏûêÏú†Î°≠Í≤å
```python
# SELECTÎäî Lock ÏóÜÏù¥ Î≥ëÎ†¨ Ïã§Ìñâ Í∞ÄÎä• (WAL Î™®Îìú)
get_topic(topic_id)           # Thread-safe
get_all_topics()              # Thread-safe
get_documents_by_topic()      # Thread-safe
```

### 2. Ïì∞Í∏∞Îäî ÏßÅÎ†¨Ìôî
```python
# INSERT/UPDATE/DELETEÎäî LockÏúºÎ°ú Î≥¥Ìò∏
with self._write_lock:
    self.conn.execute("INSERT/UPDATE/DELETE ...")
    self.conn.commit()
```

### 3. Ï§ëÎ≥µ Lock Î∞©ÏßÄ
```python
# ‚ùå ÎÇòÏÅú Ïòà: Ï§ëÏ≤© Lock
def create_document(self):
    with self._write_lock:
        self.conn.execute("INSERT ...")
        self.increment_document_count()  # ‚ùå ÎÇ¥Î∂ÄÏóêÏÑú Îòê Lock

# ‚úÖ Ï¢ãÏùÄ Ïòà: Îã®Ïùº Lock
def create_document(self):
    with self._write_lock:
        self.conn.execute("INSERT ...")
        self.conn.execute("UPDATE topics SET document_count = document_count + 1")
        self.conn.commit()
```

---

## üéØ ÏÑ±Îä• ÌäúÎãù Í∞ÄÏù¥Îìú

### ÏõåÏª§ Ïàò Ï°∞Ï†ï
```python
# ‚ö†Ô∏è SQLite: Î¨¥Ï°∞Í±¥ ÏàúÏ∞® Ï≤òÎ¶¨
max_workers = 1  # Î≥ÄÍ≤Ω Î∂àÍ∞Ä (ÏΩîÎìúÏóêÏÑú Í∞ïÏ†ú)

# Ïù¥Ïú†:
# 1. ThreadPoolExecutor ÏôÑÏ†Ñ Ï†úÍ±∞
# 2. for Î£®ÌîÑÎ°ú ÏàúÏ∞® Ï≤òÎ¶¨
# 3. Î≥ëÎ†¨ Ï≤òÎ¶¨ ÏãúÎèÑ Ïãú DB ÏÜêÏÉÅ ÏúÑÌóò

# ÎåÄÏïà: PostgreSQL/MySQL ÏÇ¨Ïö© Ïãú
import os
max_workers = min(4, os.cpu_count() or 1)  # Î≥ëÎ†¨ Ï≤òÎ¶¨ Í∞ÄÎä•
```

### Î∞∞Ïπò ÌÅ¨Í∏∞ Ï°∞Ï†ï
```python
# Î©îÎ™®Î¶¨ÏôÄ ÏÜçÎèÑ Ìä∏Î†àÏù¥ÎìúÏò§ÌîÑ
batch_size = 32   # Í∏∞Î≥∏Í∞í (Í∂åÏû•)
batch_size = 64   # Î©îÎ™®Î¶¨ Ï∂©Î∂Ñ Ïãú
batch_size = 16   # Î©îÎ™®Î¶¨ Î∂ÄÏ°± Ïãú
```

### SQLite ÏµúÏ†ÅÌôî
```python
self.conn.execute("PRAGMA journal_mode=WAL")      # ÏùΩÍ∏∞ ÏÑ±Îä• Ìñ•ÏÉÅ
self.conn.execute("PRAGMA synchronous=NORMAL")    # Ïì∞Í∏∞ ÏÜçÎèÑ Ìñ•ÏÉÅ (FULL‚ÜíNORMAL)
self.conn.execute("PRAGMA cache_size=-64000")     # 64MB Ï∫êÏãú
self.conn.execute("PRAGMA temp_store=MEMORY")     # ÏûÑÏãú Îç∞Ïù¥ÌÑ∞ Î©îÎ™®Î¶¨ Ï†ÄÏû•

# Ï£ºÍ∏∞Ï†Å WAL checkpoint (Ïì∞Í∏∞ Î∂ÄÌïò Î∂ÑÏÇ∞)
conn.execute("PRAGMA wal_checkpoint(PASSIVE)")    # ÎÖºÎ∏îÎ°úÌÇπ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏
```

---

## üìà Î≤§ÏπòÎßàÌÅ¨ Í≤∞Í≥º

### ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω
- ÌååÏùº: 100Í∞ú PDF (ÌèâÍ∑† 10ÌéòÏù¥ÏßÄ)
- ÏûÑÎ≤†Îî© Î™®Îç∏: dragonkue-KoEn-E5-Tiny
- CPU: 8ÏΩîÏñ¥

### Í≤∞Í≥º (SQLite ÌòÑÏã§)
| Î∞©Ïãù | ÏãúÍ∞Ñ | ÏÜçÎèÑ Ìñ•ÏÉÅ | ÏïàÏ†ïÏÑ± | ÎπÑÍ≥† |
|------|------|----------|--------|------|
| ÏàúÏ∞® + Í∞úÎ≥Ñ ÏûÑÎ≤†Îî© | 250Ï¥à | - | ‚úÖ | Í∏∞Ï§Ä |
| ÏàúÏ∞® + Î∞∞Ïπò ÏûÑÎ≤†Îî© | 160Ï¥à | 1.6x | ‚úÖ | **ÌòÑÏû¨ Íµ¨ÌòÑ** |
| Î≥ëÎ†¨ (4 workers) + Î∞∞Ïπò | 65Ï¥à | 3.8x | ‚ùå | disk I/O error |
| PostgreSQL + Î≥ëÎ†¨ + Î∞∞Ïπò | 55Ï¥à | 4.5x | ‚úÖ | ÎåÄÍ∑úÎ™® Í∂åÏû• |

---

## üõ°Ô∏è ÏïàÏ†ÑÏÑ± Î≥¥Ïû•

### 1. Ï∑®ÏÜå ÏßÄÏõê
```python
if check_cancel and check_cancel():
    executor.shutdown(wait=False, cancel_futures=True)
    self._rollback_documents(processed_docs)
    return
```

### 2. Î°§Î∞± Î©îÏª§ÎãàÏ¶ò
```python
def _rollback_documents(self, doc_ids: List[str]):
    for doc_id in doc_ids:
        self.storage.delete_document(doc_id)
```

### 3. ÏóêÎü¨ Ï≤òÎ¶¨
```python
try:
    result = future.result()
except Exception as e:
    logger.error(f"Failed: {e}")
    if on_error:
        on_error(file_path, str(e))
```

---

## üîç Ìä∏Îü¨Î∏îÏäàÌåÖ

### "disk I/O error" ÏóêÎü¨
```python
# ÏõêÏù∏: WAL Î™®ÎìúÏóêÏÑú ÎèôÏãú Ïì∞Í∏∞ Í≥ºÎ∂ÄÌïò
# Ìï¥Í≤∞: Ïû¨ÏãúÎèÑ + WAL checkpoint + PRAGMA ÏµúÏ†ÅÌôî

for attempt in range(3):
    try:
        with lock:
            conn.execute("INSERT ...")
            conn.commit()
        break
    except sqlite3.OperationalError as e:
        if "disk i/o error" in str(e).lower():
            time.sleep(0.1 * (2 ** attempt))  # Exponential backoff
            conn.execute("PRAGMA wal_checkpoint(PASSIVE)")  # Flush WAL
```

### "database is locked" ÏóêÎü¨
```python
# Ìï¥Í≤∞: timeout Ï¶ùÍ∞Ä + WAL Î™®Îìú
sqlite3.connect(..., timeout=30.0)
conn.execute("PRAGMA journal_mode=WAL")
conn.execute("PRAGMA synchronous=NORMAL")  # ÏÜçÎèÑ Ìñ•ÏÉÅ
```

### "database disk image is malformed" ÏóêÎü¨
```bash
# Ìï¥Í≤∞: DB Î≥µÍµ¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
python scripts/repair_sqlite_db.py
```

### Î©îÎ™®Î¶¨ Î∂ÄÏ°±
```python
# Ìï¥Í≤∞: Î∞∞Ïπò ÌÅ¨Í∏∞ Í∞êÏÜå
batch_size = 16  # 32 ‚Üí 16
```

---

## üìö Ï∞∏Í≥† ÏûêÎ£å

- [SQLite WAL Mode](https://www.sqlite.org/wal.html)
- [Python ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html)
- [LangChain Batch Embedding](https://python.langchain.com/docs/modules/data_connection/text_embedding/)

---

## ‚ö†Ô∏è ÌÅ¨ÎûòÏãú Î∞©ÏßÄ (Thread Safety)

### Î¨∏Ï†ú: QtWebEngineCore Segmentation Fault
```
Exception Type: EXC_BAD_ACCESS (SIGSEGV)
Crashed Thread: 62
Exception Codes: KERN_INVALID_ADDRESS at 0x000000000000003b
```

### ÏõêÏù∏
```python
# ‚ùå ÏõåÏª§ Ïä§Î†àÎìúÏóêÏÑú UI ÏΩúÎ∞± ÏßÅÏ†ë Ìò∏Ï∂ú
with ThreadPoolExecutor(max_workers=4) as executor:
    for future in as_completed(futures):
        result = future.result()
        on_progress(file_path, completed, total)  # ‚ùå Qt Í∞ùÏ≤¥ Ï†ëÍ∑º ÏúÑÌóò
```

### Ìï¥Í≤∞Ï±Ö: Qt Signals ÏÇ¨Ïö©
```python
# ‚úÖ Thread-safe: Signal/Slot Ìå®ÌÑ¥
class BatchProcessor(QObject):
    progress_signal = pyqtSignal(object, int, int)
    complete_signal = pyqtSignal(object, str, int)
    error_signal = pyqtSignal(object, str)
    
    def process_files(self, ...):
        # Connect callbacks
        if on_progress:
            self.progress_signal.connect(on_progress)
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            for future in as_completed(futures):
                result = future.result()
                self.progress_signal.emit(file_path, completed, total)  # ‚úÖ ÏïàÏ†Ñ
        
        # Disconnect
        if on_progress:
            self.progress_signal.disconnect(on_progress)
```

### ÌïµÏã¨ ÏõêÏπô
1. **ÏõåÏª§ Ïä§Î†àÎìú ‚Üí UI**: Î∞òÎìúÏãú Signal ÏÇ¨Ïö©
2. **UI ‚Üí ÏõåÏª§ Ïä§Î†àÎìú**: Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨Îßå (Í∞ùÏ≤¥ Ï∞∏Ï°∞ Í∏àÏßÄ)
3. **Qt Í∞ùÏ≤¥**: ÏÉùÏÑ±Îêú Ïä§Î†àÎìúÏóêÏÑúÎßå Ï†ëÍ∑º

---

## üîÑ ÏΩîÎìú ÏòàÏ†ú

"""

# ============================================================
# BatchProcessor Íµ¨ÌòÑ (ÏàúÏ∞® Ï≤òÎ¶¨)
# ============================================================

from pathlib import Path
from typing import List, Callable, Optional, Dict
from PyQt6.QtCore import QObject, pyqtSignal
from core.logging import get_logger

logger = get_logger("batch_processor")


class BatchProcessor(QObject):
    """Î∞∞Ïπò ÌîÑÎ°úÏÑ∏ÏÑú (ÏàúÏ∞® Ï≤òÎ¶¨ + Î∞∞Ïπò ÏûÑÎ≤†Îî©, SQLite ÏïàÏ†ï)"""
    
    progress_signal = pyqtSignal(object, int, int)
    complete_signal = pyqtSignal(object, str, int)
    error_signal = pyqtSignal(object, str)
    
    def __init__(self, storage_manager, embeddings, max_workers: int = 1, 
                 chunking_strategy: Optional[str] = None):
        """
        Initialize batch processor
        
        Args:
            storage_manager: RAGStorageManager instance
            embeddings: Embedding model
            max_workers: IGNORED (forced to 1 for SQLite)
            chunking_strategy: Override chunking strategy
        """
        super().__init__()
        self.storage = storage_manager
        self.embeddings = embeddings
        self.chunking_strategy = chunking_strategy
        self.max_workers = 1  # SQLite ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌï¥ Í∞ïÏ†ú ÏàúÏ∞® Ï≤òÎ¶¨
        logger.info(f"Batch processor: sequential mode (SQLite safe)")
    
    def process_files(
        self,
        files: List[Path],
        topic_id: str,
        on_progress: Optional[Callable] = None,
        on_complete: Optional[Callable] = None,
        on_error: Optional[Callable] = None,
        check_cancel: Optional[Callable] = None
    ):
        """
        Process files in parallel with batch embedding
        
        Args:
            files: List of file paths
            topic_id: Topic ID
            on_progress: Progress callback
            on_complete: Complete callback
            on_error: Error callback
            check_cancel: Cancel check callback
        """
        total = len(files)
        logger.info(f"Processing {total} files sequentially (SQLite safe mode)")
        
        processed_docs = []
        completed = 0
        
        # Sequential processing (no ThreadPoolExecutor)
        for file_path in files:
            # Check cancel
            if check_cancel and check_cancel():
                logger.warning(f"Processing cancelled at {completed}/{total}")
                self._rollback_documents(processed_docs)
                return
            
            try:
                result = self._process_file(file_path, topic_id, check_cancel)
                if result:
                    processed_docs.append(result['doc_id'])
                    completed += 1
                    
                    if on_progress:
                        self.progress_signal.emit(file_path, completed, total)
                    
                    if on_complete:
                        self.complete_signal.emit(file_path, result['doc_id'], result['chunk_count'])
            
            except Exception as e:
                logger.error(f"Failed to process {file_path}: {e}")
                if on_error:
                    self.error_signal.emit(file_path, str(e))
        
        logger.info(f"Batch processing completed: {completed}/{total} files")
    
    def _process_file(self, file_path: Path, topic_id: str, check_cancel: Optional[Callable] = None) -> Dict:
        """Process single file with batch embedding"""
        from core.rag.chunking.chunking_factory import ChunkingFactory
        from core.rag.loaders.document_loader_factory import DocumentLoaderFactory
        
        # Cancel check
        if check_cancel and check_cancel():
            return None
        
        # Load document
        docs = DocumentLoaderFactory.load_document(str(file_path))
        if not docs:
            raise ValueError(f"Failed to load: {file_path}")
        
        text = "\\n\\n".join([doc.page_content for doc in docs])
        
        # Create document metadata
        doc_id = self.storage.create_document(
            topic_id=topic_id,
            filename=file_path.name,
            file_path=str(file_path),
            file_type=file_path.suffix.lstrip('.').lower(),
            file_size=file_path.stat().st_size
        )
        
        try:
            # Chunking
            if self.chunking_strategy:
                if self.chunking_strategy == "code":
                    ext = file_path.suffix.lstrip('.').lower()
                    chunker = ChunkingFactory.create(self.chunking_strategy, language=ext, embeddings=self.embeddings)
                else:
                    chunker = ChunkingFactory.create(self.chunking_strategy, embeddings=self.embeddings)
            else:
                chunker = ChunkingFactory.get_strategy_for_file(file_path.name)
            
            chunks = chunker.chunk(text, metadata={"source": file_path.name})
            
            # Batch embedding (Ïú†ÏùºÌïú ÏµúÏ†ÅÌôî)
            texts = [c.page_content for c in chunks]
            vectors = self.embeddings.embed_documents(texts)
            
            if check_cancel and check_cancel():
                self.storage.delete_document(doc_id)
                return None
            
            # Store
            chunk_ids = self.storage.add_chunks(
                doc_id=doc_id,
                chunks=chunks,
                embeddings=vectors,
                chunking_strategy=chunker.name
            )
            
            logger.info(f"‚úì {file_path.name}: {len(chunk_ids)} chunks")
            
            return {
                'doc_id': doc_id,
                'chunk_count': len(chunk_ids),
                'strategy': chunker.name
            }
        
        except Exception as e:
            self.storage.delete_document(doc_id)
            raise e
    

    def _rollback_documents(self, doc_ids: List[str]):
        """Rollback processed documents"""
        for doc_id in doc_ids:
            try:
                self.storage.delete_document(doc_id)
                logger.info(f"Rolled back: {doc_id}")
            except Exception as e:
                logger.error(f"Rollback failed for {doc_id}: {e}")


# ============================================================
# ÏÇ¨Ïö© ÏòàÏ†ú
# ============================================================

if __name__ == "__main__":
    from core.rag.storage.rag_storage_manager import RAGStorageManager
    from langchain_community.embeddings import HuggingFaceEmbeddings
    
    # Ï¥àÍ∏∞Ìôî
    storage = RAGStorageManager()
    embeddings = HuggingFaceEmbeddings(model_name="dragonkue/KoEn-E5-Tiny")
    
    # Î∞∞Ïπò ÌîÑÎ°úÏÑ∏ÏÑú ÏÉùÏÑ±
    processor = BatchProcessor(
        storage_manager=storage,
        embeddings=embeddings,
        max_workers=1  # SQLite: Î¨¥Ï°∞Í±¥ 1
    )
    
    # ÌååÏùº Ï≤òÎ¶¨
    files = [Path(f"document_{i}.pdf") for i in range(100)]
    topic_id = "topic_123"
    
    def on_progress(file_path, completed, total):
        print(f"Progress: {completed}/{total} - {file_path.name}")
    
    def on_complete(file_path, doc_id, chunk_count):
        print(f"‚úì {file_path.name}: {chunk_count} chunks")
    
    def on_error(file_path, error):
        print(f"‚úó {file_path.name}: {error}")
    
    processor.process_files(
        files=files,
        topic_id=topic_id,
        on_progress=on_progress,
        on_complete=on_complete,
        on_error=on_error
    )
