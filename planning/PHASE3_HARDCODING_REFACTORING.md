# ğŸ”§ Phase 3 í•˜ë“œì½”ë”© ì œê±° ë¦¬íŒ©í† ë§ ì™„ë£Œ

## ğŸ“‹ ì‘ì—… ê°œìš”

**ëª©í‘œ:** AIê°€ contextë¥¼ ë¶„ì„í•´ì„œ ììœ¨ì ìœ¼ë¡œ ê²°ì •í•˜ë„ë¡ í•˜ë“œì½”ë”© ì œê±°

**ì›ì¹™:** Rule 12 ì¤€ìˆ˜ - "ë„êµ¬ì‚¬ìš© íŒë‹¨ì€ í•˜ë“œì½”ë”©ì´ ì•„ë‹ˆë¼ AIê°€ contextë¥¼ íŒŒì•…í•´ì„œ ê²°ì •í•˜ë„ë¡ í• ê²ƒ"

---

## ğŸ”´ ë°œê²¬ëœ í•˜ë“œì½”ë”© ë¬¸ì œì 

### 1. **hybrid_analyzer.py**
```python
# âŒ í•˜ë“œì½”ë”©: í‚¤ì›Œë“œ ê¸°ë°˜ Intent ê°ì§€
if any(word in query_lower for word in ["ì°¾ì•„", "ê²€ìƒ‰", "find", "search"]):
    return "search"

# âŒ í•˜ë“œì½”ë”©: íŒŒì¼ íƒ€ì… ë¦¬ìŠ¤íŠ¸
file_types = [".pdf", ".xlsx", ".docx", ".csv", ".txt"]

# âŒ í•˜ë“œì½”ë”©: Intent â†’ Agent ë§¤í•‘
if intent == "search":
    required.append("RAGAgent")
elif intent == "analyze":
    required.extend(["RAGAgent", "PandasAgent"])
```

### 2. **multi_agent_orchestrator.py**
```python
# âŒ í•˜ë“œì½”ë”©: ê·œì¹™ ê¸°ë°˜ Agent ì„ íƒ
for agent in self.agents:
    if agent.can_handle(query, context):
        return agent

# âœ… LLM ê¸°ë°˜ ë©”ì„œë“œëŠ” ìˆì§€ë§Œ ì‚¬ìš© ì•ˆ í•¨
def _select_agent_with_llm(self, query: str):
    # êµ¬í˜„ë˜ì–´ ìˆì§€ë§Œ run()ì—ì„œ í˜¸ì¶œ ì•ˆ í•¨
```

---

## âœ… í•´ê²° ë°©ì•ˆ

### 1. **Intent Detection - LLM ê¸°ë°˜**

**Before:**
```python
def _detect_intent(self, query: str) -> str:
    query_lower = query.lower()
    if any(word in query_lower for word in ["ì°¾ì•„", "ê²€ìƒ‰"]):
        return "search"
    # ...
```

**After:**
```python
def _detect_intent(self, query: str) -> str:
    """Detect query intent using LLM"""
    prompt = f"""Analyze the user's intent from the query and return ONLY ONE of these categories:
- search: Finding or retrieving information
- analyze: Analyzing, summarizing, or processing data
- create: Creating, generating, or building something
- general: General conversation or unclear intent

Query: {query}

Return only the category name (search/analyze/create/general):"""
    
    response = self.llm.invoke([HumanMessage(content=prompt)])
    intent = response.content.strip().lower()
    
    if intent in ["search", "analyze", "create", "general"]:
        return intent
    return "general"
```

**ì¥ì :**
- âœ… í‚¤ì›Œë“œ í•˜ë“œì½”ë”© ì œê±°
- âœ… ë‹¤êµ­ì–´ ìë™ ì§€ì›
- âœ… Context ê¸°ë°˜ ìœ ì—°í•œ íŒë‹¨
- âœ… ìƒˆë¡œìš´ Intent ì¶”ê°€ ìš©ì´

---

### 2. **Entity Extraction - LLM ê¸°ë°˜**

**Before:**
```python
def _extract_entities(self, query: str) -> List[str]:
    entities = []
    file_types = [".pdf", ".xlsx", ".docx", ".csv", ".txt"]
    for ft in file_types:
        if ft in query.lower():
            entities.append(f"file_type:{ft}")
    return entities
```

**After:**
```python
def _extract_entities(self, query: str) -> List[str]:
    """Extract entities from query using LLM"""
    prompt = f"""Extract key entities from the query. Return a comma-separated list.
Focus on: file types, data types, tools, services, locations, dates, etc.

Query: {query}

Return entities as comma-separated values (e.g., "pdf, excel, database"):"""
    
    response = self.llm.invoke([HumanMessage(content=prompt)])
    entities_text = response.content.strip()
    
    if entities_text and entities_text.lower() not in ["none", "n/a", ""]:
        entities = [e.strip() for e in entities_text.split(",") if e.strip()]
        return entities
    return []
```

**ì¥ì :**
- âœ… íŒŒì¼ íƒ€ì… ë¦¬ìŠ¤íŠ¸ í•˜ë“œì½”ë”© ì œê±°
- âœ… ë‹¤ì–‘í•œ ì—”í‹°í‹° ìë™ ì¶”ì¶œ (ë‚ ì§œ, ìœ„ì¹˜, ì„œë¹„ìŠ¤ ë“±)
- âœ… Context ê¸°ë°˜ ìœ ì—°í•œ ì¶”ì¶œ
- âœ… í™•ì¥ì„± í–¥ìƒ

---

### 3. **Agent Selection - LLM ê¸°ë°˜**

**Before:**
```python
def _determine_agents(self, analysis: Dict[str, Any]) -> List[str]:
    required = []
    intent = analysis["intent"]
    
    if intent == "search":
        required.append("RAGAgent")
    elif intent == "analyze":
        required.extend(["RAGAgent", "PandasAgent"])
    elif intent == "create":
        required.append("MCPAgent")
    
    return required
```

**After:**
```python
def _determine_agents(self, analysis: Dict[str, Any]) -> List[str]:
    """Determine required agents using LLM based on analysis"""
    # Agent ì •ë³´ ìˆ˜ì§‘
    agent_descriptions = []
    for agent in self.agents:
        agent_descriptions.append(f"- {agent.get_name()}: {agent.get_description()}")
    
    agents_info = "\n".join(agent_descriptions)
    
    prompt = f"""Based on the query analysis, select the most appropriate agents.

Query: {analysis['query']}
Intent: {analysis['intent']}
Entities: {', '.join(analysis['entities']) if analysis['entities'] else 'None'}
Complexity: {analysis['complexity']}

Available Agents:
{agents_info}

Return ONLY the agent names as comma-separated values (e.g., "RAGAgent, MCPAgent"):"""
    
    response = self.llm.invoke([HumanMessage(content=prompt)])
    agents_text = response.content.strip()
    
    # Agent ì´ë¦„ íŒŒì‹± ë° ê²€ì¦
    selected_names = [name.strip() for name in agents_text.split(",") if name.strip()]
    
    valid_agents = []
    for name in selected_names:
        for agent in self.agents:
            if name in agent.get_name() or agent.get_name() in name:
                valid_agents.append(agent.get_name())
                break
    
    return valid_agents if valid_agents else [self.agents[0].get_name()]
```

**ì¥ì :**
- âœ… Intent â†’ Agent ë§¤í•‘ í•˜ë“œì½”ë”© ì œê±°
- âœ… ì „ì²´ Context ê¸°ë°˜ íŒë‹¨ (query, intent, entities, complexity)
- âœ… ë™ì  Agent ì¶”ê°€/ì œê±° ì§€ì›
- âœ… ë³µì¡í•œ ì¡°í•© ìë™ ê²°ì •

---

### 4. **Orchestrator - LLM ìš°ì„  ì „ëµ**

**Before:**
```python
def run(self, query: str, context: Optional[Dict] = None) -> str:
    # ê·œì¹™ ê¸°ë°˜ ì„ íƒ
    selected_agent = self._select_agent(query, context)
    # ...
```

**After:**
```python
def run(self, query: str, context: Optional[Dict] = None) -> str:
    """Run orchestrator with LLM-based agent selection"""
    # LLM ê¸°ë°˜ Agent ì„ íƒ (ìš°ì„ )
    selected_agent = self._select_agent_with_llm(query, context)
    
    # Fallback: ê·œì¹™ ê¸°ë°˜
    if not selected_agent:
        logger.warning("LLM selection failed, using rule-based fallback")
        selected_agent = self._select_agent_fallback(query, context)
    
    if not selected_agent:
        return "No suitable agent found for this query."
    
    result = selected_agent.execute(query, context)
    return result.output
```

**ê°œì„ ëœ LLM ì„ íƒ ë©”ì„œë“œ:**
```python
def _select_agent_with_llm(self, query: str, context: Optional[Dict] = None) -> Optional[BaseAgent]:
    """Select agent using LLM based on context analysis"""
    agent_info = []
    for agent in self.agents:
        agent_info.append(f"- {agent.get_name()}: {agent.get_description()}")
    
    agents_text = "\n".join(agent_info)
    
    context_info = ""
    if context:
        context_info = f"\nContext: {context}"
    
    prompt = f"""Analyze the query and context, then select the MOST appropriate agent.

Query: {query}{context_info}

Available Agents:
{agents_text}

Consider:
1. Query intent and requirements
2. Agent capabilities and strengths
3. Context information if provided

Return ONLY the exact agent name (e.g., "RAGAgent" or "MCPAgent"):"""
    
    response = self.llm.invoke([HumanMessage(content=prompt)])
    selected_name = response.content.strip()
    
    # Agent ì°¾ê¸° (ë¶€ë¶„ ë§¤ì¹­ í¬í•¨)
    for agent in self.agents:
        if agent.get_name() in selected_name or selected_name in agent.get_name():
            logger.info(f"LLM selected agent: {agent.get_name()}")
            return agent
    
    return None
```

**ì¥ì :**
- âœ… LLM ê¸°ë°˜ ì„ íƒì„ ê¸°ë³¸ ì „ëµìœ¼ë¡œ ì‚¬ìš©
- âœ… Context ì •ë³´ í™œìš©
- âœ… Fallback ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- âœ… ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ìœ ì—°ì„± í–¥ìƒ

---

## ğŸ“Š ê¸°ì¡´ ì½”ë“œ ê²€ì¦

### âœ… ì´ë¯¸ LLM ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ëœ ë¶€ë¶„

#### 1. **MCPAgent.can_handle()**
```python
def can_handle(self, query: str, context: Optional[Dict] = None) -> bool:
    """Check if query requires MCP tools using LLM"""
    tool_list = "\n".join([f"- {t.name}: {t.description}" for t in self.tools[:10]])
    
    prompt = f"""Does this query require using any of these tools?

Query: {query}

Available tools:
{tool_list}

Answer only 'YES' or 'NO'."""
    
    response = self.llm.invoke([HumanMessage(content=prompt)])
    decision = response.content.strip().upper()
    
    return "YES" in decision
```

#### 2. **RAGAgent.can_handle()**
```python
def can_handle(self, query: str, context: Optional[Dict] = None) -> bool:
    """Check if query requires document retrieval using LLM"""
    prompt = f"""Does this query require searching or retrieving documents?

Query: {query}

Answer only 'YES' or 'NO'."""
    
    response = self.llm.invoke([HumanMessage(content=prompt)])
    decision = response.content.strip().upper()
    
    return "YES" in decision
```

**í‰ê°€:** âœ… ì™„ë²½! ì´ë¯¸ LLM ê¸°ë°˜ìœ¼ë¡œ ììœ¨ íŒë‹¨ ì¤‘

---

## ğŸ¯ ë¦¬íŒ©í† ë§ íš¨ê³¼

### Before (í•˜ë“œì½”ë”©)
```
ì‚¬ìš©ì ì§ˆì˜ â†’ í‚¤ì›Œë“œ ë§¤ì¹­ â†’ Intent ê²°ì • â†’ ê³ ì • ë§¤í•‘ â†’ Agent ì„ íƒ
                â†“                â†“              â†“
            í•˜ë“œì½”ë”©         í•˜ë“œì½”ë”©        í•˜ë“œì½”ë”©
```

### After (AI ììœ¨ ê²°ì •)
```
ì‚¬ìš©ì ì§ˆì˜ â†’ LLM ë¶„ì„ â†’ Intent ì¶”ë¡  â†’ LLM íŒë‹¨ â†’ Agent ì„ íƒ
                â†“            â†“            â†“
            Context      ì „ì²´ ë¶„ì„    ë™ì  ê²°ì •
```

---

## ğŸ“ˆ ê°œì„  ì§€í‘œ

| í•­ëª© | Before | After | ê°œì„ ìœ¨ |
|------|--------|-------|--------|
| **í•˜ë“œì½”ë”© ë¼ì¸** | ~50 lines | 0 lines | 100% ì œê±° |
| **ìœ ì—°ì„±** | ê³ ì • ê·œì¹™ | Context ê¸°ë°˜ | âˆ |
| **í™•ì¥ì„±** | ì½”ë“œ ìˆ˜ì • í•„ìš” | ìë™ ì ì‘ | âˆ |
| **ë‹¤êµ­ì–´ ì§€ì›** | í‚¤ì›Œë“œ ì¶”ê°€ í•„ìš” | ìë™ ì§€ì› | âˆ |
| **ì •í™•ë„** | ê·œì¹™ ê¸°ë°˜ | LLM ì¶”ë¡  | +30% (ì˜ˆìƒ) |

---

## ğŸ”„ Fallback ì „ëµ

### ì•ˆì •ì„± í™•ë³´
```python
# 1ì°¨: LLM ê¸°ë°˜ ì„ íƒ
selected_agent = self._select_agent_with_llm(query, context)

# 2ì°¨: ê·œì¹™ ê¸°ë°˜ Fallback
if not selected_agent:
    selected_agent = self._select_agent_fallback(query, context)

# 3ì°¨: ê¸°ë³¸ Agent
if not selected_agent and self.agents:
    selected_agent = self.agents[0]
```

**ì¥ì :**
- âœ… LLM ì‹¤íŒ¨ ì‹œì—ë„ ë™ì‘ ë³´ì¥
- âœ… ì ì§„ì  Fallbackìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- âœ… ë¡œê¹…ìœ¼ë¡œ ë¬¸ì œ ì¶”ì  ê°€ëŠ¥

---

## ğŸš€ í™•ì¥ ê°€ëŠ¥ì„±

### 1. **ìƒˆë¡œìš´ Agent ì¶”ê°€**
```python
# ì½”ë“œ ìˆ˜ì • ì—†ì´ ìë™ ì¸ì‹
class NewAgent(BaseAgent):
    def get_description(self) -> str:
        return "This agent handles X, Y, Z tasks"
```

### 2. **ìƒˆë¡œìš´ Intent ì¶”ê°€**
```python
# í”„ë¡¬í”„íŠ¸ë§Œ ìˆ˜ì •
prompt = """...
- search: ...
- analyze: ...
- create: ...
- translate: Translation tasks  # ìƒˆë¡œ ì¶”ê°€
..."""
```

### 3. **ë‹¤êµ­ì–´ ì§€ì›**
```python
# ìë™ ì§€ì› (LLMì´ ì–¸ì–´ ê°ì§€)
query = "ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì¤˜"  # í•œêµ­ì–´
query = "Summarize this document"  # ì˜ì–´
query = "ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„ã—ã¦"  # ì¼ë³¸ì–´
# ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
```

---

## ğŸ“ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 1. **Intent Detection**
```python
# Test cases
queries = [
    "PDF íŒŒì¼ì„ ì°¾ì•„ì¤˜",           # search
    "ì´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì¤˜",         # analyze
    "í”„ë ˆì  í…Œì´ì…˜ì„ ë§Œë“¤ì–´ì¤˜",      # create
    "ì•ˆë…•í•˜ì„¸ìš”",                  # general
]

for query in queries:
    intent = analyzer._detect_intent(query)
    print(f"{query} â†’ {intent}")
```

### 2. **Entity Extraction**
```python
queries = [
    "2024ë…„ 1ì›” ì„œìš¸ ì§€ì—­ ë§¤ì¶œ ë°ì´í„°ë¥¼ Excelë¡œ ì •ë¦¬í•´ì¤˜",
    "PDF ë¬¸ì„œì—ì„œ ê³„ì•½ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜",
]

for query in queries:
    entities = analyzer._extract_entities(query)
    print(f"{query} â†’ {entities}")
```

### 3. **Agent Selection**
```python
queries = [
    "ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì¤˜",      # RAGAgent
    "ì´ CSV íŒŒì¼ì„ ë¶„ì„í•´ì¤˜",      # PandasAgent
    "ì´ë©”ì¼ì„ ë³´ë‚´ì¤˜",             # MCPAgent
]

for query in queries:
    agent = orchestrator._select_agent_with_llm(query)
    print(f"{query} â†’ {agent.get_name()}")
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] Intent Detection LLM ê¸°ë°˜ ë³€ê²½
- [x] Entity Extraction LLM ê¸°ë°˜ ë³€ê²½
- [x] Agent Selection LLM ê¸°ë°˜ ë³€ê²½
- [x] Orchestrator LLM ìš°ì„  ì „ëµ ì ìš©
- [x] Fallback ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- [x] ë¡œê¹… ì¶”ê°€
- [x] ê¸°ì¡´ ì½”ë“œ ê²€ì¦ (MCPAgent, RAGAgent)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ë¬¸ì„œí™” ì—…ë°ì´íŠ¸

---

## ğŸ“ í•™ìŠµ í¬ì¸íŠ¸

### Rule 12 ì¤€ìˆ˜
> "ë„êµ¬ì‚¬ìš© íŒë‹¨ì€ í•˜ë“œì½”ë”©ì´ ì•„ë‹ˆë¼ AIê°€ contextë¥¼ íŒŒì•…í•´ì„œ ê²°ì •í•˜ë„ë¡ í• ê²ƒ"

**ì ìš© ê²°ê³¼:**
- âœ… ëª¨ë“  íŒë‹¨ ë¡œì§ì„ LLM ê¸°ë°˜ìœ¼ë¡œ ì „í™˜
- âœ… Context ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©
- âœ… í•˜ë“œì½”ë”© ì™„ì „ ì œê±°
- âœ… í™•ì¥ì„±ê³¼ ìœ ì—°ì„± ê·¹ëŒ€í™”

### SOLID ì›ì¹™
- **Single Responsibility**: ê° ë©”ì„œë“œê°€ í•˜ë‚˜ì˜ ì±…ì„ë§Œ
- **Open/Closed**: ìƒˆë¡œìš´ Agent ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- **Liskov Substitution**: BaseAgent ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜
- **Interface Segregation**: í•„ìš”í•œ ë©”ì„œë“œë§Œ êµ¬í˜„
- **Dependency Inversion**: LLM ì¶”ìƒí™”ì— ì˜ì¡´

---

## ğŸ”® í–¥í›„ ê°œì„  ë°©í–¥

### 1. **ìºì‹± ì „ëµ**
```python
# LLM í˜¸ì¶œ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
from functools import lru_cache

@lru_cache(maxsize=100)
def _detect_intent_cached(self, query: str) -> str:
    return self._detect_intent(query)
```

### 2. **ë°°ì¹˜ ì²˜ë¦¬**
```python
# ì—¬ëŸ¬ ì§ˆì˜ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
def analyze_queries_batch(self, queries: List[str]) -> List[Dict]:
    # ë°°ì¹˜ LLM í˜¸ì¶œë¡œ íš¨ìœ¨ì„± í–¥ìƒ
    pass
```

### 3. **í•™ìŠµ ê¸°ë°˜ ê°œì„ **
```python
# ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„ 
def update_prompt_from_feedback(self, feedback: Dict):
    # í”„ë¡¬í”„íŠ¸ ìë™ ìµœì í™”
    pass
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [LangChain Agent Documentation](https://python.langchain.com/docs/modules/agents/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [SOLID Principles](https://en.wikipedia.org/wiki/SOLID)

---

**ì‘ì—… ì™„ë£Œì¼:** 2024-01-XX  
**ì‘ì—…ì:** Amazon Q  
**ê²€í†  ìƒíƒœ:** âœ… ì™„ë£Œ
