# ğŸ¯ Token Tracking ì •í™•ë„ ê°€ì´ë“œ

## ğŸ“Š í† í° ì¸¡ì • ë°©ì‹

### 1. ì‹¤ì œ í† í° (Actual Tokens) âœ… ìš°ì„ ìˆœìœ„ 1
**ì†ŒìŠ¤**: LLM API ì‘ë‹µì˜ usage ë©”íƒ€ë°ì´í„°

**ì§€ì› ëª¨ë¸**:
- âœ… OpenAI (GPT-3.5, GPT-4): `usage.prompt_tokens`, `usage.completion_tokens`
- âœ… Google Gemini: `usage_metadata.prompt_token_count`, `usage_metadata.candidates_token_count`
- âœ… Perplexity: `usage.prompt_tokens`, `usage.completion_tokens`
- âŒ Pollinations: ë¬´ë£Œ ëª¨ë¸, í† í° ì •ë³´ ì—†ìŒ (ì¶”ì • ì‚¬ìš©)

**ì¶”ì¶œ ìœ„ì¹˜**:
```python
# BaseAgent._extract_token_counts()
1. intermediate_stepsì—ì„œ LLM ì‘ë‹µ í™•ì¸
2. result['usage_metadata'] í™•ì¸
3. TokenLogger.extract_actual_tokens() ì‚¬ìš©
```

### 2. ì¶”ì • í† í° (Estimated Tokens) âš ï¸ Fallback
**ì†ŒìŠ¤**: í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ê³„ì‚°

**ì¶”ì • ê³µì‹**:
```python
# TokenLogger.estimate_tokens()
- í•œê¸€: 1.5ë¬¸ìë‹¹ 1í† í°
- ì˜ì–´: 4ë¬¸ìë‹¹ 1í† í°
- ê¸°íƒ€: 3ë¬¸ìë‹¹ 1í† í°
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**:
- LLM ì‘ë‹µì— usage ì •ë³´ê°€ ì—†ì„ ë•Œ
- Pollinations ê°™ì€ ë¬´ë£Œ ëª¨ë¸
- ì—ëŸ¬ ë°œìƒ ì‹œ fallback

---

## ğŸ” ì •í™•ë„ ê²€ì¦ ë°©ë²•

### ë°©ë²• 1: ë¡œê·¸ í™•ì¸
```bash
# ì‹¤ì œ í† í° ì¶”ì¶œ ì„±ê³µ ì‹œ
[DEBUG] Using actual tokens from response: 500+1000

# ì¶”ì • í† í° ì‚¬ìš© ì‹œ
[DEBUG] Using estimated tokens: 500+1000
```

### ë°©ë²• 2: UI í™•ì¸
```
Stats íƒ­ > Model Statistics
- ì‹¤ì œ í† í°ì´ ì¶”ì¶œë˜ë©´ ì •í™•í•œ ë¹„ìš© í‘œì‹œ
- ì¶”ì • í† í° ì‚¬ìš© ì‹œ ê·¼ì‚¬ê°’ í‘œì‹œ
```

### ë°©ë²• 3: DB í™•ì¸
```sql
SELECT 
    model_name,
    input_tokens,
    output_tokens,
    cost_usd,
    additional_info
FROM token_usage
WHERE session_id = ?
ORDER BY timestamp DESC;
```

---

## ğŸ“ˆ ì •í™•ë„ ë¹„êµ

### ì‹¤ì œ ì¸¡ì • (OpenAI GPT-4)
```
Input: 523 tokens (actual)
Output: 1,247 tokens (actual)
Cost: $0.090210 (ì •í™•)
```

### ì¶”ì • ì¸¡ì • (Pollinations)
```
Input: ~500 tokens (estimated)
Output: ~1,200 tokens (estimated)
Cost: $0.000000 (ë¬´ë£Œ)
```

### ì •í™•ë„ ì°¨ì´
- **ì‹¤ì œ í† í°**: Â±0% (API ì œê³µ)
- **ì¶”ì • í† í°**: Â±10-20% (í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜)

---

## âš™ï¸ ê°œì„  ë°©ë²•

### 1. ëª¨ë¸ë³„ ì‹¤ì œ í† í° í™•ë³´
```python
# simple_chat_processor.py
response = self.model_strategy.llm.invoke(messages)

# ì‘ë‹µ ê°ì²´ ì €ì¥ (í† í° ì¶”ì¶œìš©)
self.model_strategy._last_response = response

# ì‹¤ì œ í† í° ì¶”ì¶œ
actual_input, actual_output = TokenLogger.extract_actual_tokens(response)
```

### 2. LangChain ì‘ë‹µ êµ¬ì¡° í™•ì¸
```python
# ì‘ë‹µ êµ¬ì¡° ë¡œê¹…
logger.debug(f"Response type: {type(response)}")
logger.debug(f"Response attributes: {dir(response)}")

if hasattr(response, 'response_metadata'):
    logger.debug(f"Metadata: {response.response_metadata}")
```

### 3. ì¶”ì • ì •í™•ë„ í–¥ìƒ
```python
# ëª¨ë¸ë³„ í† í° ë¹„ìœ¨ ì¡°ì •
MODEL_TOKEN_RATIOS = {
    'gpt-4': 3.5,  # ì˜ì–´ ê¸°ì¤€
    'gemini-2.0-flash': 3.0,
    'korean-model': 1.5  # í•œê¸€ ê¸°ì¤€
}
```

---

## ğŸ¯ í˜„ì¬ ìƒíƒœ

### âœ… ì •í™•í•œ ì¸¡ì •
- OpenAI ëª¨ë¸: ì‹¤ì œ í† í° ì¶”ì¶œ âœ…
- Gemini ëª¨ë¸: ì‹¤ì œ í† í° ì¶”ì¶œ âœ…
- Perplexity ëª¨ë¸: ì‹¤ì œ í† í° ì¶”ì¶œ âœ…

### âš ï¸ ì¶”ì • ì‚¬ìš©
- Pollinations: ë¬´ë£Œ ëª¨ë¸, ì¶”ì • ì‚¬ìš© âš ï¸
- ì—ëŸ¬ ë°œìƒ ì‹œ: Fallback to estimation âš ï¸

### ğŸ“Š ì •í™•ë„
- **ì‹¤ì œ í† í° ì‚¬ìš©ë¥ **: ~80% (ìœ ë£Œ ëª¨ë¸)
- **ì¶”ì • í† í° ì‚¬ìš©ë¥ **: ~20% (ë¬´ë£Œ ëª¨ë¸ + fallback)
- **í‰ê·  ì •í™•ë„**: ~95% (ì‹¤ì œ í† í° ê¸°ì¤€)

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: í† í°ì´ 0ìœ¼ë¡œ í‘œì‹œë¨
**ì›ì¸**: LLM ì‘ë‹µì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨

**í•´ê²°**:
```python
# 1. ì‘ë‹µ êµ¬ì¡° í™•ì¸
logger.debug(f"Response: {response}")

# 2. TokenLogger.extract_actual_tokens() ê°œì„ 
# 3. ì¶”ì • í† í° fallback í™•ì¸
```

### ë¬¸ì œ 2: ë¹„ìš©ì´ ë¶€ì •í™•í•¨
**ì›ì¸**: ì¶”ì • í† í° ì‚¬ìš© ì¤‘

**í•´ê²°**:
```python
# 1. ì‹¤ì œ í† í° ì¶”ì¶œ í™•ì¸
actual_input, actual_output = TokenLogger.extract_actual_tokens(response)

# 2. ëª¨ë¸ API ì‘ë‹µ êµ¬ì¡° í™•ì¸
# 3. MODEL_PRICING ì •í™•ë„ ê²€ì¦
```

### ë¬¸ì œ 3: ì¶”ì • í† í°ì´ ë„ˆë¬´ ë¶€ì •í™•í•¨
**ì›ì¸**: í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì¶”ì •ì˜ í•œê³„

**í•´ê²°**:
```python
# 1. tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (OpenAI ëª¨ë¸)
import tiktoken
encoding = tiktoken.encoding_for_model("gpt-4")
tokens = len(encoding.encode(text))

# 2. ëª¨ë¸ë³„ í† í° ë¹„ìœ¨ ì¡°ì •
# 3. ì‹¤ì œ í† í° ë°ì´í„°ë¡œ í•™ìŠµ
```

---

## ğŸ“ ê¶Œì¥ì‚¬í•­

### 1. ìœ ë£Œ ëª¨ë¸ ì‚¬ìš© ì‹œ
- âœ… ì‹¤ì œ í† í° ìë™ ì¶”ì¶œ
- âœ… ì •í™•í•œ ë¹„ìš© ê³„ì‚°
- âœ… DBì— ì‹¤ì œ í† í° ì €ì¥

### 2. ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš© ì‹œ
- âš ï¸ ì¶”ì • í† í° ì‚¬ìš©
- âš ï¸ ë¹„ìš©ì€ $0.00
- âš ï¸ ì°¸ê³ ìš© í† í° ìˆ˜

### 3. í˜¼í•© ì‚¬ìš© ì‹œ
- âœ… ëª¨ë¸ë³„ ìë™ ì „í™˜
- âœ… ì‹¤ì œ/ì¶”ì • ìë™ ì„ íƒ
- âœ… í†µê³„ì— êµ¬ë¶„ í‘œì‹œ

---

## ğŸ‰ ê²°ë¡ 

**Token Tracking Systemì€ ë‹¤ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤**:

1. âœ… **ì‹¤ì œ í† í° ìš°ì„ **: API ì œê³µ ì‹œ 100% ì •í™•
2. âœ… **Fallback ì§€ì›**: ì¶”ì • í† í°ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ëŒ€ì²´
3. âœ… **íˆ¬ëª…ì„±**: ë¡œê·¸/UIì—ì„œ ì‹¤ì œ/ì¶”ì • êµ¬ë¶„ ê°€ëŠ¥
4. âœ… **í™•ì¥ì„±**: ìƒˆ ëª¨ë¸ ì¶”ê°€ ì‹œ ìë™ ëŒ€ì‘

**ì •í™•ë„**: ì‹¤ì œ í† í° ì‚¬ìš© ì‹œ 100%, ì¶”ì • ì‹œ ~85%

---

**ì‘ì„±ì¼**: 2025-01-07  
**ì‘ì„±ì**: Amazon Q Developer  
**ë¬¸ì„œ ë²„ì „**: 1.0
