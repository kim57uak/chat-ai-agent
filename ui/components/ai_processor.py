from PyQt6.QtCore import QObject, pyqtSignal
import threading
import time
from concurrent.futures import ThreadPoolExecutor, Future
from ui.components.status_display import status_display
from core.token_logger import TokenLogger
from core.simple_token_accumulator import token_accumulator
from core.logging import get_logger

logger = get_logger('ui.ai_processor')


class AIProcessor(QObject):
    """AI ìš”ì²­ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ (ìŠ¤ë ˆë“œ í’€ ìµœì í™”)"""
    
    finished = pyqtSignal(str, str, list)  # sender, text, used_tools
    error = pyqtSignal(str)
    streaming = pyqtSignal(str, str)  # sender, partial_text
    streaming_complete = pyqtSignal(str, str, list)  # sender, full_text, used_tools
    conversation_completed = pyqtSignal(object)  # ConversationTokens ê°ì²´
    
    def __init__(self, parent=None, max_workers=3):
        super().__init__(parent)
        self._cancelled = False
        self._current_client = None
        self._executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="AIProcessor")
        self._current_future = None
        self._lock = threading.Lock()
    
    def cancel(self):
        """ìš”ì²­ ì·¨ì†Œ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._lock:
            self._cancelled = True
            if self._current_future and not self._current_future.done():
                self._current_future.cancel()
            if self._current_client:
                self._current_client.cancel_streaming()
        status_display.finish_processing(False)
    
    def shutdown(self):
        """ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ (ë¦¬ì†ŒìŠ¤ ì •ë¦¬)"""
        self._executor.shutdown(wait=False)
        logger.info("AIProcessor ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ")
    
    def process_request(self, api_key, model, messages, user_text=None, agent_mode=False, file_prompt=None, chat_mode="simple", session_id=None):
        """AI ìš”ì²­ ì²˜ë¦¬ - ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨"""
        def _process():
            request_start_time = time.time()
            request_id = None
            try:
                if self._cancelled:
                    return
                
                # ìƒíƒœ í‘œì‹œ ì‹œì‘
                mode = 'agent' if agent_mode else 'ask'
                status_display.start_processing(model, mode)
                
                # AI ìš”ì²­ ë¡œê¹…
                available_tools = []
                try:
                    from mcp.client.mcp_client import mcp_manager
                    all_tools = mcp_manager.get_all_tools()
                    available_tools = [f"{server}.{tool['name']}" for server, tools in all_tools.items() for tool in tools]
                except:
                    pass
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“œë³„ ì°¨ë“± ì ìš©)
                from ui.prompts import prompt_manager
                provider = prompt_manager.get_provider_from_model(model)
                
                # ëª¨ë“œì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
                system_prompt = prompt_manager.get_system_prompt(provider, use_tools=agent_mode)
                
                # ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œ ë¡œê¹… (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ)
                if agent_mode:
                    # Agent ëª¨ë“œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    prompt_components = [
                        ('system_base', prompt_manager.get_prompt('common', 'system_base')),
                        ('context_awareness', prompt_manager.get_prompt('common', 'context_awareness')),
                        ('response_tone', prompt_manager.get_prompt('common', 'response_tone')),
                        ('emoji_usage', prompt_manager.get_prompt('common', 'emoji_usage')),
                        ('tool_usage', prompt_manager.get_prompt('common', 'tool_usage')),
                        ('formatting', prompt_manager.get_prompt('common', 'formatting')),
                        ('code_rules', prompt_manager.get_prompt('common', 'code_rules')),
                        ('mermaid_rules', prompt_manager.get_prompt('common', 'mermaid_rules')),
                        ('agent_base', prompt_manager.get_prompt('common', 'agent_base')),
                        ('react_format', prompt_manager.get_prompt('common', 'react_format')),
                        ('json_format', prompt_manager.get_prompt('common', 'json_format')),
                        ('execution_rules', prompt_manager.get_prompt('common', 'execution_rules')),
                        # ëª¨ë¸ë³„ íŠ¹ìˆ˜ í”„ë¡¬í”„íŠ¸
                        ('model_enhancement', prompt_manager.get_custom_prompt(provider, 'system_enhancement') or ''),
                        ('agent_system', prompt_manager.get_custom_prompt(provider, 'agent_system') or ''),
                        ('react_template', prompt_manager.get_custom_prompt(provider, 'react_template') or ''),
                        ('tool_decision', prompt_manager.get_custom_prompt(provider, 'tool_decision') or ''),
                        ('image_generation', prompt_manager.get_custom_prompt(provider, 'image_generation') or '')
                    ]
                else:
                    # Ask ëª¨ë“œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    prompt_components = [
                        ('system_base', prompt_manager.get_prompt('common', 'system_base')),
                        ('context_awareness', prompt_manager.get_prompt('common', 'context_awareness')),
                        ('response_tone', prompt_manager.get_prompt('common', 'response_tone')),
                        ('emoji_usage', prompt_manager.get_prompt('common', 'emoji_usage')),
                        ('formatting', prompt_manager.get_prompt('common', 'formatting')),
                        ('mermaid_rules', prompt_manager.get_prompt('common', 'mermaid_rules')),
                        ('ask_mode', prompt_manager.get_prompt('common', 'ask_mode')),
                        # ëª¨ë¸ë³„ íŠ¹ìˆ˜ í”„ë¡¬í”„íŠ¸
                        ('model_enhancement', prompt_manager.get_custom_prompt(provider, 'system_enhancement') or '')
                    ]
                
                logger.debug(f"Prompt components - Model: {model}, Mode: {'Agent' if agent_mode else 'Ask'}")
                for key, value in prompt_components:
                    if value:
                        logger.debug(f"{key}: {value[:100]}...")
                
                # ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ì…ë ¥ ê²°ì •
                input_for_detection = (file_prompt or "") + (user_text or "")
                
                # ì–¸ì–´ ê°ì§€ ë° ê°•ì œ ì–¸ì–´ ì§€ì‹œ ì¶”ê°€
                processed_file_prompt = file_prompt
                processed_user_text = user_text
                
                if input_for_detection.strip():
                    korean_ratio = self._detect_korean_ratio(input_for_detection)
                    korean_threshold = self._get_korean_threshold()
                    
                    if korean_ratio >= korean_threshold:
                        language_instruction = "\n\n[CRITICAL: ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. Answer only in Korean.]"
                    else:
                        language_instruction = "\n\n[CRITICAL: Please answer only in English. ì˜ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.]"
                    
                    # ì‚¬ìš©ì ì…ë ¥ì— ì–¸ì–´ ì§€ì‹œ ì¶”ê°€
                    if processed_file_prompt:
                        processed_file_prompt = processed_file_prompt + language_instruction
                    if processed_user_text:
                        processed_user_text = processed_user_text + language_instruction
                    
                    logger.debug(f"Language detection - Korean ratio: {korean_ratio:.3f}, Threshold: {korean_threshold}, Language: {'Korean' if korean_ratio >= korean_threshold else 'English'}")
                
                # ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥ ê²°ì •
                actual_user_input = processed_file_prompt or processed_user_text or ""
                
                # request_id = ai_logger.log_request(
                #     model=model,
                #     system_prompt=system_prompt,
                #     user_input=actual_user_input,
                #     conversation_history=messages,
                #     tools_available=available_tools if agent_mode else [],
                #     agent_mode=agent_mode
                # )
                request_id = None
                
                from core.ai_client import AIClient
                client = AIClient(api_key, model)
                
                # session_id ì„¤ì •
                if session_id is not None:
                    client.set_session_id(session_id)
                
                self._current_client = client
                self._current_model = model
                
                # RAG ëª¨ë“œì¼ ê²½ìš° RAG Manager ì´ˆê¸°í™” ë° Agent ëª¨ë“œ ì„¤ì •
                if chat_mode == "rag":
                    from core.rag.rag_manager import RAGManager
                    if not hasattr(client, 'rag_manager'):
                        client.rag_manager = RAGManager()
                    
                    # Agentì— RAG ëª¨ë“œ ì„¤ì •
                    if hasattr(client, 'agent') and hasattr(client.agent, 'set_chat_mode'):
                        client.agent.set_chat_mode("rag")
                        client.agent.set_vectorstore(client.rag_manager.vectorstore)
                        logger.info("RAG mode activated and configured")
                    else:
                        logger.warning("Agent does not support RAG mode configuration")
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì„¤ì •
                if messages:
                    client.conversation_history = messages
                    if hasattr(client, '_conversation_manager'):
                        client._conversation_manager.conversation_history = messages
                
                response = None
                sender = 'AI'
                used_tools = []
                
                if processed_file_prompt:
                    # íŒŒì¼ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
                    if agent_mode:
                        result = client.agent_chat(processed_file_prompt)
                        if isinstance(result, tuple):
                            response, used_tools = result
                        else:
                            response = result
                            used_tools = []
                        sender = 'ì—ì´ì „íŠ¸'
                    else:
                        response = client.simple_chat(processed_file_prompt)
                        sender = 'AI'
                        used_tools = []
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    if chat_mode == "rag":
                        # RAG ëª¨ë“œ: RAG + Multi-Agent
                        logger.info(f"RAG mode processing: {processed_user_text[:50]}")
                        
                        # RAG Manager ê°€ì ¸ì˜¤ê¸°
                        from core.rag.rag_manager import RAGManager
                        if not hasattr(client, 'rag_manager'):
                            client.rag_manager = RAGManager()
                        
                        # RAG ê²€ìƒ‰ ë¨¼ì € ìˆ˜í–‰
                        # ì„¤ì •ì—ì„œ top_k ê°€ì ¸ì˜¤ê¸°
                        try:
                            from utils.config_path import config_path_manager
                            import json
                            config_path = config_path_manager.get_config_path('rag_config.json')
                            if config_path.exists():
                                with open(config_path, 'r', encoding='utf-8') as f:
                                    rag_config = json.load(f)
                                    top_k = rag_config.get('top_k', 5)
                            else:
                                top_k = 5
                        except:
                            top_k = 5
                        
                        rag_results = client.rag_manager.search(processed_user_text, k=top_k)
                        
                        if rag_results:
                            # RAG ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                            context = "\n\n[ê´€ë ¨ ë¬¸ì„œ]\n"
                            for i, doc in enumerate(rag_results, 1):
                                context += f"{i}. {doc.page_content[:200]}...\n\n"
                            
                            enhanced_text = f"{context}\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{processed_user_text}"
                            logger.info(f"RAG: Found {len(rag_results)} documents")
                        else:
                            enhanced_text = processed_user_text
                            logger.info("RAG: No documents found")
                        
                        # RAG ëª¨ë“œ: ë¬¸ì„œ ê²€ìƒ‰ + ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥
                        if messages:
                            full_messages = messages + [{'role': 'user', 'content': enhanced_text}]
                            result = client.chat(full_messages, force_agent=True)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        else:
                            result = client.agent_chat(enhanced_text)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        sender = 'RAG+Agent'
                    elif agent_mode:
                        # ì—ì´ì „íŠ¸ ëª¨ë“œ: ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥
                        if messages:
                            full_messages = messages + [{'role': 'user', 'content': processed_user_text}]
                            result = client.chat(full_messages, force_agent=True)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        else:
                            result = client.agent_chat(processed_user_text)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        sender = 'ì—ì´ì „íŠ¸'
                    else:
                        # Ask ëª¨ë“œ: ë„êµ¬ ì‚¬ìš© ì—†ì´ ë‹¨ìˆœ ì±„íŒ…ë§Œ
                        if messages:
                            # Ask ëª¨ë“œ: force_agent=Falseë¡œ ëª…ì‹œì  ì „ë‹¬
                            full_messages = messages + [{'role': 'user', 'content': processed_user_text}]
                            result = client.chat(full_messages, force_agent=False)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        else:
                            response = client.simple_chat(processed_user_text)
                            used_tools = []
                        sender = 'AI'
                        
                        # Ask ëª¨ë“œì—ì„œëŠ” ë„êµ¬ ì‚¬ìš© ë¶ˆê°€ ë©”ì‹œì§€ ì œê±° (AIê°€ ì»¨í…ìŠ¤íŠ¸ íŒŒì•…í•´ì„œ íŒë‹¨)
                
                if not self._cancelled and response:
                    # í† í° ì¶”ì ì€ chat_processorì—ì„œ ì´ë¯¸ ì™„ë£Œë¨ (ì¤‘ë³µ ë°©ì§€)
                    # ì—¬ê¸°ì„œëŠ” UI ì—…ë°ì´íŠ¸ë§Œ ìˆ˜í–‰
                    from core.token_tracker import token_tracker
                    
                    # AI ì‘ë‹µ ë¡œê¹…
                    response_time = time.time() - request_start_time
                    
                    # í† í° ì •ë³´ëŠ” trackerì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    tracker_stats = token_tracker.get_conversation_stats()
                    if tracker_stats:
                        actual_input_tokens = tracker_stats.get('total_actual_input', 0) or tracker_stats.get('total_estimated_input', 0)
                        actual_output_tokens = tracker_stats.get('total_actual_output', 0) or tracker_stats.get('total_estimated_output', 0)
                    else:
                        actual_input_tokens = 0
                        actual_output_tokens = 0
                    
                    # token_accumulatorì— í† í° ì¶”ê°€ (ì±„íŒ… í•˜ë‹¨ í‘œì‹œìš©)
                    if actual_input_tokens > 0 or actual_output_tokens > 0:
                        token_accumulator.add(actual_input_tokens, actual_output_tokens)
                        logger.debug(f"Token accumulator updated: {token_accumulator.get_total()}")
                    
                    token_usage = {
                        'input_tokens': actual_input_tokens,
                        'output_tokens': actual_output_tokens,
                        'total_tokens': actual_input_tokens + actual_output_tokens
                    }
                    
                    # if request_id:
                    #     ai_logger.log_response(
                    #         request_id=request_id,
                    #         model=model,
                    #         response=str(response),
                    #         used_tools=[str(tool) for tool in used_tools],
                    #         token_usage=token_usage,
                    #         response_time=response_time
                    #     )
                    
                    # AI ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ ë¡œê¹…
                    logger.info(f"AI Response - Model: {model}, Agent: {agent_mode}, Time: {response_time:.2f}s, Tokens: IN:{token_usage.get('input_tokens', 0)} OUT:{token_usage.get('output_tokens', 0)}, Tools: {len(used_tools)}")
                    logger.debug(f"Response type: {type(response)}, Length: {len(str(response))} chars")
                    
                    # ì‘ë‹µì´ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                    if not isinstance(response, str):
                        response = str(response)
                    
                    # ì‚¬ìš©ëœ ë„êµ¬ ì—…ë°ì´íŠ¸
                    for tool in used_tools:
                        status_display.add_tool_used(str(tool))
                    

                    
                    # ìƒíƒœ í‘œì‹œì— í† í° ì •ë³´ ì—…ë°ì´íŠ¸
                    status_display.update_tokens(actual_input_tokens, actual_output_tokens)
                    
                    # ìƒíƒœ í‘œì‹œ ì™„ë£Œ
                    status_display.finish_processing(True)
                    
                    # senderì— ëª¨ë¸ ì •ë³´ì™€ í† í° ì •ë³´ í¬í•¨
                    total_tokens = actual_input_tokens + actual_output_tokens
                    if total_tokens > 0:
                        token_info = f" | ğŸ“Š {total_tokens:,}í† í°"
                    else:
                        token_info = ""
                    
                    model_sender = f"{sender}_{model}{token_info}"
                    
                    # ì‘ë‹µ ì „ì†¡
                    self.finished.emit(model_sender, response, used_tools)
                    
                    # ëŒ€í™” ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡ (ì‘ë‹µ ì „ì†¡ í›„)
                    self.conversation_completed.emit(None)
                elif not self._cancelled:
                    status_display.finish_processing(False)
                    logger.warning("Failed to generate response")
                    self.error.emit("ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                if not self._cancelled:
                    status_display.finish_processing(False)
                    error_msg = f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
                    logger.error(f"AI processing error - Model: {model}, Agent: {agent_mode}", exc_info=True)
                    self.error.emit(error_msg)
                    
                    # if request_id:
                    #     ai_logger.log_response(
                    #         request_id=request_id,
                    #         model=model,
                    #         response="",
                    #         used_tools=[],
                    #         token_usage={},
                    #         response_time=time.time() - request_start_time,
                    #         error=str(e)
                    #     )
        
        # ìŠ¤ë ˆë“œ í’€ì— ì‘ì—… ì œì¶œ
        with self._lock:
            self._cancelled = False
            self._current_future = self._executor.submit(_process)
        
        # ì—ëŸ¬ í•¸ë“¤ë§
        def _handle_future_exception(future: Future):
            try:
                future.result()
            except Exception as e:
                if not self._cancelled:
                    logger.error(f"Future ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        
        self._current_future.add_done_callback(_handle_future_exception)
    
    def _detect_korean_ratio(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°"""
        if not text:
            return 0.0
        
        korean_chars = 0
        total_chars = 0
        
        for char in text:
            if char.strip():  # ê³µë°± ì œì™¸
                total_chars += 1
                # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: ê°€(0xAC00) ~ í£(0xD7A3)
                if 0xAC00 <= ord(char) <= 0xD7A3:
                    korean_chars += 1
        
        return korean_chars / total_chars if total_chars > 0 else 0.0
    
    def _get_korean_threshold(self) -> float:
        """config.jsonì—ì„œ í•œê¸€ ì„ê³„ê°’ ì½ê¸°"""
        try:
            import json
            import os
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config.json')
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('language_detection', {}).get('korean_threshold', 0.1)
        except Exception:
            return 0.1  # ê¸°ë³¸ê°’