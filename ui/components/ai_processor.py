from PyQt6.QtCore import QObject, pyqtSignal
import threading
import logging
import time
from ui.components.status_display import status_display
from core.token_logger import TokenLogger
from core.ai_logger import ai_logger


class AIProcessor(QObject):
    """AI ìš”ì²­ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ (SRP)"""
    
    finished = pyqtSignal(str, str, list)  # sender, text, used_tools
    error = pyqtSignal(str)
    streaming = pyqtSignal(str, str)  # sender, partial_text
    streaming_complete = pyqtSignal(str, str, list)  # sender, full_text, used_tools
    
    def __init__(self, parent=None):
        super().__init__(parent)
        self._cancelled = False
        self._current_client = None
    
    def cancel(self):
        """ìš”ì²­ ì·¨ì†Œ"""
        self._cancelled = True
        status_display.finish_processing(False)
        if self._current_client:
            self._current_client.cancel_streaming()
    
    def process_request(self, api_key, model, messages, user_text=None, agent_mode=False, file_prompt=None):
        """AI ìš”ì²­ ì²˜ë¦¬ - ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨"""
        def _process():
            request_start_time = time.time()
            request_id = None
            try:
                if self._cancelled:
                    return
                
                # ìƒíƒœ í‘œì‹œ ì‹œì‘
                mode = 'agent' if agent_mode else 'ask'
                status_display.start_processing(model, mode)
                
                # AI ìš”ì²­ ë¡œê¹…
                available_tools = []
                try:
                    from mcp.client.mcp_client import mcp_manager
                    all_tools = mcp_manager.get_all_tools()
                    available_tools = [f"{server}.{tool['name']}" for server, tools in all_tools.items() for tool in tools]
                except:
                    pass
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“œë³„ ì°¨ë“± ì ìš©)
                from ui.prompts import prompt_manager
                provider = prompt_manager.get_provider_from_model(model)
                
                # ëª¨ë“œì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
                system_prompt = prompt_manager.get_system_prompt(provider, use_tools=agent_mode)
                
                # ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œ ë¡œê¹…
                if agent_mode:
                    # Agent ëª¨ë“œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    prompt_components = [
                        ('system_base', prompt_manager.get_prompt('common', 'system_base')),
                        ('tone_guidelines', prompt_manager.get_prompt('common', 'tone_guidelines')),
                        ('tool_selection', prompt_manager.get_prompt('common', 'tool_selection')),
                        ('schema_compliance', prompt_manager.get_prompt('common', 'schema_compliance')),
                        ('table_formatting', prompt_manager.get_prompt('common', 'table_formatting')),
                        ('error_handling', prompt_manager.get_prompt('common', 'error_handling')),
                        ('response_format', prompt_manager.get_prompt('common', 'response_format')),
                        ('markdown_standard', prompt_manager.get_prompt('common', 'markdown_standard')),
                        ('readability_enhancement', prompt_manager.get_prompt('common', 'readability_enhancement')),
                        ('code_block_strict', prompt_manager.get_prompt('common', 'code_block_strict')),
                        ('mermaid_diagram_rule', prompt_manager.get_prompt('common', 'mermaid_diagram_rule')),
                        ('agent_base', prompt_manager.get_prompt('common', 'agent_base')),
                        ('react_format', prompt_manager.get_prompt('common', 'react_format')),
                        ('json_output_format', prompt_manager.get_prompt('common', 'json_output_format')),
                        ('common_agent_rules', prompt_manager.get_prompt('common', 'common_agent_rules')),
                        ('model_enhancement', prompt_manager.get_custom_prompt(provider, 'system_enhancement') or '')
                    ]
                else:
                    # Ask ëª¨ë“œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    prompt_components = [
                        ('system_base', prompt_manager.get_prompt('common', 'system_base')),
                        ('tone_guidelines', prompt_manager.get_prompt('common', 'tone_guidelines')),
                        ('table_formatting', prompt_manager.get_prompt('common', 'table_formatting')),
                        ('markdown_standard', prompt_manager.get_prompt('common', 'markdown_standard')),
                        ('readability_enhancement', prompt_manager.get_prompt('common', 'readability_enhancement')),
                        ('mermaid_diagram_rule', prompt_manager.get_prompt('common', 'mermaid_diagram_rule')),
                        ('model_enhancement', prompt_manager.get_custom_prompt(provider, 'system_enhancement') or '')
                    ]
                
                print(f"\n=== PROMPT COMPONENTS LOG ===\nModel: {model} | Mode: {'Agent' if agent_mode else 'Ask'}")
                for key, value in prompt_components:
                    if value:
                        print(f"\n--- {key.upper()} ---\n{value[:200]}{'...' if len(value) > 200 else ''}")
                
                # ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ì…ë ¥ ê²°ì •
                input_for_detection = (file_prompt or "") + (user_text or "")
                
                # ì–¸ì–´ ê°ì§€ ë° ê°•ì œ ì–¸ì–´ ì§€ì‹œ ì¶”ê°€
                processed_file_prompt = file_prompt
                processed_user_text = user_text
                
                if input_for_detection.strip():
                    korean_ratio = self._detect_korean_ratio(input_for_detection)
                    korean_threshold = self._get_korean_threshold()
                    
                    if korean_ratio >= korean_threshold:
                        language_instruction = "\n\n[CRITICAL: ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. Answer only in Korean.]"
                    else:
                        language_instruction = "\n\n[CRITICAL: Please answer only in English. ì˜ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.]"
                    
                    # ì‚¬ìš©ì ì…ë ¥ì— ì–¸ì–´ ì§€ì‹œ ì¶”ê°€
                    if processed_file_prompt:
                        processed_file_prompt = processed_file_prompt + language_instruction
                    if processed_user_text:
                        processed_user_text = processed_user_text + language_instruction
                    
                    print(f"\n=== LANGUAGE DETECTION ===\nKorean ratio: {korean_ratio:.3f} | Threshold: {korean_threshold}")
                    print(f"Language instruction: {'Korean' if korean_ratio >= korean_threshold else 'English'}")
                
                # ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥ ê²°ì •
                actual_user_input = processed_file_prompt or processed_user_text or ""
                
                request_id = ai_logger.log_request(
                    model=model,
                    system_prompt=system_prompt,
                    user_input=actual_user_input,
                    conversation_history=messages,
                    tools_available=available_tools if agent_mode else [],
                    agent_mode=agent_mode
                )
                
                from core.ai_client import AIClient
                client = AIClient(api_key, model)
                self._current_client = client
                self._current_model = model
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì„¤ì •
                if messages:
                    client.conversation_history = messages
                    if hasattr(client, '_conversation_manager'):
                        client._conversation_manager.conversation_history = messages
                
                response = None
                sender = 'AI'
                used_tools = []
                
                if processed_file_prompt:
                    # íŒŒì¼ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
                    if agent_mode:
                        result = client.agent_chat(processed_file_prompt)
                        if isinstance(result, tuple):
                            response, used_tools = result
                        else:
                            response = result
                            used_tools = []
                        sender = 'ì—ì´ì „íŠ¸'
                    else:
                        response = client.simple_chat(processed_file_prompt)
                        sender = 'AI'
                        used_tools = []
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    if agent_mode:
                        # ì—ì´ì „íŠ¸ ëª¨ë“œ: ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥
                        if messages:
                            # íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                            full_messages = messages + [{'role': 'user', 'content': processed_user_text}]
                            # AIClient.chat() ë©”ì„œë“œë¥¼ í†µí•´ force_agent=True ì „ë‹¬
                            result = client.chat(full_messages, force_agent=True)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        else:
                            result = client.agent_chat(processed_user_text)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        sender = 'ì—ì´ì „íŠ¸'
                    else:
                        # Ask ëª¨ë“œ: ë„êµ¬ ì‚¬ìš© ì—†ì´ ë‹¨ìˆœ ì±„íŒ…ë§Œ
                        if messages:
                            # Ask ëª¨ë“œ: force_agent=Falseë¡œ ëª…ì‹œì  ì „ë‹¬
                            full_messages = messages + [{'role': 'user', 'content': processed_user_text}]
                            result = client.chat(full_messages, force_agent=False)
                            if isinstance(result, tuple):
                                response, used_tools = result
                            else:
                                response = result
                                used_tools = []
                        else:
                            response = client.simple_chat(processed_user_text)
                            used_tools = []
                        sender = 'AI'
                        
                        # Ask ëª¨ë“œì—ì„œëŠ” ë„êµ¬ ì‚¬ìš© ë¶ˆê°€ ë©”ì‹œì§€ ì œê±° (AIê°€ ì»¨í…ìŠ¤íŠ¸ íŒŒì•…í•´ì„œ íŒë‹¨)
                
                if not self._cancelled and response:
                    # ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë¨¼ì € ìˆ˜í–‰
                    actual_input_tokens, actual_output_tokens = 0, 0
                    if hasattr(client, '_last_response'):
                        actual_input_tokens, actual_output_tokens = TokenLogger.extract_actual_tokens(client._last_response)
                    
                    # ì‹¤ì œ í† í°ì´ ì—†ìœ¼ë©´ ì¶”ì •ì¹˜ ì‚¬ìš©
                    if actual_input_tokens == 0 and actual_output_tokens == 0:
                        # ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
                        input_text = ""
                        if messages:
                            for msg in messages:
                                if isinstance(msg, dict) and msg.get('content'):
                                    input_text += str(msg['content']) + "\n"
                        if user_text:
                            input_text += user_text
                        if file_prompt:
                            input_text += file_prompt
                        
                        actual_input_tokens = TokenLogger.estimate_tokens(input_text, model)
                        actual_output_tokens = TokenLogger.estimate_tokens(response, model)
                    
                    # AI ì‘ë‹µ ë¡œê¹… (ì •í™•í•œ í† í° ì •ë³´ í¬í•¨)
                    response_time = time.time() - request_start_time
                    token_usage = {
                        'input_tokens': actual_input_tokens,
                        'output_tokens': actual_output_tokens
                    }
                    
                    if request_id:
                        ai_logger.log_response(
                            request_id=request_id,
                            model=model,
                            response=str(response),
                            used_tools=[str(tool) for tool in used_tools],
                            token_usage=token_usage,
                            response_time=response_time
                        )
                        
                        # ì¶”ê°€ ìƒì„¸ ë¡œê¹…
                        print(f"\n=== AI THINKING PROCESS LOG ===\nRequest ID: {request_id}")
                        print(f"Model: {model} | Agent Mode: {agent_mode}")
                        print(f"Input Length: {len(actual_user_input)} chars")
                        print(f"Response Length: {len(str(response))} chars")
                        print(f"Tools Used: {used_tools}")
                        print(f"Response Time: {response_time:.2f}s")
                        print(f"Tokens: IN:{token_usage.get('input_tokens', 0)} OUT:{token_usage.get('output_tokens', 0)}")
                        if hasattr(client, '_last_response') and client._last_response:
                            print(f"\n--- RAW AI RESPONSE ---\n{str(client._last_response)[:500]}...")
                    
                    logging.info(f"AI Response Type: {type(response)}")
                    logging.info(f"AI Response: {str(response)}")
                    logging.info(f"Used Tools: {used_tools}")
                    
                    # ì‚¬ìš©ëœ ë„êµ¬ ì—…ë°ì´íŠ¸
                    for tool in used_tools:
                        status_display.add_tool_used(str(tool))
                    

                    
                    # ìƒíƒœ í‘œì‹œì— ì‹¤ì œ í† í° ì •ë³´ ì—…ë°ì´íŠ¸
                    status_display.update_tokens(actual_input_tokens, actual_output_tokens)
                    
                    # ë¡œê·¸ì— ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                    if hasattr(client, '_last_response') and client._last_response:
                        TokenLogger.log_actual_token_usage(model, client._last_response, "agent_chat" if agent_mode else "simple_chat")
                    else:
                        TokenLogger.log_token_usage(model, input_text, response, "agent_chat" if agent_mode else "simple_chat")
                    
                    # ìƒíƒœ í‘œì‹œ ì™„ë£Œ
                    status_display.finish_processing(True)
                    
                    # senderì— ëª¨ë¸ ì •ë³´ì™€ í† í° ì •ë³´ í¬í•¨ - TokenLoggerì™€ ë™ì¼í•œ í˜•ì‹
                    total_tokens = actual_input_tokens + actual_output_tokens
                    token_info = f" | ğŸ“Š {total_tokens:,}í† í° (IN:{actual_input_tokens:,} OUT:{actual_output_tokens:,})"
                    model_sender = f"{sender}_{model}{token_info}"
                    self.finished.emit(model_sender, response, used_tools)
                elif not self._cancelled:
                    status_display.finish_processing(False)
                    self.error.emit("ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                if not self._cancelled:
                    status_display.finish_processing(False)
                    error_msg = f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
                    self.error.emit(error_msg)
                    
                    # ì˜¤ë¥˜ ìƒì„¸ ë¡œê¹…
                    print(f"\n=== ERROR LOG ===\nRequest ID: {request_id}")
                    print(f"Model: {model} | Agent Mode: {agent_mode}")
                    print(f"Error: {str(e)}")
                    print(f"Error Type: {type(e).__name__}")
                    import traceback
                    print(f"Traceback:\n{traceback.format_exc()}")
                    
                    if request_id:
                        ai_logger.log_response(
                            request_id=request_id,
                            model=model,
                            response="",
                            used_tools=[],
                            token_usage={},
                            response_time=time.time() - request_start_time,
                            error=str(e)
                        )
        
        thread = threading.Thread(target=_process, daemon=True)
        thread.start()
    
    def _detect_korean_ratio(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°"""
        if not text:
            return 0.0
        
        korean_chars = 0
        total_chars = 0
        
        for char in text:
            if char.strip():  # ê³µë°± ì œì™¸
                total_chars += 1
                # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: ê°€(0xAC00) ~ í£(0xD7A3)
                if 0xAC00 <= ord(char) <= 0xD7A3:
                    korean_chars += 1
        
        return korean_chars / total_chars if total_chars > 0 else 0.0
    
    def _get_korean_threshold(self) -> float:
        """config.jsonì—ì„œ í•œê¸€ ì„ê³„ê°’ ì½ê¸°"""
        try:
            import json
            import os
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config.json')
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('language_detection', {}).get('korean_threshold', 0.1)
        except Exception:
            return 0.1  # ê¸°ë³¸ê°’